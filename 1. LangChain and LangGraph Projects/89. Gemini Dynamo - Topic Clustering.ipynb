{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyME+nyO1kkjesG/RSuwtyJS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ektE-RM_jqy0","executionInfo":{"status":"ok","timestamp":1727024158813,"user_tz":300,"elapsed":51013,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"4992468f-9d9a-4c49-a202-7896b5f684c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain_core\n","  Downloading langchain_core-0.3.5-py3-none-any.whl.metadata (6.3 kB)\n","Collecting langchain_community\n","  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n","Collecting langchain-openai\n","  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting openai\n","  Downloading openai-1.47.0-py3-none-any.whl.metadata (24 kB)\n","Collecting langchain_chroma\n","  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n","Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting chromadb\n","  Downloading chromadb-0.5.7-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n","  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.125-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n","  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n","  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting jiter<1,>=0.4.0 (from openai)\n","  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n","Collecting fastapi<1,>=0.95.2 (from langchain_chroma)\n","  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2)\n","Collecting chroma-hnswlib==0.7.6 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n","Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n","Collecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.6.6-py2.py3-none-any.whl.metadata (2.0 kB)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Collecting orjson>=3.9.12 (from chromadb)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.8.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1,>=0.95.2->langchain_chroma)\n","  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n","  Downloading durationpy-0.7.tar.gz (3.2 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n","Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.5-py3-none-any.whl (399 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.47.0-py3-none-any.whl (375 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chromadb-0.5.7-py3-none-any.whl (599 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.2/599.2 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n","Downloading langsmith-0.1.125-py3-none-any.whl (290 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n","Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n","Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n","Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n","Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n","Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n","Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n","Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n","Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Building wheels for collected packages: pypika, durationpy\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=02886df7e399998a96ac1173ef635ee6d0b8074799807f38e646a0b0560e7a5b\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for durationpy: filename=durationpy-0.7-py3-none-any.whl size=3462 sha256=f2de5bcbe8c6eb97b6be90e8fb3762ea5518113746732112560a8588de62d29a\n","  Stored in directory: /root/.cache/pip/wheels/a2/52/58/701659d0f7467c85c273d8d06d8f74f2646ee7da4145ce77b5\n","Successfully built pypika durationpy\n","Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, tenacity, python-dotenv, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, jsonpointer, jiter, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, httpcore, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, openai, langsmith, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain_core, langchain-text-splitters, langchain-openai, chromadb, langchain_chroma, langchain, langchain_community\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib_metadata 8.5.0\n","    Uninstalling importlib_metadata-8.5.0:\n","      Successfully uninstalled importlib_metadata-8.5.0\n","Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.7 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 durationpy-0.7 fastapi-0.115.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.2 humanfriendly-10.0 importlib-metadata-8.4.0 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-31.0.0 langchain-0.3.0 langchain-openai-0.2.0 langchain-text-splitters-0.3.0 langchain_chroma-0.1.4 langchain_community-0.3.0 langchain_core-0.3.5 langsmith-0.1.125 marshmallow-3.22.0 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.19.2 openai-1.47.0 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 orjson-3.10.7 overrides-7.7.0 posthog-3.6.6 pydantic-settings-2.5.2 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.38.5 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.1\n"]}],"source":["!pip install langchain langchain_core langchain_community langchain-openai openai langchain_chroma tiktoken chromadb"]},{"cell_type":"code","source":["from typing import List, Dict, Optional\n","from pydantic import BaseModel, Field, conlist\n","\n","class InputData(BaseModel):\n","    topic: str = Field(..., description=\"The main topic\")\n","    file_url: str = Field(..., description=\"URL of the file to be processed\")\n","    file_type: str = Field(..., description=\"Type of the file (e.g., csv, json, xls, xlsx, xml)\")\n","    lang: str = Field(..., description=\"Language of the document\")\n","\n","class Topic(BaseModel):\n","    topic_id: str = Field(..., description=\"Unique identifier for the topic\")\n","    topic_name: str = Field(..., description=\"Human-readable name of the topic\")\n","    keywords: List[str] = Field(..., description=\"List of keywords that define the topic\")\n","    importance_score: float = Field(..., description=\"Score representing the importance of this topic in the clustering\")\n","    description: Optional[str] = Field(None, description=\"Optional description of the topic\")\n","\n","\n","class ClusterMetadata(BaseModel):\n","    num_clusters: int = Field(..., description=\"Total number of clusters generated\")\n","    method: str = Field(..., description=\"Method used for clustering (e.g., K-means, LDA, etc.)\")\n","    timestamp: Optional[str] = Field(None, description=\"Timestamp when the clustering process was completed\")\n","\n","\n","class TopicCluster(BaseModel):\n","    cluster_id: str = Field(..., description=\"Unique identifier for the topic cluster\")\n","    topics: List[Topic] = Field(..., description=\"List of topics within this cluster\")\n","    central_topic: Optional[str] = Field(None, description=\"The central topic or most important topic within the cluster, if applicable\")\n","\n","\n","class TopicClusteringOutput(BaseModel):\n","    clustering_algorithm: str = Field(..., description=\"Algorithm used to generate the clusters\")\n","    clusters: conlist(TopicCluster) = Field(..., description=\"List of all topic clusters\")\n","    metadata: ClusterMetadata = Field(..., description=\"Metadata regarding the clustering process\")\n"],"metadata":{"id":"aFOykW6Ul2Fz","executionInfo":{"status":"ok","timestamp":1727024247902,"user_tz":300,"elapsed":541,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"GLemniX9m4e1","executionInfo":{"status":"ok","timestamp":1727024261314,"user_tz":300,"elapsed":3176,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip install langchain_google_genai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dg4ojE1Lm6xj","executionInfo":{"status":"ok","timestamp":1727024267983,"user_tz":300,"elapsed":4829,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"255c9a98-b06a-4cbc-e4b0-4218375f8094"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_google_genai\n","  Downloading langchain_google_genai-2.0.0-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.7.2)\n","Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.3.5)\n","Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (2.9.2)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.6.6)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.19.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.137.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.27.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.20.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.66.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.12.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.24.0)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (0.1.125)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (24.1)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (8.5.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.23.4)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.65.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.9)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.10.7)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.1.1)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.64.1)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.48.2)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.1.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.0.5)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (0.14.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.0.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.2.2)\n","Downloading langchain_google_genai-2.0.0-py3-none-any.whl (39 kB)\n","Installing collected packages: langchain_google_genai\n","Successfully installed langchain_google_genai-2.0.0\n"]}]},{"cell_type":"code","source":["os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"],"metadata":{"id":"hM2tnjMQm9C1","executionInfo":{"status":"ok","timestamp":1727024277244,"user_tz":300,"elapsed":2252,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from enum import Enum\n","\n","class FileType(Enum):\n","    PDF = 'pdf'\n","    CSV = 'csv'\n","    TXT = 'txt'\n","    MD = 'md'\n","    URL = \"url\"\n","    PPTX = 'pptx'\n","    DOCX = \"docx\"\n","    XLS = \"xls\"\n","    XLSX = \"xlsx\"\n","    XML = 'xml'\n","\n","    GDOC = 'gdoc'\n","    GSHEET = \"gsheet\"\n","    GSLIDE = \"gslide\"\n","    GPDF = 'gpdf'\n","\n","    YOUTUBE_URL = 'youtube_url'\n","    IMG = 'img'"],"metadata":{"id":"DAiWTYl9m-9r","executionInfo":{"status":"ok","timestamp":1727024280433,"user_tz":300,"elapsed":571,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class FileHandlerError(Exception):\n","    \"\"\"Raised when a file content cannot be loaded. Used for tools which require file handling.\"\"\"\n","    def __init__(self, message, url=None):\n","        self.message = message\n","        self.url = url\n","        super().__init__(self.message)\n","\n","    def __str__(self):\n","        return f\"{self.message}\"\n","\n","class ImageHandlerError(Exception):\n","    \"\"\"Raised when an image cannot be loaded. Used for tools which require image handling.\"\"\"\n","    def __init__(self, message, url):\n","        self.message = message\n","        self.url = url\n","        super().__init__(self.message)\n","\n","    def __str__(self):\n","        return f\"{self.message}\"\n","\n","class VideoTranscriptError(Exception):\n","    \"\"\"Raised when a video transcript cannot be loaded. Used for tools which require video transcripts.\"\"\"\n","    def __init__(self, message, url):\n","        self.message = message\n","        self.url = url\n","        super().__init__(self.message)\n","\n","    def __str__(self):\n","        return f\"{self.message}\""],"metadata":{"id":"wV2zcHPEnYkX","executionInfo":{"status":"ok","timestamp":1727024284303,"user_tz":300,"elapsed":458,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["%%writefile document_loaders.py\n","from langchain_community.document_loaders import YoutubeLoader, PyPDFLoader, TextLoader, UnstructuredURLLoader, UnstructuredPowerPointLoader, Docx2txtLoader, UnstructuredExcelLoader, UnstructuredXMLLoader\n","from langchain_community.document_loaders.csv_loader import CSVLoader\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.messages import HumanMessage\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_core.documents import Document\n","import os\n","import tempfile\n","import uuid\n","import requests\n","import gdown\n","from enum import Enum\n","\n","class FileType(Enum):\n","    PDF = 'pdf'\n","    CSV = 'csv'\n","    TXT = 'txt'\n","    MD = 'md'\n","    URL = 'url'\n","    PPTX = 'pptx'\n","    DOCX = 'docx'\n","    XLS = 'xls'\n","    XLSX = 'xlsx'\n","    XML = 'xml'\n","    GDOC = 'gdoc'\n","    GSHEET = 'gsheet'\n","    GSLIDE = 'gslide'\n","    GPDF = 'gpdf'\n","    YOUTUBE_URL = 'youtube_url'\n","    IMG = 'img'\n","\n","STRUCTURED_TABULAR_FILE_EXTENSIONS = {\"csv\", \"xls\", \"xlsx\", \"gsheet\", \"xml\"}\n","\n","splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 1000,\n","    chunk_overlap = 100\n",")\n","\n","def read_text_file(file_path):\n","    # Get the directory containing the script file\n","    script_dir = os.path.dirname(os.path.abspath(__file__))\n","\n","    # Combine the script directory with the relative file path\n","    absolute_file_path = os.path.join(script_dir, file_path)\n","\n","    with open(absolute_file_path, 'r') as file:\n","        return file.read()\n","\n","def get_docs(file_url: str, file_type: str, verbose=True):\n","    file_type = file_type.lower()\n","    try:\n","        file_loader = file_loader_map[FileType(file_type)]\n","        docs = file_loader(file_url, verbose)\n","\n","        return docs\n","\n","    except Exception as e:\n","        print(e)\n","        print(f\"Unsupported file type: {file_type}\")\n","        raise FileHandlerError(f\"Unsupported file type\", file_url) from e\n","\n","class FileHandler:\n","    def __init__(self, file_loader, file_extension):\n","        self.file_loader = file_loader\n","        self.file_extension = file_extension\n","\n","    def load(self, url):\n","        # Generate a unique filename with a UUID prefix\n","        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n","\n","        # Download the file from the URL and save it to a temporary file\n","        response = requests.get(url)\n","        response.raise_for_status()  # Ensure the request was successful\n","\n","        with tempfile.NamedTemporaryFile(delete=False, prefix=unique_filename) as temp_file:\n","            temp_file.write(response.content)\n","            temp_file_path = temp_file.name\n","\n","        # Use the file_loader to load the documents\n","        try:\n","            loader = self.file_loader(file_path=temp_file_path)\n","        except Exception as e:\n","            print(f\"No such file found at {temp_file_path}\")\n","            raise FileHandlerError(f\"No file found\", temp_file_path) from e\n","\n","        try:\n","            documents = loader.load()\n","        except Exception as e:\n","            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n","            raise FileHandlerError(f\"No file content available\", temp_file_path) from e\n","\n","        # Remove the temporary file\n","        os.remove(temp_file_path)\n","\n","        return documents\n","\n","def load_pdf_documents(pdf_url: str, verbose=False):\n","    pdf_loader = FileHandler(PyPDFLoader, \"pdf\")\n","    docs = pdf_loader.load(pdf_url)\n","\n","    if docs:\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found PDF file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_csv_documents(csv_url: str, verbose=False):\n","    csv_loader = FileHandler(CSVLoader, \"csv\")\n","    docs = csv_loader.load(csv_url)\n","\n","    if docs:\n","        if verbose:\n","            print(f\"Found CSV file\")\n","            print(f\"Splitting documents into {len(docs)} chunks\")\n","\n","        return docs\n","\n","def load_txt_documents(notes_url: str, verbose=False):\n","    notes_loader = FileHandler(TextLoader, \"txt\")\n","    docs = notes_loader.load(notes_url)\n","\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found TXT file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_md_documents(notes_url: str, verbose=False):\n","    notes_loader = FileHandler(TextLoader, \"md\")\n","    docs = notes_loader.load(notes_url)\n","\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found MD file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_url_documents(url: str, verbose=False):\n","    url_loader = UnstructuredURLLoader(urls=[url])\n","    docs = url_loader.load()\n","\n","    if docs:\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found URL\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_pptx_documents(pptx_url: str, verbose=False):\n","    pptx_handler = FileHandler(UnstructuredPowerPointLoader, 'pptx')\n","\n","    docs = pptx_handler.load(pptx_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found PPTX file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_docx_documents(docx_url: str, verbose=False):\n","    docx_handler = FileHandler(Docx2txtLoader, 'docx')\n","    docs = docx_handler.load(docx_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found DOCX file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_xls_documents(xls_url: str, verbose=False):\n","    xls_handler = FileHandler(UnstructuredExcelLoader, 'xls')\n","    docs = xls_handler.load(xls_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found XLS file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_xlsx_documents(xlsx_url: str, verbose=False):\n","    xlsx_handler = FileHandler(UnstructuredExcelLoader, 'xlsx')\n","    docs = xlsx_handler.load(xlsx_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found XLSX file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_xml_documents(xml_url: str, verbose=False):\n","    xml_handler = FileHandler(UnstructuredXMLLoader, 'xml')\n","    docs = xml_handler.load(xml_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found XML file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","class FileHandlerForGoogleDrive:\n","    def __init__(self, file_loader, file_extension='docx'):\n","        self.file_loader = file_loader\n","        self.file_extension = file_extension\n","\n","    def load(self, url):\n","\n","        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n","\n","        try:\n","            gdown.download(url=url, output=unique_filename, fuzzy=True)\n","        except Exception as e:\n","            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n","            raise FileHandlerError(f\"No file content available\") from e\n","\n","        try:\n","            loader = self.file_loader(file_path=unique_filename)\n","        except Exception as e:\n","            print(f\"No such file found at {unique_filename}\")\n","            raise FileHandlerError(f\"No file found\", unique_filename) from e\n","\n","        try:\n","            documents = loader.load()\n","        except Exception as e:\n","            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n","            raise FileHandlerError(f\"No file content available\") from e\n","\n","        os.remove(unique_filename)\n","\n","        return documents\n","\n","def load_gdocs_documents(drive_folder_url: str, verbose=False):\n","\n","    gdocs_loader = FileHandlerForGoogleDrive(Docx2txtLoader)\n","\n","    docs = gdocs_loader.load(drive_folder_url)\n","\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found Google Docs files\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_gsheets_documents(drive_folder_url: str, verbose=False):\n","    gsheets_loader = FileHandlerForGoogleDrive(UnstructuredExcelLoader, 'xlsx')\n","    docs = gsheets_loader.load(drive_folder_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found Google Sheets files\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_gslides_documents(drive_folder_url: str, verbose=False):\n","    gslides_loader = FileHandlerForGoogleDrive(UnstructuredPowerPointLoader, 'pptx')\n","    docs = gslides_loader.load(drive_folder_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found Google Slides files\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_gpdf_documents(drive_folder_url: str, verbose=False):\n","\n","    gpdf_loader = FileHandlerForGoogleDrive(PyPDFLoader,'pdf')\n","\n","    docs = gpdf_loader.load(drive_folder_url)\n","    if docs:\n","\n","        if verbose:\n","            print(f\"Found Google PDF files\")\n","            print(f\"Splitting documents into {len(docs)} chunks\")\n","\n","        return docs\n","\n","def load_docs_youtube_url(youtube_url: str, verbose=True) -> str:\n","    try:\n","        loader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=True)\n","    except Exception as e:\n","        print(f\"No such video found at {youtube_url}\")\n","        raise VideoTranscriptError(f\"No video found\", youtube_url) from e\n","\n","    try:\n","        docs = loader.load()\n","        length = docs[0].metadata[\"length\"]\n","        title = docs[0].metadata[\"title\"]\n","\n","    except Exception as e:\n","        print(f\"Video transcript might be private or unavailable in 'en' or the URL is incorrect.\")\n","        raise VideoTranscriptError(f\"No video transcripts available\", youtube_url) from e\n","\n","    if verbose:\n","        print(f\"Found video with title: {title} and length: {length}\")\n","        print(f\"Combined documents into a single string.\")\n","        print(f\"Beginning to process transcript...\")\n","\n","    split_docs = splitter.split_documents(docs)\n","\n","    return split_docs\n","\n","llm_for_img = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n","\n","def generate_docs_from_img(img_url, verbose: bool=False):\n","    message = HumanMessage(\n","    content=[\n","            {\n","                \"type\": \"text\",\n","                \"text\": \"Give me a summary of what you see in the image. It must be 3 detailed paragraphs.\",\n","            },\n","            {\"type\": \"image_url\", \"image_url\": img_url},\n","        ]\n","    )\n","\n","    try:\n","        response = llm_for_img.invoke([message]).content\n","        print(f\"Generated summary: {response}\")\n","        docs = Document(page_content=response, metadata={\"source\": img_url})\n","        split_docs = splitter.split_documents([docs])\n","    except Exception as e:\n","        print(f\"Error processing the request due to Invalid Content or Invalid Image URL\")\n","        raise ImageHandlerError(f\"Error processing the request\", img_url) from e\n","\n","    return split_docs\n","\n","file_loader_map = {\n","    FileType.PDF: load_pdf_documents,\n","    FileType.CSV: load_csv_documents,\n","    FileType.TXT: load_txt_documents,\n","    FileType.MD: load_md_documents,\n","    FileType.URL: load_url_documents,\n","    FileType.PPTX: load_pptx_documents,\n","    FileType.DOCX: load_docx_documents,\n","    FileType.XLS: load_xls_documents,\n","    FileType.XLSX: load_xlsx_documents,\n","    FileType.XML: load_xml_documents,\n","    FileType.GDOC: load_gdocs_documents,\n","    FileType.GSHEET: load_gsheets_documents,\n","    FileType.GSLIDE: load_gslides_documents,\n","    FileType.GPDF: load_gpdf_documents,\n","    FileType.YOUTUBE_URL: load_docs_youtube_url,\n","    FileType.IMG: generate_docs_from_img\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JBGO1_QnbRi","executionInfo":{"status":"ok","timestamp":1727024301258,"user_tz":300,"elapsed":662,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"e53da8e6-78a2-43b4-943a-cd3beec62558"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing document_loaders.py\n"]}]},{"cell_type":"code","source":["!pip install pypdf fpdf youtube-transcript-api pytube unstructured python-pptx docx2txt networkx pandas xlrd openpyxl gdown pytest PyPDF2 psutil"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOYc8kyHne6k","executionInfo":{"status":"ok","timestamp":1727024331060,"user_tz":300,"elapsed":20594,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"1020617d-4e90-4eb9-9a9c-4161455f68db"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-5.0.0-py3-none-any.whl.metadata (7.4 kB)\n","Collecting fpdf\n","  Downloading fpdf-1.7.2.tar.gz (39 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting youtube-transcript-api\n","  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n","Collecting pytube\n","  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting unstructured\n","  Downloading unstructured-0.15.13-py3-none-any.whl.metadata (29 kB)\n","Collecting python-pptx\n","  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n","Collecting docx2txt\n","  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n","Collecting filetype (from unstructured)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting python-magic (from unstructured)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n","Collecting emoji (from unstructured)\n","  Downloading emoji-2.13.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n","Collecting python-iso639 (from unstructured)\n","  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n","Collecting langdetect (from unstructured)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n","Collecting rapidfuzz (from unstructured)\n","  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n","Collecting unstructured-client (from unstructured)\n","  Downloading unstructured_client-0.25.9-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.5)\n","Collecting python-oxmsg (from unstructured)\n","  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (10.4.0)\n","Collecting XlsxWriter>=0.5.7 (from python-pptx)\n","  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.1)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.5.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.2)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.22.0)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n","Collecting olefile (from python-oxmsg->unstructured)\n","  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.8.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.1)\n","Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n","  Downloading deepdiff-8.0.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.2)\n","Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n","  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n","Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n","Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n","Collecting orderly-set==5.2.2 (from deepdiff>=6.0->unstructured-client->unstructured)\n","  Downloading orderly_set-5.2.2-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n","Downloading pypdf-5.0.0-py3-none-any.whl (292 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n","Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured-0.15.13-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading emoji-2.13.0-py3-none-any.whl (553 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n","Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_client-0.25.9-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deepdiff-8.0.1-py3-none-any.whl (82 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orderly_set-5.2.2-py3-none-any.whl (11 kB)\n","Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n","Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: fpdf, docx2txt, langdetect\n","  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=aa6765370d931c51075e1ca12830a8fe5ae0c846aa07c05712c40ba584815923\n","  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n","  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=7ed2aa8e170ff93b0571e9e0892479ab447d2618f898210a47e57dd58c3b0022\n","  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=7a66f2e1e8c79b51ce9373e9e659bd841cc6ccc32554a7d8811204adb92df47c\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built fpdf docx2txt langdetect\n","Installing collected packages: fpdf, filetype, docx2txt, XlsxWriter, rapidfuzz, pytube, python-magic, python-iso639, PyPDF2, pypdf, orderly-set, olefile, langdetect, jsonpath-python, emoji, youtube-transcript-api, requests-toolbelt, python-pptx, python-oxmsg, deepdiff, unstructured-client, unstructured\n","Successfully installed PyPDF2-3.0.1 XlsxWriter-3.2.0 deepdiff-8.0.1 docx2txt-0.8 emoji-2.13.0 filetype-1.2.0 fpdf-1.7.2 jsonpath-python-1.0.6 langdetect-1.0.9 olefile-0.47 orderly-set-5.2.2 pypdf-5.0.0 python-iso639-2024.4.27 python-magic-0.4.27 python-oxmsg-0.0.1 python-pptx-1.0.2 pytube-15.0.0 rapidfuzz-3.9.7 requests-toolbelt-1.0.0 unstructured-0.15.13 unstructured-client-0.25.9 youtube-transcript-api-0.6.2\n"]}]},{"cell_type":"code","source":["!pip uninstall -y nltk\n","!pip install nltk\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXrV-3tDoGFj","executionInfo":{"status":"ok","timestamp":1727024487753,"user_tz":300,"elapsed":11067,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"ef6bd673-fea9-4567-db56-0b95d35eff4b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: nltk 3.8.1\n","Uninstalling nltk-3.8.1:\n","  Successfully uninstalled nltk-3.8.1\n","Collecting nltk\n","  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nltk\n","Successfully installed nltk-3.9.1\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["def read_text_file(file_path):\n","    # Get the current working directory\n","    script_dir = os.getcwd()\n","\n","    # Combine the script directory with the relative file path\n","    absolute_file_path = os.path.join(script_dir, file_path)\n","\n","    try:\n","        with open(absolute_file_path, 'r') as file:\n","            content = file.read()\n","        return content\n","    except FileNotFoundError:\n","        # Handle the case where the file is not found\n","        print(f\"File not found: {absolute_file_path}\")\n","        return None"],"metadata":{"id":"10otU-aVoGB-","executionInfo":{"status":"ok","timestamp":1727024498709,"user_tz":300,"elapsed":5,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from langchain_openai.embeddings import OpenAIEmbeddings\n","embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"],"metadata":{"id":"XYelxYf1p9j3","executionInfo":{"status":"ok","timestamp":1727024511803,"user_tz":300,"elapsed":3407,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm_chat_openai = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"],"metadata":{"id":"0ucGq3zbp_DC","executionInfo":{"status":"ok","timestamp":1727024514413,"user_tz":300,"elapsed":779,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import GoogleGenerativeAI\n","\n","llm_google_genai = GoogleGenerativeAI(model=\"gemini-1.5-pro\")"],"metadata":{"id":"m_DdeEfM03MK","executionInfo":{"status":"ok","timestamp":1727024523820,"user_tz":300,"elapsed":1528,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","hyde_prompt_template = \"\"\"\n","Generate a hypothetical answer to the user's query.\n","Query: {query}\n","Hypothetical Answer:\n","\"\"\"\n","\n","hyde_prompt = PromptTemplate(input_variables=[\"query\"], template=hyde_prompt_template)\n","\n","hyde_chain = hyde_prompt | llm_google_genai"],"metadata":{"id":"JhiJwKsJvwxj","executionInfo":{"status":"ok","timestamp":1727024526442,"user_tz":300,"elapsed":519,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from langchain.schema import Document\n","from langchain.chains import  HypotheticalDocumentEmbedder\n","from langchain.vectorstores import Chroma\n","\n","def build_docsearch(documents: List[Document], embedding_model):\n","  embeddings = HypotheticalDocumentEmbedder(\n","    llm_chain=hyde_chain,\n","    base_embeddings=embedding_model\n","  )\n","\n","  docsearch = Chroma.from_documents(documents, embeddings)\n","\n","  return docsearch"],"metadata":{"id":"BytSlH9BsnVO","executionInfo":{"status":"ok","timestamp":1727024529746,"user_tz":300,"elapsed":1029,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","parser = JsonOutputParser(pydantic_object=TopicClusteringOutput)\n","format_instructions = parser.get_format_instructions()"],"metadata":{"id":"CEmIXa3TwnMY","executionInfo":{"status":"ok","timestamp":1727024534842,"user_tz":300,"elapsed":5,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["prompt_template = \"\"\"\n","Analyze the following document and perform topic clustering. Based on the document's content, select the most appropriate clustering algorithm, identify relevant topics, group them into clusters, and determine the importance of each topic within its cluster.\n","\n","Main Topic:\n","-----------------------------\n","{topic}\n","\n","Context:\n","-----------------------------\n","{context}\n","\n","Formatting:\n","-----------------------------\n","{format_instructions}\n","\n","Ensure the following:\n","- Automatically select the clustering algorithm based on the document.\n","- Provide metadata about the number of clusters and the method used.\n","- Identify and cluster topics, including their importance score and relevant keywords.\n","- Highlight any central topic within each cluster if applicable.\n","\n","You must respond in this language: {lang}\n","\"\"\""],"metadata":{"id":"Tz0NHTcRx7q2","executionInfo":{"status":"ok","timestamp":1727024544771,"user_tz":300,"elapsed":623,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","prompt = PromptTemplate(\n","    template=prompt_template,\n","    input_variables=[\"topic\", \"context\", \"lang\"],\n","    partial_variables={\"format_instructions\": format_instructions}\n",")"],"metadata":{"id":"XSdwWC7DyAcF","executionInfo":{"status":"ok","timestamp":1727024549175,"user_tz":300,"elapsed":520,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["chain = prompt | llm_chat_openai | parser"],"metadata":{"id":"OR95H2NMyJKm","executionInfo":{"status":"ok","timestamp":1727024626691,"user_tz":300,"elapsed":518,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"],"metadata":{"id":"3X6I2SS_yKqd","executionInfo":{"status":"ok","timestamp":1727024628271,"user_tz":300,"elapsed":2,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from document_loaders import get_docs\n","\n","def run_chain(input_data: InputData):\n","    documents = get_docs(input_data.file_url, input_data.file_type, verbose=True)\n","    if not documents:\n","        print(\"No documents loaded.\")\n","        return\n","\n","    split_docs = splitter.split_documents(documents)\n","\n","    docstore = build_docsearch(split_docs, embedding_model)\n","\n","    user_query = \"Develop a topic clustering based in this main topic: \"+input_data.topic\n","\n","    relevant_docs = docstore.similarity_search(user_query)\n","\n","    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n","\n","    output = chain.invoke({'topic': input_data.topic, 'context': context, 'lang': input_data.lang})\n","\n","    print(output)\n","\n","    del docstore\n","\n","    return output"],"metadata":{"id":"SFm7c1gXyXKJ","executionInfo":{"status":"ok","timestamp":1727024634742,"user_tz":300,"elapsed":665,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    input_data = InputData(\n","        topic=\"LLM\",\n","        file_url=\"https://raw.github.com/AaronSosaRamos/mission-flights/main/files-for-test/text.pdf\",\n","        file_type=\"pdf\",\n","        lang=\"en\"\n","    )\n","\n","    result = run_chain(input_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Pqa7suczEMM","executionInfo":{"status":"ok","timestamp":1726926408416,"user_tz":300,"elapsed":22384,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"e99b7e33-6ee3-435a-bc13-2bbd7abd959e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found PDF file\n","Splitting documents into 2 chunks\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"]},{"output_type":"stream","name":"stdout","text":["{'clustering_algorithm': 'K-means', 'clusters': [{'cluster_id': 'cluster_1', 'central_topic': 'Applications of LLMs', 'topics': [{'topic_id': 'topic_1', 'topic_name': 'Healthcare Applications', 'keywords': ['healthcare', 'medical documentation', 'research'], 'importance_score': 0.9, 'description': 'Use of LLMs in healthcare for documentation and research.'}, {'topic_id': 'topic_2', 'topic_name': 'Education Applications', 'keywords': ['education', 'intelligent tutoring systems'], 'importance_score': 0.8, 'description': 'Implementation of LLMs in educational tools and systems.'}, {'topic_id': 'topic_3', 'topic_name': 'Business Applications', 'keywords': ['business', 'customer service', 'chatbots', 'virtual assistants'], 'importance_score': 0.85, 'description': 'Leveraging LLMs for improving customer service in businesses.'}]}, {'cluster_id': 'cluster_2', 'central_topic': 'Challenges of LLMs', 'topics': [{'topic_id': 'topic_4', 'topic_name': 'Bias in LLMs', 'keywords': ['bias', 'societal prejudices', 'data'], 'importance_score': 0.95, 'description': 'Concerns regarding biases in LLMs due to training data.'}, {'topic_id': 'topic_5', 'topic_name': 'Computational Resources', 'keywords': ['computational resources', 'training', 'deployment'], 'importance_score': 0.85, 'description': 'High resource requirements for training and deploying LLMs.'}, {'topic_id': 'topic_6', 'topic_name': 'Interpretability', 'keywords': ['interpretability', 'trust', 'transparency'], 'importance_score': 0.9, 'description': 'Challenges in understanding LLM outputs and ensuring transparency.'}]}, {'cluster_id': 'cluster_3', 'central_topic': 'Future of LLMs', 'topics': [{'topic_id': 'topic_7', 'topic_name': 'Advancements in LLMs', 'keywords': ['advancements', 'reducing biases', 'improving efficiency'], 'importance_score': 0.8, 'description': 'Future research directions aimed at enhancing LLM capabilities.'}, {'topic_id': 'topic_8', 'topic_name': 'AI-driven Innovation', 'keywords': ['AI', 'innovation', 'applications'], 'importance_score': 0.75, 'description': 'The role of LLMs in driving future innovations in AI.'}]}], 'metadata': {'num_clusters': 3, 'method': 'K-means', 'timestamp': None}}\n"]}]},{"cell_type":"code","source":["result"],"metadata":{"id":"ko38DPqk1lMD","executionInfo":{"status":"ok","timestamp":1726926445080,"user_tz":300,"elapsed":373,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"3252e510-c9e1-4737-a07c-f1d564e4dfde","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'clustering_algorithm': 'K-means',\n"," 'clusters': [{'cluster_id': 'cluster_1',\n","   'central_topic': 'Applications of LLMs',\n","   'topics': [{'topic_id': 'topic_1',\n","     'topic_name': 'Healthcare Applications',\n","     'keywords': ['healthcare', 'medical documentation', 'research'],\n","     'importance_score': 0.9,\n","     'description': 'Use of LLMs in healthcare for documentation and research.'},\n","    {'topic_id': 'topic_2',\n","     'topic_name': 'Education Applications',\n","     'keywords': ['education', 'intelligent tutoring systems'],\n","     'importance_score': 0.8,\n","     'description': 'Implementation of LLMs in educational tools and systems.'},\n","    {'topic_id': 'topic_3',\n","     'topic_name': 'Business Applications',\n","     'keywords': ['business',\n","      'customer service',\n","      'chatbots',\n","      'virtual assistants'],\n","     'importance_score': 0.85,\n","     'description': 'Leveraging LLMs for improving customer service in businesses.'}]},\n","  {'cluster_id': 'cluster_2',\n","   'central_topic': 'Challenges of LLMs',\n","   'topics': [{'topic_id': 'topic_4',\n","     'topic_name': 'Bias in LLMs',\n","     'keywords': ['bias', 'societal prejudices', 'data'],\n","     'importance_score': 0.95,\n","     'description': 'Concerns regarding biases in LLMs due to training data.'},\n","    {'topic_id': 'topic_5',\n","     'topic_name': 'Computational Resources',\n","     'keywords': ['computational resources', 'training', 'deployment'],\n","     'importance_score': 0.85,\n","     'description': 'High resource requirements for training and deploying LLMs.'},\n","    {'topic_id': 'topic_6',\n","     'topic_name': 'Interpretability',\n","     'keywords': ['interpretability', 'trust', 'transparency'],\n","     'importance_score': 0.9,\n","     'description': 'Challenges in understanding LLM outputs and ensuring transparency.'}]},\n","  {'cluster_id': 'cluster_3',\n","   'central_topic': 'Future of LLMs',\n","   'topics': [{'topic_id': 'topic_7',\n","     'topic_name': 'Advancements in LLMs',\n","     'keywords': ['advancements', 'reducing biases', 'improving efficiency'],\n","     'importance_score': 0.8,\n","     'description': 'Future research directions aimed at enhancing LLM capabilities.'},\n","    {'topic_id': 'topic_8',\n","     'topic_name': 'AI-driven Innovation',\n","     'keywords': ['AI', 'innovation', 'applications'],\n","     'importance_score': 0.75,\n","     'description': 'The role of LLMs in driving future innovations in AI.'}]}],\n"," 'metadata': {'num_clusters': 3, 'method': 'K-means', 'timestamp': None}}"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    input_data = InputData(\n","        topic=\"LLM\",\n","        file_url=\"https://raw.github.com/AaronSosaRamos/mission-flights/main/files-for-test/text.pdf\",\n","        file_type=\"pdf\",\n","        lang=\"es\"\n","    )\n","\n","    result2 = run_chain(input_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eags0EDYsP3-","executionInfo":{"status":"ok","timestamp":1727024688151,"user_tz":300,"elapsed":21148,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"1dc308c4-2c80-4cd5-dc80-b6b992eb80db"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Found PDF file\n","Splitting documents into 2 chunks\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"]},{"output_type":"stream","name":"stdout","text":["{'clustering_algorithm': 'K-means', 'clusters': [{'cluster_id': 'cluster_1', 'central_topic': 'Desafíos de los LLM', 'topics': [{'topic_id': 'topic_1', 'topic_name': 'Bias en LLM', 'keywords': ['bias', 'prejuicios', 'sociedad'], 'importance_score': 0.8, 'description': 'El sesgo en los modelos de lenguaje puede reflejar prejuicios sociales presentes en los datos de entrenamiento.'}, {'topic_id': 'topic_2', 'topic_name': 'Recursos computacionales', 'keywords': ['recursos', 'computación', 'costos'], 'importance_score': 0.7, 'description': 'Los LLM requieren recursos computacionales significativos para su entrenamiento y despliegue.'}, {'topic_id': 'topic_3', 'topic_name': 'Interpretabilidad', 'keywords': ['interpretabilidad', 'transparencia', 'confianza'], 'importance_score': 0.6, 'description': 'La dificultad para entender las salidas de los modelos afecta la confianza en los resultados generados por IA.'}]}, {'cluster_id': 'cluster_2', 'central_topic': 'Aplicaciones de los LLM', 'topics': [{'topic_id': 'topic_4', 'topic_name': 'Uso en salud', 'keywords': ['salud', 'documentación médica', 'investigación'], 'importance_score': 0.9, 'description': 'Los LLM ayudan en la documentación médica y en la investigación en el sector salud.'}, {'topic_id': 'topic_5', 'topic_name': 'Uso en educación', 'keywords': ['educación', 'sistemas de tutoría', 'inteligencia'], 'importance_score': 0.8, 'description': 'Se utilizan para desarrollar sistemas de tutoría inteligentes en el ámbito educativo.'}, {'topic_id': 'topic_6', 'topic_name': 'Mejora del servicio al cliente', 'keywords': ['servicio al cliente', 'chatbots', 'asistentes virtuales'], 'importance_score': 0.7, 'description': 'Las empresas utilizan LLM para mejorar el servicio al cliente a través de chatbots y asistentes virtuales.'}]}], 'metadata': {'num_clusters': 2, 'method': 'K-means', 'timestamp': None}}\n"]}]},{"cell_type":"code","source":["result2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdfKjchmtCdE","executionInfo":{"status":"ok","timestamp":1727024869492,"user_tz":300,"elapsed":750,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"6daace12-579e-4f96-b5cc-04c1843a5bf4"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'clustering_algorithm': 'K-means',\n"," 'clusters': [{'cluster_id': 'cluster_1',\n","   'central_topic': 'Desafíos de los LLM',\n","   'topics': [{'topic_id': 'topic_1',\n","     'topic_name': 'Bias en LLM',\n","     'keywords': ['bias', 'prejuicios', 'sociedad'],\n","     'importance_score': 0.8,\n","     'description': 'El sesgo en los modelos de lenguaje puede reflejar prejuicios sociales presentes en los datos de entrenamiento.'},\n","    {'topic_id': 'topic_2',\n","     'topic_name': 'Recursos computacionales',\n","     'keywords': ['recursos', 'computación', 'costos'],\n","     'importance_score': 0.7,\n","     'description': 'Los LLM requieren recursos computacionales significativos para su entrenamiento y despliegue.'},\n","    {'topic_id': 'topic_3',\n","     'topic_name': 'Interpretabilidad',\n","     'keywords': ['interpretabilidad', 'transparencia', 'confianza'],\n","     'importance_score': 0.6,\n","     'description': 'La dificultad para entender las salidas de los modelos afecta la confianza en los resultados generados por IA.'}]},\n","  {'cluster_id': 'cluster_2',\n","   'central_topic': 'Aplicaciones de los LLM',\n","   'topics': [{'topic_id': 'topic_4',\n","     'topic_name': 'Uso en salud',\n","     'keywords': ['salud', 'documentación médica', 'investigación'],\n","     'importance_score': 0.9,\n","     'description': 'Los LLM ayudan en la documentación médica y en la investigación en el sector salud.'},\n","    {'topic_id': 'topic_5',\n","     'topic_name': 'Uso en educación',\n","     'keywords': ['educación', 'sistemas de tutoría', 'inteligencia'],\n","     'importance_score': 0.8,\n","     'description': 'Se utilizan para desarrollar sistemas de tutoría inteligentes en el ámbito educativo.'},\n","    {'topic_id': 'topic_6',\n","     'topic_name': 'Mejora del servicio al cliente',\n","     'keywords': ['servicio al cliente', 'chatbots', 'asistentes virtuales'],\n","     'importance_score': 0.7,\n","     'description': 'Las empresas utilizan LLM para mejorar el servicio al cliente a través de chatbots y asistentes virtuales.'}]}],\n"," 'metadata': {'num_clusters': 2, 'method': 'K-means', 'timestamp': None}}"]},"metadata":{},"execution_count":25}]}]}