{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from enum import Enum\n",
        "from typing import Literal\n",
        "\n",
        "class FileType(Enum):\n",
        "    PDF = 'pdf'\n",
        "    CSV = 'csv'\n",
        "    TXT = 'txt'\n",
        "    MD = 'md'\n",
        "    URL = 'url'\n",
        "    PPTX = 'pptx'\n",
        "    DOCX = 'docx'\n",
        "    XLS = 'xls'\n",
        "    XLSX = 'xlsx'\n",
        "    XML = 'xml'\n",
        "    GDOC = 'gdoc'\n",
        "    GSHEET = 'gsheet'\n",
        "    GSLIDE = 'gslide'\n",
        "    GPDF = 'gpdf'\n",
        "    YOUTUBE_URL = 'youtube_url'\n",
        "    IMG = 'img'\n"
      ],
      "metadata": {
        "id": "iP6VwKO3egio"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FileHandlerError(Exception):\n",
        "    \"\"\"Raised when a file content cannot be loaded. Used for tools which require file handling.\"\"\"\n",
        "    def __init__(self, message, url=None):\n",
        "        self.message = message\n",
        "        self.url = url\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.message}\"\n",
        "\n",
        "class ImageHandlerError(Exception):\n",
        "    \"\"\"Raised when an image cannot be loaded. Used for tools which require image handling.\"\"\"\n",
        "    def __init__(self, message, url):\n",
        "        self.message = message\n",
        "        self.url = url\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.message}\"\n",
        "\n",
        "class VideoTranscriptError(Exception):\n",
        "    \"\"\"Raised when a video transcript cannot be loaded. Used for tools which require video transcripts.\"\"\"\n",
        "    def __init__(self, message, url):\n",
        "        self.message = message\n",
        "        self.url = url\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.message}\""
      ],
      "metadata": {
        "id": "ogQVJ34iejAW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile document_loaders.py\n",
        "from langchain_community.document_loaders import YoutubeLoader, PyPDFLoader, TextLoader, UnstructuredURLLoader, UnstructuredPowerPointLoader, Docx2txtLoader, UnstructuredExcelLoader, UnstructuredXMLLoader\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "import os\n",
        "import tempfile\n",
        "import uuid\n",
        "import requests\n",
        "import gdown\n",
        "from enum import Enum\n",
        "\n",
        "class FileType(Enum):\n",
        "    PDF = 'pdf'\n",
        "    CSV = 'csv'\n",
        "    TXT = 'txt'\n",
        "    MD = 'md'\n",
        "    URL = 'url'\n",
        "    PPTX = 'pptx'\n",
        "    DOCX = 'docx'\n",
        "    XLS = 'xls'\n",
        "    XLSX = 'xlsx'\n",
        "    XML = 'xml'\n",
        "    GDOC = 'gdoc'\n",
        "    GSHEET = 'gsheet'\n",
        "    GSLIDE = 'gslide'\n",
        "    GPDF = 'gpdf'\n",
        "    YOUTUBE_URL = 'youtube_url'\n",
        "    IMG = 'img'\n",
        "\n",
        "STRUCTURED_TABULAR_FILE_EXTENSIONS = {\"csv\", \"xls\", \"xlsx\", \"gsheet\", \"xml\"}\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 100\n",
        ")\n",
        "\n",
        "def read_text_file(file_path):\n",
        "    # Get the directory containing the script file\n",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    # Combine the script directory with the relative file path\n",
        "    absolute_file_path = os.path.join(script_dir, file_path)\n",
        "\n",
        "    with open(absolute_file_path, 'r') as file:\n",
        "        return file.read()\n",
        "\n",
        "def get_docs(file_url: str, file_type: str, verbose=True):\n",
        "    file_type = file_type.lower()\n",
        "    try:\n",
        "        file_loader = file_loader_map[FileType(file_type)]\n",
        "        docs = file_loader(file_url, verbose)\n",
        "\n",
        "        return docs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(f\"Unsupported file type: {file_type}\")\n",
        "        raise FileHandlerError(f\"Unsupported file type\", file_url) from e\n",
        "\n",
        "class FileHandler:\n",
        "    def __init__(self, file_loader, file_extension):\n",
        "        self.file_loader = file_loader\n",
        "        self.file_extension = file_extension\n",
        "\n",
        "    def load(self, url):\n",
        "        # Generate a unique filename with a UUID prefix\n",
        "        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n",
        "\n",
        "        # Download the file from the URL and save it to a temporary file\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, prefix=unique_filename) as temp_file:\n",
        "            temp_file.write(response.content)\n",
        "            temp_file_path = temp_file.name\n",
        "\n",
        "        # Use the file_loader to load the documents\n",
        "        try:\n",
        "            loader = self.file_loader(file_path=temp_file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"No such file found at {temp_file_path}\")\n",
        "            raise FileHandlerError(f\"No file found\", temp_file_path) from e\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n",
        "            raise FileHandlerError(f\"No file content available\", temp_file_path) from e\n",
        "\n",
        "        # Remove the temporary file\n",
        "        os.remove(temp_file_path)\n",
        "\n",
        "        return documents\n",
        "\n",
        "def load_pdf_documents(pdf_url: str, verbose=False):\n",
        "    pdf_loader = FileHandler(PyPDFLoader, \"pdf\")\n",
        "    docs = pdf_loader.load(pdf_url)\n",
        "\n",
        "    if docs:\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found PDF file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_csv_documents(csv_url: str, verbose=False):\n",
        "    csv_loader = FileHandler(CSVLoader, \"csv\")\n",
        "    docs = csv_loader.load(csv_url)\n",
        "\n",
        "    if docs:\n",
        "        if verbose:\n",
        "            print(f\"Found CSV file\")\n",
        "            print(f\"Splitting documents into {len(docs)} chunks\")\n",
        "\n",
        "        return docs\n",
        "\n",
        "def load_txt_documents(notes_url: str, verbose=False):\n",
        "    notes_loader = FileHandler(TextLoader, \"txt\")\n",
        "    docs = notes_loader.load(notes_url)\n",
        "\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found TXT file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_md_documents(notes_url: str, verbose=False):\n",
        "    notes_loader = FileHandler(TextLoader, \"md\")\n",
        "    docs = notes_loader.load(notes_url)\n",
        "\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found MD file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_url_documents(url: str, verbose=False):\n",
        "    url_loader = UnstructuredURLLoader(urls=[url])\n",
        "    docs = url_loader.load()\n",
        "\n",
        "    if docs:\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found URL\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_pptx_documents(pptx_url: str, verbose=False):\n",
        "    pptx_handler = FileHandler(UnstructuredPowerPointLoader, 'pptx')\n",
        "\n",
        "    docs = pptx_handler.load(pptx_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found PPTX file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_docx_documents(docx_url: str, verbose=False):\n",
        "    docx_handler = FileHandler(Docx2txtLoader, 'docx')\n",
        "    docs = docx_handler.load(docx_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found DOCX file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_xls_documents(xls_url: str, verbose=False):\n",
        "    xls_handler = FileHandler(UnstructuredExcelLoader, 'xls')\n",
        "    docs = xls_handler.load(xls_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found XLS file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_xlsx_documents(xlsx_url: str, verbose=False):\n",
        "    xlsx_handler = FileHandler(UnstructuredExcelLoader, 'xlsx')\n",
        "    docs = xlsx_handler.load(xlsx_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found XLSX file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_xml_documents(xml_url: str, verbose=False):\n",
        "    xml_handler = FileHandler(UnstructuredXMLLoader, 'xml')\n",
        "    docs = xml_handler.load(xml_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found XML file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "class FileHandlerForGoogleDrive:\n",
        "    def __init__(self, file_loader, file_extension='docx'):\n",
        "        self.file_loader = file_loader\n",
        "        self.file_extension = file_extension\n",
        "\n",
        "    def load(self, url):\n",
        "\n",
        "        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n",
        "\n",
        "        try:\n",
        "            gdown.download(url=url, output=unique_filename, fuzzy=True)\n",
        "        except Exception as e:\n",
        "            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n",
        "            raise FileHandlerError(f\"No file content available\") from e\n",
        "\n",
        "        try:\n",
        "            loader = self.file_loader(file_path=unique_filename)\n",
        "        except Exception as e:\n",
        "            print(f\"No such file found at {unique_filename}\")\n",
        "            raise FileHandlerError(f\"No file found\", unique_filename) from e\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n",
        "            raise FileHandlerError(f\"No file content available\") from e\n",
        "\n",
        "        os.remove(unique_filename)\n",
        "\n",
        "        return documents\n",
        "\n",
        "def load_gdocs_documents(drive_folder_url: str, verbose=False):\n",
        "\n",
        "    gdocs_loader = FileHandlerForGoogleDrive(Docx2txtLoader)\n",
        "\n",
        "    docs = gdocs_loader.load(drive_folder_url)\n",
        "\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found Google Docs files\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_gsheets_documents(drive_folder_url: str, verbose=False):\n",
        "    gsheets_loader = FileHandlerForGoogleDrive(UnstructuredExcelLoader, 'xlsx')\n",
        "    docs = gsheets_loader.load(drive_folder_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found Google Sheets files\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_gslides_documents(drive_folder_url: str, verbose=False):\n",
        "    gslides_loader = FileHandlerForGoogleDrive(UnstructuredPowerPointLoader, 'pptx')\n",
        "    docs = gslides_loader.load(drive_folder_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found Google Slides files\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_gpdf_documents(drive_folder_url: str, verbose=False):\n",
        "\n",
        "    gpdf_loader = FileHandlerForGoogleDrive(PyPDFLoader,'pdf')\n",
        "\n",
        "    docs = gpdf_loader.load(drive_folder_url)\n",
        "    if docs:\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found Google PDF files\")\n",
        "            print(f\"Splitting documents into {len(docs)} chunks\")\n",
        "\n",
        "        return docs\n",
        "\n",
        "def load_docs_youtube_url(youtube_url: str, verbose=True) -> str:\n",
        "    try:\n",
        "        loader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=True)\n",
        "    except Exception as e:\n",
        "        print(f\"No such video found at {youtube_url}\")\n",
        "        raise VideoTranscriptError(f\"No video found\", youtube_url) from e\n",
        "\n",
        "    try:\n",
        "        docs = loader.load()\n",
        "        length = docs[0].metadata[\"length\"]\n",
        "        title = docs[0].metadata[\"title\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Video transcript might be private or unavailable in 'en' or the URL is incorrect.\")\n",
        "        raise VideoTranscriptError(f\"No video transcripts available\", youtube_url) from e\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Found video with title: {title} and length: {length}\")\n",
        "        print(f\"Combined documents into a single string.\")\n",
        "        print(f\"Beginning to process transcript...\")\n",
        "\n",
        "    split_docs = splitter.split_documents(docs)\n",
        "\n",
        "    return split_docs\n",
        "\n",
        "llm_for_img = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "def generate_docs_from_img(img_url, verbose: bool=False):\n",
        "    message = HumanMessage(\n",
        "    content=[\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"Give me a summary of what you see in the image. It must be 3 detailed paragraphs.\",\n",
        "            },\n",
        "            {\"type\": \"image_url\", \"image_url\": img_url},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = llm_for_img.invoke([message]).content\n",
        "        print(f\"Generated summary: {response}\")\n",
        "        docs = Document(page_content=response, metadata={\"source\": img_url})\n",
        "        split_docs = splitter.split_documents([docs])\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the request due to Invalid Content or Invalid Image URL\")\n",
        "        raise ImageHandlerError(f\"Error processing the request\", img_url) from e\n",
        "\n",
        "    return split_docs\n",
        "\n",
        "file_loader_map = {\n",
        "    FileType.PDF: load_pdf_documents,\n",
        "    FileType.CSV: load_csv_documents,\n",
        "    FileType.TXT: load_txt_documents,\n",
        "    FileType.MD: load_md_documents,\n",
        "    FileType.URL: load_url_documents,\n",
        "    FileType.PPTX: load_pptx_documents,\n",
        "    FileType.DOCX: load_docx_documents,\n",
        "    FileType.XLS: load_xls_documents,\n",
        "    FileType.XLSX: load_xlsx_documents,\n",
        "    FileType.XML: load_xml_documents,\n",
        "    FileType.GDOC: load_gdocs_documents,\n",
        "    FileType.GSHEET: load_gsheets_documents,\n",
        "    FileType.GSLIDE: load_gslides_documents,\n",
        "    FileType.GPDF: load_gpdf_documents,\n",
        "    FileType.YOUTUBE_URL: load_docs_youtube_url,\n",
        "    FileType.IMG: generate_docs_from_img\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxuSGzbCfwXV",
        "outputId": "064a47cb-bc33-4228-dd66-9d51ad818ba6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing document_loaders.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-google-genai langchain_community langchain-chroma chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NicPY2lfys1",
        "outputId": "fb45061a-bbc0-4974-972f-0d11a5f6d1c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.3.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting chroma\n",
            "  Downloading Chroma-0.2.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.123-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.7.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma)\n",
            "  Downloading chromadb-0.5.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting fastapi<1,>=0.95.2 (from langchain-chroma)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading posthog-3.6.6-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.66.5)\n",
            "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.5)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading mmh3-5.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.27.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.8.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1,>=0.95.2->langchain-chroma)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.24.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.65.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.8.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.13.2)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.24.7)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.2-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
            "Downloading chromadb-0.5.7-py3-none-any.whl (599 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.2/599.2 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.123-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: chroma, pypika\n",
            "  Building wheel for chroma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma: filename=Chroma-0.2.0-py3-none-any.whl size=7095 sha256=916f80d1429f67d87562f2624b85fa6a40c23a999ce8a685fd8e8600a46c1832\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/74/75/a6ab7999ae473ecbe819bc5cae9ccb902429dd6c60795f5112\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=58aeb14dbf901dded5ff7c553bcff9e8cf56855a7676f71bc69184916376b925\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma pypika\n",
            "Installing collected packages: pypika, monotonic, chroma, websockets, uvloop, tenacity, python-dotenv, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, jsonpointer, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, httpcore, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, langsmith, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, langchain-text-splitters, chromadb, langchain-google-genai, langchain-chroma, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-0.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.7 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 fastapi-0.115.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.2 humanfriendly-10.0 importlib-metadata-8.4.0 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-30.1.0 langchain-0.3.0 langchain-chroma-0.1.4 langchain-core-0.3.2 langchain-google-genai-2.0.0 langchain-text-splitters-0.3.0 langchain_community-0.3.0 langsmith-0.1.123 marshmallow-3.22.0 mmh3-5.0.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 orjson-3.10.7 overrides-7.7.0 posthog-3.6.6 pydantic-settings-2.5.2 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.38.5 tenacity-8.5.0 typing-inspect-0.9.0 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf fpdf youtube-transcript-api pytube unstructured python-pptx docx2txt networkx pandas xlrd openpyxl gdown pytest PyPDF2 psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxZ97M-Of1kn",
        "outputId": "a92c26d6-10ae-43ab-fe75-be145da31e0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.15.12-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.13.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.25.9-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (10.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.1)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
            "  Downloading deepdiff-8.0.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.2)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Collecting orderly-set==5.2.2 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
            "  Downloading orderly_set-5.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Downloading pypdf-5.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.13.0-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.25.9-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepdiff-8.0.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orderly_set-5.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fpdf, docx2txt, langdetect\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=23ca8ec25685d8a75fedd97298e432cc769cda8fc5bcc99463fc83bac2fe0586\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=7b27086d0db71afb11ecafb338d26fcea48be3d4956a1974235b27185546c07c\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=dc83d38ff1e0a933e9e5fbb91df45d12a78e799b0f2f8f44d676ded3461ed027\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built fpdf docx2txt langdetect\n",
            "Installing collected packages: fpdf, filetype, docx2txt, XlsxWriter, rapidfuzz, pytube, python-magic, python-iso639, PyPDF2, pypdf, orderly-set, olefile, langdetect, jsonpath-python, emoji, youtube-transcript-api, requests-toolbelt, python-pptx, python-oxmsg, deepdiff, unstructured-client, unstructured\n",
            "Successfully installed PyPDF2-3.0.1 XlsxWriter-3.2.0 deepdiff-8.0.1 docx2txt-0.8 emoji-2.13.0 filetype-1.2.0 fpdf-1.7.2 jsonpath-python-1.0.6 langdetect-1.0.9 olefile-0.47 orderly-set-5.2.2 pypdf-5.0.0 python-iso639-2024.4.27 python-magic-0.4.27 python-oxmsg-0.0.1 python-pptx-1.0.2 pytube-15.0.0 rapidfuzz-3.9.7 requests-toolbelt-1.0.0 unstructured-0.15.12 unstructured-client-0.25.9 youtube-transcript-api-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y nltk\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNGQzeCkf-m0",
        "outputId": "a2c7e291-c3b6-4e38-ecee-eacb3169b071"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.8.1\n",
            "Uninstalling nltk-3.8.1:\n",
            "  Successfully uninstalled nltk-3.8.1\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "Successfully installed nltk-3.9.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "import os\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "dkaqmFxGgWIl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_file(file_path):\n",
        "    # Get the current working directory\n",
        "    script_dir = os.getcwd()\n",
        "\n",
        "    # Combine the script directory with the relative file path\n",
        "    absolute_file_path = os.path.join(script_dir, file_path)\n",
        "\n",
        "    try:\n",
        "        with open(absolute_file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        # Handle the case where the file is not found\n",
        "        print(f\"File not found: {absolute_file_path}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "P6j8zuH6gd3N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uv5EMMp1-s8Y"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class ConnectWithThemArgs(BaseModel):\n",
        "    grade_level: str = Field(..., description=\"The grade level the teacher is instructing.\")\n",
        "    task_description: str = Field(..., description=\"A brief description of the subject or topic the teacher is instructing.\")\n",
        "    students_description: str = Field(..., description=\"A description of the students including age group, interests, location, and any relevant cultural or social factors.\")\n",
        "    file_url: str = Field(..., description=\"URL of any relevant file associated with the teaching material.\")\n",
        "    file_type: str = Field(..., description=\"The type of the file\")\n",
        "    lang: str = Field(..., description=\"The language in which the subject is being taught.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field, HttpUrl\n",
        "from typing import List\n",
        "\n",
        "class Recommendation(BaseModel):\n",
        "    project_overview: str = Field(..., description=\"A detailed description of the project or activity recommendation.\")\n",
        "    rationale: str = Field(..., description=\"An explanation of why this recommendation is relevant to the students' interests or background.\")\n",
        "    difficulty_level: str = Field(..., description=\"The difficulty level of the project (e.g., easy, moderate, challenging).\")\n",
        "    required_tools: List[str] = Field(..., description=\"A list of tools, software, or resources required to complete the project.\")\n",
        "    estimated_time: str = Field(..., description=\"The estimated time to complete the project or activity.\")\n",
        "\n",
        "class RecommendationsOutput(BaseModel):\n",
        "    recommendations: List[Recommendation] = Field(..., description=\"A list of personalized recommendations based on the input.\")"
      ],
      "metadata": {
        "id": "9oe_uyUaCqMa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AIConnectWithThemGenerator:\n",
        "    def __init__(self, args=None, vectorstore_class=Chroma, prompt=None, embedding_model=None, model=None, parser=None, verbose=False):\n",
        "        default_config = {\n",
        "            \"model\": GoogleGenerativeAI(model=\"gemini-1.5-flash\"),\n",
        "            \"embedding_model\": GoogleGenerativeAIEmbeddings(model='models/embedding-001'),\n",
        "            \"parser\": JsonOutputParser(pydantic_object=RecommendationsOutput),\n",
        "            \"prompt\": read_text_file(\"prompt/connect-with-them-prompt.txt\"),\n",
        "            \"vectorstore_class\": Chroma\n",
        "        }\n",
        "\n",
        "        self.prompt = prompt or default_config[\"prompt\"]\n",
        "        self.model = model or default_config[\"model\"]\n",
        "        self.parser = parser or default_config[\"parser\"]\n",
        "        self.embedding_model = embedding_model or default_config[\"embedding_model\"]\n",
        "\n",
        "        self.vectorstore_class = vectorstore_class or default_config[\"vectorstore_class\"]\n",
        "        self.vectorstore, self.retriever, self.runner = None, None, None\n",
        "        self.args = args\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if vectorstore_class is None: raise ValueError(\"Vectorstore must be provided\")\n",
        "        if args.grade_level is None: raise ValueError(\"Grade Level must be provided\")\n",
        "        if args.task_description is None: raise ValueError(\"Task Description must be provided\")\n",
        "        if args.students_description is None: raise ValueError(\"Student Description Level must be provided\")\n",
        "        if args.lang is None: raise ValueError(\"Language must be provided\")\n",
        "\n",
        "\n",
        "    def compile(self, documents: List[Document]):\n",
        "        # Return the chain\n",
        "        prompt = PromptTemplate(\n",
        "            template=self.prompt,\n",
        "            input_variables=[\"attribute_collection\"],\n",
        "            partial_variables={\"format_instructions\": self.parser.get_format_instructions()}\n",
        "        )\n",
        "\n",
        "        if self.runner is None:\n",
        "            print(f\"Creating vectorstore from {len(documents)} documents\") if self.verbose else None\n",
        "            self.vectorstore = self.vectorstore_class.from_documents(documents, self.embedding_model)\n",
        "            print(f\"Vectorstore created\") if self.verbose else None\n",
        "\n",
        "            self.retriever = self.vectorstore.as_retriever()\n",
        "            print(f\"Retriever created successfully\") if self.verbose else None\n",
        "\n",
        "            self.runner = RunnableParallel(\n",
        "                {\"context\": self.retriever,\n",
        "                \"attribute_collection\": RunnablePassthrough()\n",
        "                }\n",
        "            )\n",
        "\n",
        "        chain = self.runner | prompt | self.model | self.parser\n",
        "\n",
        "        if self.verbose: print(f\"Chain compilation complete\")\n",
        "\n",
        "        return chain\n",
        "\n",
        "    def generate_suggestion(self, documents: List[Document]):\n",
        "        if self.verbose: print(f\"Creating the AI Connect with Them suggestions\")\n",
        "\n",
        "        chain = self.compile(documents)\n",
        "\n",
        "        response = chain.invoke(f\"\"\"Grade Level: {self.args.grade_level},\n",
        "          Task Description: {self.args.task_description},\n",
        "          Student's Description: {self.args.students_description},\n",
        "          Language (YOU MUST RESPOND IN THIS LANGUAGE): {self.args.lang}\"\"\")\n",
        "\n",
        "        if self.verbose: print(f\"Deleting vectorstore\")\n",
        "        self.vectorstore.delete_collection()\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "qbx2JvNPgg1m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "qL48Q7iHiHw7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from document_loaders import get_docs\n",
        "\n",
        "input_data = {\n",
        "    \"grade_level\": \"College\",\n",
        "    \"task_description\": \"Develop a Neural Network Architecture for emulating Data Science Workflow\",\n",
        "    \"students_description\": \"Students aged 19-22, interested in hands-on activities, located in an urban area, with diverse cultural backgrounds.\",\n",
        "    \"file_url\": \"http://ijsmsjournal.org/2021/volume-4%20issue-4/ijsms-v4i4p137.pdf\",\n",
        "    \"file_type\": \"pdf\",\n",
        "    \"lang\": \"en\"\n",
        "}\n",
        "\n",
        "args = ConnectWithThemArgs(**input_data)\n",
        "\n",
        "docs = get_docs(args.file_url, args.file_type, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZPWa3QSiq13",
        "outputId": "e9c01353-8b05-493f-b7a7-25cbf2b8cd94"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found PDF file\n",
            "Splitting documents into 13 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gny1pAONjpfX",
        "outputId": "27da8aed-014d-4bb1-c286-6cad17ed9d14"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 0}, page_content='International Journal of Science and Management Studies (IJSMS)                        E-ISSN: 2581 -5946  \\nDOI: 10.51386/25815946/ijsms -v4i4p13 7 \\nVolume: 4 Issue: 4                                   July to August 2021                             https://www.ijsmsjournal.org  \\n  \\n \\n                                       This is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by -nc-nd/4.0/ )      Page 407  Introduction to Data Science - An Overview  \\nThangakumar J eyaprakash#1, Padmaveni K#2   \\n#1#2Associate Professor and Department of Computer Science and Engineering  \\n#1#2Hindustan Institute of Technology and Science, India  \\n \\nAbstract Data science plays a vital role in the research field of computer science and engineering which \\ninvolves collection of data, transformation, processing, describing,  and modelling. In this article, fundamental'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 0}, page_content='theory of Data Science, Machine learning and De ep Learning with the scope and opportunities has been \\ndiscussed. This helps the researchers to get a clarity on data science and its importance.  \\n \\nKeywords — Data science, Machine learning, Deep Learning  \\n \\nI. INTRODUCTION  \\nData science is the science of collecting, storing, processing, describing, and modelling of data  \\n \\n \\nFig 1. Five tasks of Data science  \\nII. DATA COLLECTION : \\n \\n         Data scientist must be work on the following questions to collect the data  \\n\\uf0b7 What to collect?  \\n\\uf0b7 How to collect?  \\n\\uf0b7 Where to collect?  \\n\\uf0b7 Label data or not?  \\nSuppose a data scientist must collect data [1] on shopping mart, he needs to identify the items need to be \\npurchased together to do some statistical modelling [2]. For example, a mobile phone may be purchased along \\nwith SD card, charge r etc. a laptop may be purchased together with laptop bag, and wireless mouse. Such data'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 0}, page_content='helps the data scientist to do some statistics like which items do customer purchase together? Or which items \\nmoving faster etc. Data scientist should have the knowled ge of structured query language or python packages to \\ndo analytics.   \\nIII. DATA STORAGE  \\n \\n The collected data would be a transactional or operational data [3] such as an insurance schemes, stock \\ninventory, customer records, employee records, student records, pat ients records, purchase orders, stock market, \\nschool children record etc.'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 1}, page_content='International Journal of Science and Management Studies (IJSMS)                        E-ISSN: 2581 -5946  \\nDOI: 10.51386/25815946/ijsms -v4i4p13 7 \\nVolume: 4 Issue: 4                                   July to August 2021                             https://www.ijsmsjournal.org  \\n  \\n \\n                                       This is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by -nc-nd/4.0/ )      Page 408   \\nFig 2: Data records  \\n \\nThese data can be divided into structured data and unstructured data [4]. Data in a structured format or readily \\navailable for analytics collected from multiple relational database management system (RDBMS) are known as \\nstructured data which can be integra ted into a common repository to support data analytics. The unstructured \\ndata referred as text, image, voice data, data from social blogs to do some sentimental analysis. The data which'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 1}, page_content='are unstructured needs to be cleaned or pre -processed to support analy tics. A large collection of data storage \\nwith both structured and unstructured data is known as data lake referred to as a Big Data. It consists of \\nuncurated, useful or the data is not useful, and the data from everywhere.  \\nIV. DATA PROCESSING  \\n \\nThe data proces sing involves data wrangling, data cleaning and data scaling  \\n \\n \\nFigure 3: Data Processing  \\n \\nA. Data wrangling or Data Munging  \\nData wrangling involves the extraction, transformation and load known as ETL process. The extraction of \\ndata from one database to another will be transformed into another form to support analytics which will be \\nloaded to the data warehouse is known as Data  wrangling or Munging. Some of the ETL [5] tools are IBM \\ninfosphere, informatica, Oracle data integrator, Alooma, Fivetran, Snaplogic etc.  \\n \\nB. Data Cleaning  \\nWhen extracting data from multiple databases, some values may be missed in columns. To fill such missing'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 1}, page_content='values, to standardize the keyword tags, to correct the spelling errors or to identify and remove unwanted data, \\ndata cleaning [6] is required.  \\n \\nC. Data S caling  \\n            The normalizing and standardizing of data are known as Data scaling. For example, converting all \\nkilometres to miles, rupees to dollars.'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 2}, page_content='International Journal of Science and Management Studies (IJSMS)                        E-ISSN: 2581 -5946  \\nDOI: 10.51386/25815946/ijsms -v4i4p13 7 \\nVolume: 4 Issue: 4                                   July to August 2021                             https://www.ijsmsjournal.org  \\n  \\n \\n                                       This is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by -nc-nd/4.0/ )      Page 409  D. Describe:  \\nAfter processing the data, it can be described in a visualized form using a different type  of plots such as box \\nplot, scattered plot, or line chart. For example, a car show room want to visualize the car sales in terms of \\ncolours such as red. Blue and green, then the data can be described with the following example.  \\n \\nTABLE 1 :  \\nCar sales with respect to colour  \\nColour  2018  2019  \\nRed 9 7 \\nBlue  7 5 \\nGreen  5 3 \\n \\nIn figure 4, the data has been described visually. It concludes, red has the maximum sales every year'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 2}, page_content='compared to other car colours while comparing the car sales for the 2018 and 2019 with respect to red, blue and \\ngreen. With this sales analysis, the data scientist may generate a report to increase the production of red \\ncoloured vehicles to attract more customers and to increase sales.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 4: Number of cars sold vs car colour  \\nV. DATA MODELLING  \\n \\n                  The data can be modelled after collecting, storing, processing, and describing the data.  The data \\ncan be modelled in two ways.  \\nA. Statistical Modelling  \\nB. Algorithm Modelling     \\n \\nA. Statistical Modelling:  \\n                      We assume very simple models which allows robust statistical analysis which gives statistical \\nguarantees.  \\ny=  f(x)  \\nWhere y is outcome, f is function and x is input. The outcome will be depending upon the value of x. x may \\nbe anything . If you want to make a coffee, the taste of the coffee is depending upon the quantity of coffee'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 2}, page_content='power, sugar and milk. Here, the outcome y is coffee, and x are coffee power, sugar and milk.   \\ny=f (x1,x2,x3)  \\nWhere x1, x2, x3 are coffee power, sugar and mil k respectively. These simple models can be derived using \\nstatistical modelling. The statistical modelling involves linear regression, logistic regression and linear \\ndiscriminant analysis.  \\n \\nB. Algorithm modelling:  \\n The complex problems can be derived using alg orithm modelling where we can train a model using the \\ntasks of data sciences. More multi -dimensional data [7] will be used here for statistical analysis. It mainly'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 3}, page_content='International Journal of Science and Management Studies (IJSMS)                        E-ISSN: 2581 -5946  \\nDOI: 10.51386/25815946/ijsms -v4i4p13 7 \\nVolume: 4 Issue: 4                                   July to August 2021                             https://www.ijsmsjournal.org  \\n  \\n \\n                                       This is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by -nc-nd/4.0/ )      Page 410  focuses on prediction and optimization with more solutions for Machine learning [8] and Deep  learning \\nalgorithms. It involves the statistical modelling methods such as linear regression, logistic regression, linear \\ndiscriminant analysis and the algorithm -based methods such as Decision [9][10  trees, k -Nearest neighbour, \\nSupport vector machine, Naïv e Bayes methods and Multi layered neural networks.  \\n \\n \\nFig 5: Algorithm Modelling  \\nVI. CONCLUSIONS'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 3}, page_content='Fig 5: Algorithm Modelling  \\nVI. CONCLUSIONS  \\nThis paper focuses mainly the fundamental theory of data sciences in a detailed approach which helps \\nresearchers to get clarity in collecting data, data storage, data processing, describing, data modelling. Machine \\nlearning and deep learning algorithms  \\nREFERENCES  \\n \\n[1] ―Data Science and Analytics‖, Department of Industrial and Systems Engineering Lehigh University Technical Report 15T -009 \\n \\n[2] Pasquier, N., Bastide, Y., Taouil, R., Lakhal, L.: Discovering frequent closed itemsets  for association rules. In: Proceedings of ICDT. \\npp. 398 –416. Springer (1999)  \\n \\n[3] Aden, C, Kleppin, L, Schmidt, G and Schröder, W. 2009. Consolidation, visualisation and analysis of forest condition relevant  data in \\nthe WebGISWaldIS. In: Strobl, J, Blaschke , Th and Griesebner, G (eds.), AngewandteGeoinformatik 2009, 506 –515'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 3}, page_content='[4] Chavan, V and Penev, L. 2011. The data paper: a mechanism to incentivize data publishing in biodiversity science. BMC \\nBioinformatics, 12(Supl. 15): S2. DOI: https://doi.org/10.1186/1471 -2105-12-S15-S2 \\n \\n[5] Tseng, V.S., Wu, C.W., Fournier -Viger, P., Philip, S.Y.: Efficient algorithms for mining the concise and lossless representation of \\nhigh utility itemsets. IEEE Trans. Knowl. Data Eng. 27(3), 726 –739 (2015)  \\n \\n[6] Dong, X., Gabrilovich, E., Heitz, G. , Horn, W., Lao, N., Murphy, K., Strohmann, T., Sun, S., Zhang, W.: Knowledge vault: a web -\\nscale approach to probabilistic knowledge fusion. In: Proceedings of the 20th ACM SIGKDD International Conference on \\nKnowledge Discovery and Data Mining, pp. 601 –610. ACM (2014)  \\n \\n[7] Gramener: Gramener —a data science company. https://gramener.com/ (2018). Accessed Dec 2018  \\n \\n[8] Polyzou, A., Karypis, G.: Grade prediction with models specific to students and courses. Int. J. Data Sci. Anal. 2(3 –4), 159 –171 \\n(2016)'),\n",
              " Document(metadata={'source': '/tmp/7d254070-0917-4bed-b465-ddc8a8aef130.pdfb8ibvhiv', 'page': 3}, page_content='(2016)  \\n \\n[9] Lam Ky Nhan, Phu ong Hoang Yen \"The Effects of Using Infographics -based Learning on EFL Learners’ Grammar Retention\" \\nInternational Journal of Science and Management Studies (IJSMS) V4.I4 (2021): 255 -265 \\n \\n[10] Nadia Ulfa Agustin Hilman, Maya Ariyanti, AstriGhina \"Social Media Mar keting Effect towards Purchase Decision at the \\nEmbroidery MSMEs in Tasikmalaya\" International Journal of Science and Management Studies (IJSMS) V4.I4 (2021): 202 -209.')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile connect-with-them-prompt.txt\n",
        "You are an expert in generating personalized teaching recommendations. Generate three different tailored recommendations based on the teacher's input, designed to engage their students by aligning with their interests, background, and cultural context. Each recommendation should include varied strategies such as local projects, music-related data analysis, or gamified learning.\n",
        "\n",
        "Here are the grade level, task, student description and the requested language of the response:\n",
        "{attribute_collection}\n",
        "\n",
        "Your response should be in the following format:\n",
        "{format_instructions}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sh8P6sWlgeO",
        "outputId": "ecaea394-9122-47d3-f573-350c5832b79d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing connect-with-them-prompt.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = AIConnectWithThemGenerator(args=args, verbose=True).generate_suggestion(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQFkfYPvmF9T",
        "outputId": "2e5f5d00-f2ef-4b94-b68c-b0f7ece91193"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the AI Connect with Them suggestions\n",
            "Creating vectorstore from 13 documents\n",
            "Vectorstore created\n",
            "Retriever created successfully\n",
            "Chain compilation complete\n",
            "Deleting vectorstore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pnG9uw1nUgj",
        "outputId": "e1bf8bee-f5d4-43cb-fd9d-d2e207561d72"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'recommendations': [{'project_overview': 'Design and train a neural network to predict the popularity of songs on a music streaming platform. Students can collect data from a public API or scrape data from a website. They can then use this data to train a neural network model and evaluate its performance. This project can be extended to include feature engineering, hyperparameter tuning, and model deployment.',\n",
              "   'rationale': \"This project aligns with students' interest in hands-on activities and their diverse cultural backgrounds by allowing them to explore the world of music through data analysis. It also offers a practical application of neural networks in a real-world scenario.\",\n",
              "   'difficulty_level': 'Moderate',\n",
              "   'required_tools': ['Python',\n",
              "    'TensorFlow or PyTorch',\n",
              "    'Jupyter Notebook',\n",
              "    'Music streaming platform API or data scraping tools'],\n",
              "   'estimated_time': '2-3 weeks'},\n",
              "  {'project_overview': 'Develop a neural network model to analyze and classify images of local landmarks in your city. Students can collect images from online sources or take their own photos. They can then use these images to train a convolutional neural network model and evaluate its performance on classifying different landmarks. This project can be extended to include object detection and image segmentation.',\n",
              "   'rationale': 'This project encourages students to explore their urban environment and apply their knowledge of neural networks to a local context. It also allows them to engage with their cultural heritage and contribute to a project that has tangible benefits for their community.',\n",
              "   'difficulty_level': 'Moderate to Challenging',\n",
              "   'required_tools': ['Python',\n",
              "    'TensorFlow or PyTorch',\n",
              "    'Keras',\n",
              "    'Image processing libraries like OpenCV'],\n",
              "   'estimated_time': '3-4 weeks'},\n",
              "  {'project_overview': \"Create a gamified learning experience using a neural network to teach students about different cultural traditions. Students can design a game where users can learn about various cultures by interacting with a virtual environment. The neural network can be used to personalize the learning experience by adapting to the user's progress and preferences. This project can be implemented using Unity or other game development frameworks.\",\n",
              "   'rationale': 'This project leverages the power of gamified learning to engage students in a fun and interactive way. It allows them to explore different cultures while applying their knowledge of neural networks to a creative and engaging project. The project also encourages collaboration and teamwork.',\n",
              "   'difficulty_level': 'Challenging',\n",
              "   'required_tools': ['Python',\n",
              "    'TensorFlow or PyTorch',\n",
              "    'Unity or other game development framework',\n",
              "    'Game design tools'],\n",
              "   'estimated_time': '4-6 weeks'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}