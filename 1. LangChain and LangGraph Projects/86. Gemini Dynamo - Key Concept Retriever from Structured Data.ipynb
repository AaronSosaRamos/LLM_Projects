{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/s/YQyIZVjdb4DDjeGIOM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -U langchain langchain-core langchain_community langchain_openai openai tiktoken psutil chromadb unstructured jq lark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwdzhAk8KU8G","executionInfo":{"status":"ok","timestamp":1727023047675,"user_tz":300,"elapsed":7601,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"b67cd4bd-e6f7-4218-d4b1-50c4984549f3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n","Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.3.5)\n","Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.0)\n","Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.47.0)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (6.0.0)\n","Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.7)\n","Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.15.13)\n","Requirement already satisfied: jq in /usr/local/lib/python3.10/dist-packages (1.8.0)\n","Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.125)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.5.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2)\n","Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n","Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.115.0)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.6)\n","Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.6.6)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.19.2)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n","Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48b0)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (31.0.0)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.0.0)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.8.1)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n","Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n","Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.13.0)\n","Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.4.27)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.7)\n","Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n","Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.25.9)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n","Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.0.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n","Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.38.5)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n","Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.7)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n","Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n","Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n","Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n","Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n","Requirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n","Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n","Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured) (0.47)\n","Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.1)\n","Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (8.0.1)\n","Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.6)\n","Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n","Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n","Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (5.0.0)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n","Requirement already satisfied: orderly-set==5.2.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured) (5.2.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Me6mpfrOGsKM","executionInfo":{"status":"ok","timestamp":1727023208048,"user_tz":300,"elapsed":1997,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"outputs":[],"source":["import os\n","import uuid\n","import requests\n","import tempfile\n","from typing import List\n","from pydantic import BaseModel, Field\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import Document\n","from langchain.prompts import PromptTemplate\n","from langchain_openai.embeddings import OpenAIEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.retrievers.self_query.base import SelfQueryRetriever\n","from langchain.chains.query_constructor.base import AttributeInfo\n","from langchain_core.output_parsers import JsonOutputParser\n","from langchain.vectorstores import Chroma\n","from langchain.document_loaders.csv_loader import CSVLoader\n","from langchain_community.document_loaders import UnstructuredExcelLoader, UnstructuredXMLLoader\n","from langchain_community.document_loaders.json_loader import JSONLoader"]},{"cell_type":"code","source":["from google.colab import userdata\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"CdW02vP-Lypq","executionInfo":{"status":"ok","timestamp":1727023321150,"user_tz":300,"elapsed":1743,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class InputData(BaseModel):\n","    file_url: str = Field(..., description=\"URL of the file to be processed\")\n","    file_type: str = Field(..., description=\"Type of the file (e.g., csv, json, xls, xlsx, xml)\")\n","    lang: str = Field(..., description=\"Language of the document\")\n","\n","class ConceptDefinitionRelation(BaseModel):\n","    concept: str = Field(..., description=\"The main concept of the flashcard\")\n","    definition: str = Field(..., description=\"The detailed definition of the concept\")\n","\n","class OutputData(BaseModel):\n","    concepts: List[ConceptDefinitionRelation] = Field(..., description=\"List of concepts and their definitions\")"],"metadata":{"id":"wrz73RlUMgCG","executionInfo":{"status":"ok","timestamp":1727023323103,"user_tz":300,"elapsed":520,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","class FileHandler:\n","    def __init__(self, file_loader, file_extension):\n","        self.file_loader = file_loader\n","        self.file_extension = file_extension\n","\n","    def load(self, url):\n","        # Generate a unique filename with a UUID prefix\n","        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n","\n","        # Download the file from the URL and save it to a temporary file\n","        response = requests.get(url)\n","        response.raise_for_status()  # Ensure the request was successful\n","\n","        with tempfile.NamedTemporaryFile(delete=False, suffix=unique_filename) as temp_file:\n","            temp_file.write(response.content)\n","            temp_file_path = temp_file.name\n","\n","        # Use the file_loader to load the documents\n","        try:\n","\n","            if(self.file_loader == JSONLoader):\n","              loader = self.file_loader(file_path=temp_file_path, jq_schema=\".\", text_content=False)\n","            else:\n","              loader = self.file_loader(file_path=temp_file_path)\n","        except Exception as e:\n","            print(f\"No such file found at {temp_file_path}\")\n","            raise FileNotFoundError(f\"No file found at {temp_file_path}\") from e\n","\n","        try:\n","            documents = loader.load()\n","            if documents:\n","              for doc in documents:\n","                  doc.metadata['file_type'] = self.file_extension\n","                  doc.metadata['processed_at'] = datetime.now().isoformat()\n","        except Exception as e:\n","            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n","            raise ValueError(f\"No file content available at {temp_file_path}\") from e\n","\n","        # Remove the temporary file\n","        os.remove(temp_file_path)\n","\n","        return documents"],"metadata":{"id":"DEcRAwFDMl_I","executionInfo":{"status":"ok","timestamp":1727023328521,"user_tz":300,"elapsed":4,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def load_csv_documents(csv_url: str, verbose=False):\n","    csv_loader = FileHandler(CSVLoader, \"csv\")\n","    docs = csv_loader.load(csv_url)\n","\n","    if docs:\n","        if verbose:\n","            print(f\"Found CSV file\")\n","            print(f\"Loaded {len(docs)} documents\")\n","        return docs\n","\n","def load_xls_documents(xls_url: str, verbose=False):\n","    xls_handler = FileHandler(UnstructuredExcelLoader, 'xls')\n","    docs = xls_handler.load(xls_url)\n","    if docs:\n","        if verbose:\n","            print(f\"Found XLS file\")\n","            print(f\"Loaded {len(docs)} documents\")\n","        return docs\n","\n","def load_xlsx_documents(xlsx_url: str, verbose=False):\n","    xlsx_handler = FileHandler(UnstructuredExcelLoader, 'xlsx')\n","    docs = xlsx_handler.load(xlsx_url)\n","    if docs:\n","        if verbose:\n","            print(f\"Found XLSX file\")\n","            print(f\"Loaded {len(docs)} documents\")\n","        return docs\n","\n","def load_xml_documents(xml_url: str, verbose=False):\n","    xml_handler = FileHandler(UnstructuredXMLLoader, 'xml')\n","    docs = xml_handler.load(xml_url)\n","    if docs:\n","        if verbose:\n","            print(f\"Found XML file\")\n","            print(f\"Loaded {len(docs)} documents\")\n","        return docs\n","def load_json_documents(json_url: str, verbose=False):\n","    json_handler = FileHandler(JSONLoader, 'json')\n","    docs = json_handler.load(json_url)\n","    if docs:\n","        if verbose:\n","            print(f\"Found JSON file\")\n","            print(f\"Loaded {len(docs)} documents\")\n","        return docs"],"metadata":{"id":"tdpRN1mJMmxB","executionInfo":{"status":"ok","timestamp":1727023332080,"user_tz":300,"elapsed":5,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def load_documents(file_url: str, file_type: str, verbose=False) -> List[Document]:\n","    if file_type.lower() == \"csv\":\n","        return load_csv_documents(file_url, verbose)\n","    elif file_type.lower() == \"xls\":\n","        return load_xls_documents(file_url, verbose)\n","    elif file_type.lower() == \"xlsx\":\n","        return load_xlsx_documents(file_url, verbose)\n","    elif file_type.lower() == \"xml\":\n","        return load_xml_documents(file_url, verbose)\n","    elif file_type.lower() == \"json\":\n","        return load_json_documents(file_url, verbose)\n","    else:\n","        raise ValueError(f\"Unsupported file type: {file_type}\")"],"metadata":{"id":"RgzkBr78Mpng","executionInfo":{"status":"ok","timestamp":1727023336202,"user_tz":300,"elapsed":516,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"],"metadata":{"id":"krgLlEyBMr1o","executionInfo":{"status":"ok","timestamp":1727023337738,"user_tz":300,"elapsed":4,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"],"metadata":{"id":"UKGs_FnnMujy","executionInfo":{"status":"ok","timestamp":1727023339271,"user_tz":300,"elapsed":482,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"],"metadata":{"id":"CPzAK3zQM5l7","executionInfo":{"status":"ok","timestamp":1727023340503,"user_tz":300,"elapsed":4,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["document_content_description = \"Documents loaded from the file, containing data in the specified language for key concept retrieving.\""],"metadata":{"id":"dW_SlKCiM7_b","executionInfo":{"status":"ok","timestamp":1727023342224,"user_tz":300,"elapsed":493,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["metadata_field_info = [\n","    AttributeInfo(\n","        name=\"file_type\",\n","        description=\"The type of the file (e.g., csv, json, xml, xls, xlsx).\",\n","        type=\"string\"\n","    ),\n","    AttributeInfo(\n","        name=\"processed_at\",\n","        description=\"The timestamp when the document was processed, in ISO 8601 format.\",\n","        type=\"string\"\n","    ),\n","]"],"metadata":{"id":"kaYIEJk3M95r","executionInfo":{"status":"ok","timestamp":1727023346327,"user_tz":300,"elapsed":442,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def build_vectorstore(documents: List[Document], embedding_model):\n","    vectorstore = Chroma.from_documents(documents, embedding_model)\n","    return vectorstore"],"metadata":{"id":"XJsAY47hNBxj","executionInfo":{"status":"ok","timestamp":1727023348022,"user_tz":300,"elapsed":448,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["parser = JsonOutputParser(pydantic_object=OutputData)\n","format_instructions = parser.get_format_instructions()"],"metadata":{"id":"JthA0cy-NDbL","executionInfo":{"status":"ok","timestamp":1727023349211,"user_tz":300,"elapsed":5,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["prompt_template = \"\"\"\n","Extract the key concepts from the following documents:\n","\n","{context}\n","\n","Formatting:\n","-----------------------------\n","{format_instructions}\n","\n","Respond only according to the format instructions. You must respond in this language: {lang}\n","\"\"\""],"metadata":{"id":"h_fF9hnWNFPM","executionInfo":{"status":"ok","timestamp":1727023352878,"user_tz":300,"elapsed":4,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["prompt = PromptTemplate(\n","    template=prompt_template,\n","    input_variables=[\"context\", \"lang\"],\n","    partial_variables={\"format_instructions\": format_instructions}\n",")"],"metadata":{"id":"on2v1vFtNIDN","executionInfo":{"status":"ok","timestamp":1727023354171,"user_tz":300,"elapsed":4,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["chain = prompt | llm | parser"],"metadata":{"id":"_iTVSyZCN_wc","executionInfo":{"status":"ok","timestamp":1727023355866,"user_tz":300,"elapsed":4,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def run_chain(input_data: InputData):\n","    documents = load_documents(input_data.file_url, input_data.file_type, verbose=True)\n","    if not documents:\n","        print(\"No documents loaded.\")\n","        return\n","\n","    split_docs = splitter.split_documents(documents)\n","\n","    vectorstore = build_vectorstore(split_docs, embedding_model)\n","\n","    retriever = SelfQueryRetriever.from_llm(\n","        llm=llm,\n","        vectorstore=vectorstore,\n","        document_contents=document_content_description,\n","        metadata_field_info=metadata_field_info,\n","        verbose=True,\n","        enable_limit=True\n","    )\n","\n","    user_query = \"Extract key concepts from all the documents\"\n","    relevant_docs = retriever.invoke(user_query)\n","\n","    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n","\n","    output = chain.invoke({'context': context, \"lang\": input_data.lang})\n","\n","    print(output)\n","\n","    del vectorstore\n","    del retriever\n","\n","    return output"],"metadata":{"id":"DXu6KneoOJuM","executionInfo":{"status":"ok","timestamp":1727023357808,"user_tz":300,"elapsed":501,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    input_data = InputData(\n","        file_url=\"https://raw.githubusercontent.com/AaronSosaRamos/mission-flights/refs/heads/main/files-for-test/file.json\",\n","        file_type=\"json\",\n","        lang=\"en\"\n","    )\n","\n","    result = run_chain(input_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6tNXaomX2EB","executionInfo":{"status":"ok","timestamp":1727022919430,"user_tz":300,"elapsed":15071,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"f85a928c-1a3d-45bf-bdf9-391123e09056"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Found JSON file\n","Loaded 1 documents\n","{'concepts': [{'concept': 'Adaptability', 'definition': 'With fine-tuning, LLMs can be adapted to perform specific tasks, from customer support to legal document analysis.'}, {'concept': 'Contextual Understanding', 'definition': 'LLMs can capture and generate coherent text based on previous context, making their responses more accurate and relevant.'}, {'concept': 'Data Bias', 'definition': 'LLMs can reflect biases present in the training data, leading to potentially harmful or unethical outputs.'}, {'concept': 'Computational Resources', 'definition': 'Training and running LLMs require substantial computational power and resources, making them costly to develop and maintain.'}, {'concept': 'Lack of Causality', 'definition': \"While LLMs can generate coherent text, they lack true understanding of causality, meaning they can't reason about real-world consequences or make informed decisions based on external factors.\"}, {'concept': 'Incoherence in Long Texts', 'definition': 'LLMs sometimes generate irrelevant or incoherent content when asked to produce long-form text, as they struggle with maintaining consistent context over longer passages.'}, {'concept': 'GPT', 'definition': 'Generative Pretrained Transformer, a model developed by OpenAI, is one of the most widely known LLMs used for natural language generation and understanding tasks.'}, {'concept': 'BERT', 'definition': 'Bidirectional Encoder Representations from Transformers, developed by Google, is a pre-trained LLM that excels at understanding the context of words in a sentence by looking at both directions (left and right of the word).'}, {'concept': 'Ethical Issues', 'definition': 'Concerns regarding bias, misuse, and the ethical implications of LLMs in various applications.'}, {'concept': 'Perplexity', 'definition': 'Measures how well a language model predicts the next word in a sentence. A lower perplexity score indicates better performance.'}, {'concept': 'BLEU Score', 'definition': 'Used to evaluate the quality of machine-generated translations compared to human translations.'}, {'concept': 'T5', 'definition': 'Text-To-Text Transfer Transformer, developed by Google, treats every NLP problem as a text generation task, making it highly flexible.'}, {'concept': 'PaLM', 'definition': 'Pathways Language Model, also by Google, designed to improve efficiency in training and generalization across multiple tasks.'}, {'concept': 'Data Collection', 'definition': 'Gathering sufficient high-quality, diverse datasets is a major challenge for training LLMs, especially in low-resource languages.'}, {'concept': 'Ethical Considerations', 'definition': 'Ensuring that LLMs do not propagate harmful stereotypes or disinformation is crucial.'}]}\n"]}]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jAdCh67gXzUA","executionInfo":{"status":"ok","timestamp":1727022919431,"user_tz":300,"elapsed":13,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"8ddb87d8-0907-4192-cf02-efc13af4716c"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'concepts': [{'concept': 'Adaptability',\n","   'definition': 'With fine-tuning, LLMs can be adapted to perform specific tasks, from customer support to legal document analysis.'},\n","  {'concept': 'Contextual Understanding',\n","   'definition': 'LLMs can capture and generate coherent text based on previous context, making their responses more accurate and relevant.'},\n","  {'concept': 'Data Bias',\n","   'definition': 'LLMs can reflect biases present in the training data, leading to potentially harmful or unethical outputs.'},\n","  {'concept': 'Computational Resources',\n","   'definition': 'Training and running LLMs require substantial computational power and resources, making them costly to develop and maintain.'},\n","  {'concept': 'Lack of Causality',\n","   'definition': \"While LLMs can generate coherent text, they lack true understanding of causality, meaning they can't reason about real-world consequences or make informed decisions based on external factors.\"},\n","  {'concept': 'Incoherence in Long Texts',\n","   'definition': 'LLMs sometimes generate irrelevant or incoherent content when asked to produce long-form text, as they struggle with maintaining consistent context over longer passages.'},\n","  {'concept': 'GPT',\n","   'definition': 'Generative Pretrained Transformer, a model developed by OpenAI, is one of the most widely known LLMs used for natural language generation and understanding tasks.'},\n","  {'concept': 'BERT',\n","   'definition': 'Bidirectional Encoder Representations from Transformers, developed by Google, is a pre-trained LLM that excels at understanding the context of words in a sentence by looking at both directions (left and right of the word).'},\n","  {'concept': 'Ethical Issues',\n","   'definition': 'Concerns regarding bias, misuse, and the ethical implications of LLMs in various applications.'},\n","  {'concept': 'Perplexity',\n","   'definition': 'Measures how well a language model predicts the next word in a sentence. A lower perplexity score indicates better performance.'},\n","  {'concept': 'BLEU Score',\n","   'definition': 'Used to evaluate the quality of machine-generated translations compared to human translations.'},\n","  {'concept': 'T5',\n","   'definition': 'Text-To-Text Transfer Transformer, developed by Google, treats every NLP problem as a text generation task, making it highly flexible.'},\n","  {'concept': 'PaLM',\n","   'definition': 'Pathways Language Model, also by Google, designed to improve efficiency in training and generalization across multiple tasks.'},\n","  {'concept': 'Data Collection',\n","   'definition': 'Gathering sufficient high-quality, diverse datasets is a major challenge for training LLMs, especially in low-resource languages.'},\n","  {'concept': 'Ethical Considerations',\n","   'definition': 'Ensuring that LLMs do not propagate harmful stereotypes or disinformation is crucial.'}]}"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    input_data = InputData(\n","        file_url=\"https://raw.githubusercontent.com/AaronSosaRamos/mission-flights/refs/heads/main/files-for-test/file.json\",\n","        file_type=\"json\",\n","        lang=\"es\"\n","    )\n","\n","    result2 = run_chain(input_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NW9DPqAmBkz","executionInfo":{"status":"ok","timestamp":1727023387003,"user_tz":300,"elapsed":13847,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"31d8f889-9b41-4aee-999f-c5a95b264128"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Found JSON file\n","Loaded 1 documents\n","{'concepts': [{'concept': 'Modelo de Lenguaje Grande (LLM)', 'definition': 'Un tipo de modelo de inteligencia artificial que procesa y genera texto similar al humano basado en grandes cantidades de datos textuales. Los LLM se construyen utilizando técnicas de aprendizaje profundo y se entrenan en conjuntos de datos diversos, lo que les permite entender y generar texto en varios idiomas y contextos.'}, {'concept': 'Arquitectura', 'definition': 'Los LLMs utilizan típicamente arquitecturas de transformadores, que les permiten procesar texto en paralelo, haciendo que los modelos sean altamente eficientes en la captura de dependencias a largo plazo en el texto.'}, {'concept': 'Datos de Entrenamiento', 'definition': 'Los LLMs se entrenan en vastas cantidades de datos textuales de fuentes como libros, artículos, sitios web y otros textos disponibles públicamente para aprender patrones y contextos del lenguaje.'}, {'concept': 'Tokenización', 'definition': 'Los LLMs utilizan la tokenización para descomponer el texto de entrada en fragmentos más pequeños (tokens) que el modelo puede procesar. Los tokens pueden representar palabras, subpalabras o caracteres.'}, {'concept': 'Procesamiento de Lenguaje Natural', 'definition': 'Los LLMs se utilizan ampliamente en tareas como generación de texto, traducción de idiomas, resumir y responder preguntas.'}, {'concept': 'Creación de Contenido', 'definition': 'Los LLMs se aprovechan para crear contenido escrito, incluidos artículos, historias y copias de marketing.'}, {'concept': 'Autocompletado de Código', 'definition': 'Los LLMs pueden ayudar en el desarrollo de software proporcionando sugerencias de código o generando código basado en descripciones en lenguaje natural.'}, {'concept': 'Agentes Conversacionales', 'definition': 'Los LLMs son la base de chatbots avanzados y asistentes virtuales, lo que permite conversaciones similares a las humanas.'}, {'concept': 'Escalabilidad', 'definition': 'Los LLMs pueden escalarse para manejar grandes conjuntos de datos y múltiples idiomas, proporcionando flexibilidad en varios dominios.'}, {'concept': 'Capacidades Multilingües', 'definition': 'Los LLMs entrenados en conjuntos de datos multilingües pueden generar y entender texto en múltiples idiomas, lo que los hace versátiles para aplicaciones globales.'}, {'concept': 'Desafíos de Entrenamiento', 'definition': 'Reunir conjuntos de datos diversos y de alta calidad es un gran desafío para entrenar LLMs, especialmente en idiomas de bajos recursos.'}, {'concept': 'Consideraciones Éticas', 'definition': 'Asegurar que los LLMs no propaguen estereotipos dañinos o desinformación es crucial, ya que estos modelos pueden reflejar sesgos presentes en los datos de entrenamiento.'}]}\n"]}]},{"cell_type":"code","source":["result2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQ7i6pw8mC2w","executionInfo":{"status":"ok","timestamp":1727023389390,"user_tz":300,"elapsed":490,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"194d2a27-a5ff-4774-d1ca-77e14fa0c312"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'concepts': [{'concept': 'Modelo de Lenguaje Grande (LLM)',\n","   'definition': 'Un tipo de modelo de inteligencia artificial que procesa y genera texto similar al humano basado en grandes cantidades de datos textuales. Los LLM se construyen utilizando técnicas de aprendizaje profundo y se entrenan en conjuntos de datos diversos, lo que les permite entender y generar texto en varios idiomas y contextos.'},\n","  {'concept': 'Arquitectura',\n","   'definition': 'Los LLMs utilizan típicamente arquitecturas de transformadores, que les permiten procesar texto en paralelo, haciendo que los modelos sean altamente eficientes en la captura de dependencias a largo plazo en el texto.'},\n","  {'concept': 'Datos de Entrenamiento',\n","   'definition': 'Los LLMs se entrenan en vastas cantidades de datos textuales de fuentes como libros, artículos, sitios web y otros textos disponibles públicamente para aprender patrones y contextos del lenguaje.'},\n","  {'concept': 'Tokenización',\n","   'definition': 'Los LLMs utilizan la tokenización para descomponer el texto de entrada en fragmentos más pequeños (tokens) que el modelo puede procesar. Los tokens pueden representar palabras, subpalabras o caracteres.'},\n","  {'concept': 'Procesamiento de Lenguaje Natural',\n","   'definition': 'Los LLMs se utilizan ampliamente en tareas como generación de texto, traducción de idiomas, resumir y responder preguntas.'},\n","  {'concept': 'Creación de Contenido',\n","   'definition': 'Los LLMs se aprovechan para crear contenido escrito, incluidos artículos, historias y copias de marketing.'},\n","  {'concept': 'Autocompletado de Código',\n","   'definition': 'Los LLMs pueden ayudar en el desarrollo de software proporcionando sugerencias de código o generando código basado en descripciones en lenguaje natural.'},\n","  {'concept': 'Agentes Conversacionales',\n","   'definition': 'Los LLMs son la base de chatbots avanzados y asistentes virtuales, lo que permite conversaciones similares a las humanas.'},\n","  {'concept': 'Escalabilidad',\n","   'definition': 'Los LLMs pueden escalarse para manejar grandes conjuntos de datos y múltiples idiomas, proporcionando flexibilidad en varios dominios.'},\n","  {'concept': 'Capacidades Multilingües',\n","   'definition': 'Los LLMs entrenados en conjuntos de datos multilingües pueden generar y entender texto en múltiples idiomas, lo que los hace versátiles para aplicaciones globales.'},\n","  {'concept': 'Desafíos de Entrenamiento',\n","   'definition': 'Reunir conjuntos de datos diversos y de alta calidad es un gran desafío para entrenar LLMs, especialmente en idiomas de bajos recursos.'},\n","  {'concept': 'Consideraciones Éticas',\n","   'definition': 'Asegurar que los LLMs no propaguen estereotipos dañinos o desinformación es crucial, ya que estos modelos pueden reflejar sesgos presentes en los datos de entrenamiento.'}]}"]},"metadata":{},"execution_count":20}]}]}