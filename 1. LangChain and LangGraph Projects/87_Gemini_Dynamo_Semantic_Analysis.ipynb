{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Search"
      ],
      "metadata": {
        "id": "YaOva1mAph61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BM25 Retriever - Sparse retriever"
      ],
      "metadata": {
        "id": "DQKMcTwhph5h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nLUViLNnRTU",
        "outputId": "d8da90bb-5a61-45cc-bcfb-12da0896dad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.1/375.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.2/599.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain langchain-openai openai tiktoken chromadb rank_bm25 faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cl09Ob3p0ux",
        "outputId": "d27278a2-0f47-4ae0-b489-263a32be8ad9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class InputData(BaseModel):\n",
        "    topic: str = Field(..., description=\"Topic for the Semantic Analysis\")\n",
        "    file_url: str = Field(..., description=\"URL of the file to be processed\")\n",
        "    file_type: str = Field(..., description=\"Type of the file (e.g., csv, json, xls, xlsx, xml)\")\n",
        "    lang: str = Field(..., description=\"Language of the document\")\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    text: str = Field(..., description=\"The entity found in the text.\")\n",
        "    type: str = Field(..., description=\"The type of entity (e.g., person, organization, location).\")\n",
        "    start_char: int = Field(..., description=\"Start character index of the entity in the input text.\")\n",
        "    end_char: int = Field(..., description=\"End character index of the entity in the input text.\")\n",
        "    confidence: Optional[float] = Field(None, description=\"Confidence score for the entity recognition.\")\n",
        "\n",
        "class Keyword(BaseModel):\n",
        "    word: str = Field(..., description=\"A keyword extracted from the text.\")\n",
        "    importance: float = Field(..., description=\"Importance or relevance of the keyword in the text.\")\n",
        "\n",
        "class Sentiment(BaseModel):\n",
        "    polarity: float = Field(..., description=\"Sentiment polarity, where -1 is very negative, 1 is very positive, and 0 is neutral.\")\n",
        "    subjectivity: float = Field(..., description=\"Subjectivity score, where 0 is very objective and 1 is very subjective.\")\n",
        "\n",
        "class SemanticAnalysisOutput(BaseModel):\n",
        "    input_text: str = Field(..., description=\"The original text input for analysis.\")\n",
        "    entities: List[Entity] = Field(..., description=\"A list of named entities found in the text.\")\n",
        "    keywords: List[Keyword] = Field(..., description=\"A list of important keywords from the text.\")\n",
        "    sentiment: Sentiment = Field(..., description=\"Sentiment analysis result for the input text.\")\n",
        "    language: str = Field(..., description=\"Detected language of the input text.\")\n",
        "    confidence: float = Field(..., description=\"Confidence score of the overall semantic analysis.\")\n",
        "    summary: Optional[str] = Field(None, description=\"Optional summary of the input text.\")"
      ],
      "metadata": {
        "id": "PWRfpR85rsPP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "A6LZ5eqZv8ux"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVS6vUNK1-Kq",
        "outputId": "ed7b8203-a213-48f2-dd2c-1730585259a6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.7.2)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (2.9.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.24.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.117 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (0.1.125)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.23.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.10.7)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3.0->langchain_google_genai) (1.2.2)\n",
            "Downloading langchain_google_genai-2.0.0-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: langchain_google_genai\n",
            "Successfully installed langchain_google_genai-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "cRvjFQsh2An6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class FileType(Enum):\n",
        "    PDF = 'pdf'\n",
        "    CSV = 'csv'\n",
        "    TXT = 'txt'\n",
        "    MD = 'md'\n",
        "    URL = \"url\"\n",
        "    PPTX = 'pptx'\n",
        "    DOCX = \"docx\"\n",
        "    XLS = \"xls\"\n",
        "    XLSX = \"xlsx\"\n",
        "    XML = 'xml'\n",
        "\n",
        "    GDOC = 'gdoc'\n",
        "    GSHEET = \"gsheet\"\n",
        "    GSLIDE = \"gslide\"\n",
        "    GPDF = 'gpdf'\n",
        "\n",
        "    YOUTUBE_URL = 'youtube_url'\n",
        "    IMG = 'img'"
      ],
      "metadata": {
        "id": "jO_AMkxBwaIE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FileHandlerError(Exception):\n",
        "    \"\"\"Raised when a file content cannot be loaded. Used for tools which require file handling.\"\"\"\n",
        "    def __init__(self, message, url=None):\n",
        "        self.message = message\n",
        "        self.url = url\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.message}\"\n",
        "\n",
        "class ImageHandlerError(Exception):\n",
        "    \"\"\"Raised when an image cannot be loaded. Used for tools which require image handling.\"\"\"\n",
        "    def __init__(self, message, url):\n",
        "        self.message = message\n",
        "        self.url = url\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.message}\"\n",
        "\n",
        "class VideoTranscriptError(Exception):\n",
        "    \"\"\"Raised when a video transcript cannot be loaded. Used for tools which require video transcripts.\"\"\"\n",
        "    def __init__(self, message, url):\n",
        "        self.message = message\n",
        "        self.url = url\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.message}\""
      ],
      "metadata": {
        "id": "jXuoxshcwyow"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile document_loaders.py\n",
        "from langchain_community.document_loaders import YoutubeLoader, PyPDFLoader, TextLoader, UnstructuredURLLoader, UnstructuredPowerPointLoader, Docx2txtLoader, UnstructuredExcelLoader, UnstructuredXMLLoader\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "import os\n",
        "import tempfile\n",
        "import uuid\n",
        "import requests\n",
        "import gdown\n",
        "from enum import Enum\n",
        "\n",
        "class FileType(Enum):\n",
        "    PDF = 'pdf'\n",
        "    CSV = 'csv'\n",
        "    TXT = 'txt'\n",
        "    MD = 'md'\n",
        "    URL = 'url'\n",
        "    PPTX = 'pptx'\n",
        "    DOCX = 'docx'\n",
        "    XLS = 'xls'\n",
        "    XLSX = 'xlsx'\n",
        "    XML = 'xml'\n",
        "    GDOC = 'gdoc'\n",
        "    GSHEET = 'gsheet'\n",
        "    GSLIDE = 'gslide'\n",
        "    GPDF = 'gpdf'\n",
        "    YOUTUBE_URL = 'youtube_url'\n",
        "    IMG = 'img'\n",
        "\n",
        "STRUCTURED_TABULAR_FILE_EXTENSIONS = {\"csv\", \"xls\", \"xlsx\", \"gsheet\", \"xml\"}\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 100\n",
        ")\n",
        "\n",
        "def read_text_file(file_path):\n",
        "    # Get the directory containing the script file\n",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    # Combine the script directory with the relative file path\n",
        "    absolute_file_path = os.path.join(script_dir, file_path)\n",
        "\n",
        "    with open(absolute_file_path, 'r') as file:\n",
        "        return file.read()\n",
        "\n",
        "def get_docs(file_url: str, file_type: str, verbose=True):\n",
        "    file_type = file_type.lower()\n",
        "    try:\n",
        "        file_loader = file_loader_map[FileType(file_type)]\n",
        "        docs = file_loader(file_url, verbose)\n",
        "\n",
        "        return docs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(f\"Unsupported file type: {file_type}\")\n",
        "        raise FileHandlerError(f\"Unsupported file type\", file_url) from e\n",
        "\n",
        "class FileHandler:\n",
        "    def __init__(self, file_loader, file_extension):\n",
        "        self.file_loader = file_loader\n",
        "        self.file_extension = file_extension\n",
        "\n",
        "    def load(self, url):\n",
        "        # Generate a unique filename with a UUID prefix\n",
        "        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n",
        "\n",
        "        # Download the file from the URL and save it to a temporary file\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, prefix=unique_filename) as temp_file:\n",
        "            temp_file.write(response.content)\n",
        "            temp_file_path = temp_file.name\n",
        "\n",
        "        # Use the file_loader to load the documents\n",
        "        try:\n",
        "            loader = self.file_loader(file_path=temp_file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"No such file found at {temp_file_path}\")\n",
        "            raise FileHandlerError(f\"No file found\", temp_file_path) from e\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n",
        "            raise FileHandlerError(f\"No file content available\", temp_file_path) from e\n",
        "\n",
        "        # Remove the temporary file\n",
        "        os.remove(temp_file_path)\n",
        "\n",
        "        return documents\n",
        "\n",
        "def load_pdf_documents(pdf_url: str, verbose=False):\n",
        "    pdf_loader = FileHandler(PyPDFLoader, \"pdf\")\n",
        "    docs = pdf_loader.load(pdf_url)\n",
        "\n",
        "    if docs:\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found PDF file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_csv_documents(csv_url: str, verbose=False):\n",
        "    csv_loader = FileHandler(CSVLoader, \"csv\")\n",
        "    docs = csv_loader.load(csv_url)\n",
        "\n",
        "    if docs:\n",
        "        if verbose:\n",
        "            print(f\"Found CSV file\")\n",
        "            print(f\"Splitting documents into {len(docs)} chunks\")\n",
        "\n",
        "        return docs\n",
        "\n",
        "def load_txt_documents(notes_url: str, verbose=False):\n",
        "    notes_loader = FileHandler(TextLoader, \"txt\")\n",
        "    docs = notes_loader.load(notes_url)\n",
        "\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found TXT file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_md_documents(notes_url: str, verbose=False):\n",
        "    notes_loader = FileHandler(TextLoader, \"md\")\n",
        "    docs = notes_loader.load(notes_url)\n",
        "\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found MD file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_url_documents(url: str, verbose=False):\n",
        "    url_loader = UnstructuredURLLoader(urls=[url])\n",
        "    docs = url_loader.load()\n",
        "\n",
        "    if docs:\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found URL\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_pptx_documents(pptx_url: str, verbose=False):\n",
        "    pptx_handler = FileHandler(UnstructuredPowerPointLoader, 'pptx')\n",
        "\n",
        "    docs = pptx_handler.load(pptx_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found PPTX file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_docx_documents(docx_url: str, verbose=False):\n",
        "    docx_handler = FileHandler(Docx2txtLoader, 'docx')\n",
        "    docs = docx_handler.load(docx_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found DOCX file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_xls_documents(xls_url: str, verbose=False):\n",
        "    xls_handler = FileHandler(UnstructuredExcelLoader, 'xls')\n",
        "    docs = xls_handler.load(xls_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found XLS file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_xlsx_documents(xlsx_url: str, verbose=False):\n",
        "    xlsx_handler = FileHandler(UnstructuredExcelLoader, 'xlsx')\n",
        "    docs = xlsx_handler.load(xlsx_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found XLSX file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_xml_documents(xml_url: str, verbose=False):\n",
        "    xml_handler = FileHandler(UnstructuredXMLLoader, 'xml')\n",
        "    docs = xml_handler.load(xml_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found XML file\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "class FileHandlerForGoogleDrive:\n",
        "    def __init__(self, file_loader, file_extension='docx'):\n",
        "        self.file_loader = file_loader\n",
        "        self.file_extension = file_extension\n",
        "\n",
        "    def load(self, url):\n",
        "\n",
        "        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n",
        "\n",
        "        try:\n",
        "            gdown.download(url=url, output=unique_filename, fuzzy=True)\n",
        "        except Exception as e:\n",
        "            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n",
        "            raise FileHandlerError(f\"No file content available\") from e\n",
        "\n",
        "        try:\n",
        "            loader = self.file_loader(file_path=unique_filename)\n",
        "        except Exception as e:\n",
        "            print(f\"No such file found at {unique_filename}\")\n",
        "            raise FileHandlerError(f\"No file found\", unique_filename) from e\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n",
        "            raise FileHandlerError(f\"No file content available\") from e\n",
        "\n",
        "        os.remove(unique_filename)\n",
        "\n",
        "        return documents\n",
        "\n",
        "def load_gdocs_documents(drive_folder_url: str, verbose=False):\n",
        "\n",
        "    gdocs_loader = FileHandlerForGoogleDrive(Docx2txtLoader)\n",
        "\n",
        "    docs = gdocs_loader.load(drive_folder_url)\n",
        "\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found Google Docs files\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_gsheets_documents(drive_folder_url: str, verbose=False):\n",
        "    gsheets_loader = FileHandlerForGoogleDrive(UnstructuredExcelLoader, 'xlsx')\n",
        "    docs = gsheets_loader.load(drive_folder_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found Google Sheets files\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_gslides_documents(drive_folder_url: str, verbose=False):\n",
        "    gslides_loader = FileHandlerForGoogleDrive(UnstructuredPowerPointLoader, 'pptx')\n",
        "    docs = gslides_loader.load(drive_folder_url)\n",
        "    if docs:\n",
        "\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found Google Slides files\")\n",
        "            print(f\"Splitting documents into {len(split_docs)} chunks\")\n",
        "\n",
        "        return split_docs\n",
        "\n",
        "def load_gpdf_documents(drive_folder_url: str, verbose=False):\n",
        "\n",
        "    gpdf_loader = FileHandlerForGoogleDrive(PyPDFLoader,'pdf')\n",
        "\n",
        "    docs = gpdf_loader.load(drive_folder_url)\n",
        "    if docs:\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Found Google PDF files\")\n",
        "            print(f\"Splitting documents into {len(docs)} chunks\")\n",
        "\n",
        "        return docs\n",
        "\n",
        "def load_docs_youtube_url(youtube_url: str, verbose=True) -> str:\n",
        "    try:\n",
        "        loader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=True)\n",
        "    except Exception as e:\n",
        "        print(f\"No such video found at {youtube_url}\")\n",
        "        raise VideoTranscriptError(f\"No video found\", youtube_url) from e\n",
        "\n",
        "    try:\n",
        "        docs = loader.load()\n",
        "        length = docs[0].metadata[\"length\"]\n",
        "        title = docs[0].metadata[\"title\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Video transcript might be private or unavailable in 'en' or the URL is incorrect.\")\n",
        "        raise VideoTranscriptError(f\"No video transcripts available\", youtube_url) from e\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Found video with title: {title} and length: {length}\")\n",
        "        print(f\"Combined documents into a single string.\")\n",
        "        print(f\"Beginning to process transcript...\")\n",
        "\n",
        "    split_docs = splitter.split_documents(docs)\n",
        "\n",
        "    return split_docs\n",
        "\n",
        "llm_for_img = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "def generate_docs_from_img(img_url, verbose: bool=False):\n",
        "    message = HumanMessage(\n",
        "    content=[\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"Give me a summary of what you see in the image. It must be 3 detailed paragraphs.\",\n",
        "            },\n",
        "            {\"type\": \"image_url\", \"image_url\": img_url},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = llm_for_img.invoke([message]).content\n",
        "        print(f\"Generated summary: {response}\")\n",
        "        docs = Document(page_content=response, metadata={\"source\": img_url})\n",
        "        split_docs = splitter.split_documents([docs])\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the request due to Invalid Content or Invalid Image URL\")\n",
        "        raise ImageHandlerError(f\"Error processing the request\", img_url) from e\n",
        "\n",
        "    return split_docs\n",
        "\n",
        "file_loader_map = {\n",
        "    FileType.PDF: load_pdf_documents,\n",
        "    FileType.CSV: load_csv_documents,\n",
        "    FileType.TXT: load_txt_documents,\n",
        "    FileType.MD: load_md_documents,\n",
        "    FileType.URL: load_url_documents,\n",
        "    FileType.PPTX: load_pptx_documents,\n",
        "    FileType.DOCX: load_docx_documents,\n",
        "    FileType.XLS: load_xls_documents,\n",
        "    FileType.XLSX: load_xlsx_documents,\n",
        "    FileType.XML: load_xml_documents,\n",
        "    FileType.GDOC: load_gdocs_documents,\n",
        "    FileType.GSHEET: load_gsheets_documents,\n",
        "    FileType.GSLIDE: load_gslides_documents,\n",
        "    FileType.GPDF: load_gpdf_documents,\n",
        "    FileType.YOUTUBE_URL: load_docs_youtube_url,\n",
        "    FileType.IMG: generate_docs_from_img\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GvXOxfIw1hb",
        "outputId": "fca04d8a-9d31-4ec2-96f6-1c095615f6d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing document_loaders.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf fpdf youtube-transcript-api pytube unstructured python-pptx docx2txt networkx pandas xlrd openpyxl gdown pytest PyPDF2 psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZbzCqhew_8C",
        "outputId": "0b0d6bc8-f617-4502-b31b-19ecfb20a726"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.15.12-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.13.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.25.9-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (10.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.1)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
            "  Downloading deepdiff-8.0.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.2)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Collecting orderly-set==5.2.2 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
            "  Downloading orderly_set-5.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Downloading pypdf-5.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.13.0-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.25.9-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepdiff-8.0.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orderly_set-5.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fpdf, docx2txt, langdetect\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=6d354b66947cfe529a881c0116d20cf785c558fac08b8495b1d9b7c0cefb0f42\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=eab9f827d1403e748fb43a8da76ace1a40fa05255ddf788be1ad9810c0aedf02\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=03e3e3f12ced42a09ea740c4e386fc9a9ed4714015a6698d5ffc405463e992d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built fpdf docx2txt langdetect\n",
            "Installing collected packages: fpdf, filetype, docx2txt, XlsxWriter, rapidfuzz, pytube, python-magic, python-iso639, PyPDF2, pypdf, orderly-set, olefile, langdetect, jsonpath-python, emoji, youtube-transcript-api, requests-toolbelt, python-pptx, python-oxmsg, deepdiff, unstructured-client, unstructured\n",
            "Successfully installed PyPDF2-3.0.1 XlsxWriter-3.2.0 deepdiff-8.0.1 docx2txt-0.8 emoji-2.13.0 filetype-1.2.0 fpdf-1.7.2 jsonpath-python-1.0.6 langdetect-1.0.9 olefile-0.47 orderly-set-5.2.2 pypdf-5.0.0 python-iso639-2024.4.27 python-magic-0.4.27 python-oxmsg-0.0.1 python-pptx-1.0.2 pytube-15.0.0 rapidfuzz-3.9.7 requests-toolbelt-1.0.0 unstructured-0.15.12 unstructured-client-0.25.9 youtube-transcript-api-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y nltk\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcCQDVURxJTt",
        "outputId": "7b993e6d-28a7-4f37-c7ca-70170d8b12a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.8.1\n",
            "Uninstalling nltk-3.8.1:\n",
            "  Successfully uninstalled nltk-3.8.1\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "Successfully installed nltk-3.9.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_file(file_path):\n",
        "    # Get the current working directory\n",
        "    script_dir = os.getcwd()\n",
        "\n",
        "    # Combine the script directory with the relative file path\n",
        "    absolute_file_path = os.path.join(script_dir, file_path)\n",
        "\n",
        "    try:\n",
        "        with open(absolute_file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        # Handle the case where the file is not found\n",
        "        print(f\"File not found: {absolute_file_path}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "5MV8CSGQxPd2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.schema import Document\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "IXgbNnqixjRV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "GbFrUXkQx93R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def build_vectorstore(documents: List[Document], embedding_model):\n",
        "  bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "  bm25_retriever.k = 2\n",
        "\n",
        "  faiss_vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "  faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "  ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n",
        "                                       weights=[0.5, 0.5])\n",
        "\n",
        "  return ensemble_retriever"
      ],
      "metadata": {
        "id": "MYv2Yv00yRWL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=SemanticAnalysisOutput)\n",
        "format_instructions = parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "Xcn69veoy4xq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "Perform a detailed semantic analysis of the following document. Extract and analyze key concepts, such as named entities, keywords, sentiment, and language. Ensure accuracy in entity recognition, keyword importance, and sentiment evaluation:\n",
        "\n",
        "Topic: {topic}\n",
        "\n",
        "Context:\n",
        "-----------------------------\n",
        "{context}\n",
        "\n",
        "Formatting:\n",
        "-----------------------------\n",
        "{format_instructions}\n",
        "\n",
        "Respond strictly according to the format instructions.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kbuFG43Sy7XJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"topic\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "qM0h3RImy8mp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | parser"
      ],
      "metadata": {
        "id": "t7Fj2Q-vy82J"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
      ],
      "metadata": {
        "id": "xuGKHoD8zOpb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from document_loaders import get_docs\n",
        "\n",
        "def run_chain(input_data: InputData):\n",
        "    documents = get_docs(input_data.file_url, input_data.file_type, verbose=True)\n",
        "    if not documents:\n",
        "        print(\"No documents loaded.\")\n",
        "        return\n",
        "\n",
        "    split_docs = splitter.split_documents(documents)\n",
        "\n",
        "    ensemble_retriever = build_vectorstore(split_docs, embedding_model)\n",
        "\n",
        "    user_query = \"\"\n",
        "\n",
        "    relevant_docs = ensemble_retriever.get_relevant_documents(user_query)\n",
        "\n",
        "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    output = chain.invoke({'topic': input_data.topic, 'context': context})\n",
        "\n",
        "    print(output)\n",
        "\n",
        "    del ensemble_retriever\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "5p_-4qMFy-iB"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_data = InputData(\n",
        "        topic=\"LLM\",\n",
        "        file_url=\"https://raw.github.com/AaronSosaRamos/mission-flights/main/files-for-test/text.pdf\",\n",
        "        file_type=\"pdf\",\n",
        "        lang=\"en\"\n",
        "    )\n",
        "\n",
        "    result = run_chain(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK2TkADl5AgH",
        "outputId": "ce84bed6-df9d-41e4-b87a-5a1305ee2b4f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found PDF file\n",
            "Splitting documents into 2 chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-ec99581ec25d>:15: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  relevant_docs = ensemble_retriever.get_relevant_documents(user_query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_text': 'inadvertently reflect societal prejudices. Additionally, LLMs require significa nt computational resources both for training and deployment, making them expensive to develop and maintain. Interpretability is another challenge, as it is often difficult to understand why a model produces certain outputs, making trust and transparency in AI-generated results more difficult to achieve. Despite these challenges, LLMs continue to have a transformative impact across numerous sectors. In healthcare, they assist in medical documentation and research, while in education, they are used to develop intelligent tutoring systems. Businesses are lev eraging LLMs to improve customer service through chatbots and virtual assistants. As research progresses, advancements in reducing biases, improving efficiency, and enhancing model transparency are expected to further broaden the potential applications of LLMs, solidifying their role in the future of AI -driven innovation. Large Language Models (LLMs) are a type of artificial intelligence designed to understand and generate human -like language. Built using deep learning architectures, specifically neural networks, LLMs are trained on enormous datasets comprising vast amounts of text. This training enables them to learn the intricacies of language, including syntax, semantics, and context, which allows them to perform tasks such as language translation, text summarization, and question answering. Their ability to model languag e at such a deep level has revolutionized the field of natural language processing (NLP) and expanded AI’s utility in various industries. However, LLMs are not without their limitations and challenges. One of the key concerns is the issue of bias, as these models learn from the data they are trained on, which can inadvertently reflect societal prejudices. Additionally, LLMs require significa nt', 'entities': [{'text': 'LLMs', 'type': 'organization', 'start_char': 0, 'end_char': 4, 'confidence': 0.95}, {'text': 'healthcare', 'type': 'sector', 'start_char': 164, 'end_char': 174, 'confidence': 0.85}, {'text': 'education', 'type': 'sector', 'start_char': 197, 'end_char': 206, 'confidence': 0.85}, {'text': 'AI', 'type': 'technology', 'start_char': 292, 'end_char': 294, 'confidence': 0.9}, {'text': 'natural language processing (NLP)', 'type': 'field', 'start_char': 569, 'end_char': 605, 'confidence': 0.8}], 'keywords': [{'word': 'LLMs', 'importance': 0.9}, {'word': 'bias', 'importance': 0.8}, {'word': 'interpretability', 'importance': 0.7}, {'word': 'transformative impact', 'importance': 0.75}, {'word': 'artificial intelligence', 'importance': 0.85}, {'word': 'deep learning', 'importance': 0.7}, {'word': 'natural language processing', 'importance': 0.8}], 'sentiment': {'polarity': 0.2, 'subjectivity': 0.6}, 'language': 'English', 'confidence': 0.9, 'summary': 'The document discusses the challenges and transformative impacts of Large Language Models (LLMs) in various sectors, highlighting issues such as bias, interpretability, and the need for significant computational resources.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhqghFVI70K4",
        "outputId": "0b65243d-1480-4249-956c-8a4fe24fcfeb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_text': 'inadvertently reflect societal prejudices. Additionally, LLMs require significa nt computational resources both for training and deployment, making them expensive to develop and maintain. Interpretability is another challenge, as it is often difficult to understand why a model produces certain outputs, making trust and transparency in AI-generated results more difficult to achieve. Despite these challenges, LLMs continue to have a transformative impact across numerous sectors. In healthcare, they assist in medical documentation and research, while in education, they are used to develop intelligent tutoring systems. Businesses are lev eraging LLMs to improve customer service through chatbots and virtual assistants. As research progresses, advancements in reducing biases, improving efficiency, and enhancing model transparency are expected to further broaden the potential applications of LLMs, solidifying their role in the future of AI -driven innovation. Large Language Models (LLMs) are a type of artificial intelligence designed to understand and generate human -like language. Built using deep learning architectures, specifically neural networks, LLMs are trained on enormous datasets comprising vast amounts of text. This training enables them to learn the intricacies of language, including syntax, semantics, and context, which allows them to perform tasks such as language translation, text summarization, and question answering. Their ability to model languag e at such a deep level has revolutionized the field of natural language processing (NLP) and expanded AI’s utility in various industries. However, LLMs are not without their limitations and challenges. One of the key concerns is the issue of bias, as these models learn from the data they are trained on, which can inadvertently reflect societal prejudices. Additionally, LLMs require significa nt',\n",
              " 'entities': [{'text': 'LLMs',\n",
              "   'type': 'organization',\n",
              "   'start_char': 0,\n",
              "   'end_char': 4,\n",
              "   'confidence': 0.95},\n",
              "  {'text': 'healthcare',\n",
              "   'type': 'sector',\n",
              "   'start_char': 164,\n",
              "   'end_char': 174,\n",
              "   'confidence': 0.85},\n",
              "  {'text': 'education',\n",
              "   'type': 'sector',\n",
              "   'start_char': 197,\n",
              "   'end_char': 206,\n",
              "   'confidence': 0.85},\n",
              "  {'text': 'AI',\n",
              "   'type': 'technology',\n",
              "   'start_char': 292,\n",
              "   'end_char': 294,\n",
              "   'confidence': 0.9},\n",
              "  {'text': 'natural language processing (NLP)',\n",
              "   'type': 'field',\n",
              "   'start_char': 569,\n",
              "   'end_char': 605,\n",
              "   'confidence': 0.8}],\n",
              " 'keywords': [{'word': 'LLMs', 'importance': 0.9},\n",
              "  {'word': 'bias', 'importance': 0.8},\n",
              "  {'word': 'interpretability', 'importance': 0.7},\n",
              "  {'word': 'transformative impact', 'importance': 0.75},\n",
              "  {'word': 'artificial intelligence', 'importance': 0.85},\n",
              "  {'word': 'deep learning', 'importance': 0.7},\n",
              "  {'word': 'natural language processing', 'importance': 0.8}],\n",
              " 'sentiment': {'polarity': 0.2, 'subjectivity': 0.6},\n",
              " 'language': 'English',\n",
              " 'confidence': 0.9,\n",
              " 'summary': 'The document discusses the challenges and transformative impacts of Large Language Models (LLMs) in various sectors, highlighting issues such as bias, interpretability, and the need for significant computational resources.'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}