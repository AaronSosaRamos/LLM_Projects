{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlCDWksuqk1oPORKrORHCG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":23,"metadata":{"id":"bYhk-U8UDw9Q","executionInfo":{"status":"ok","timestamp":1726518276238,"user_tz":300,"elapsed":518,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"outputs":[],"source":["from pydantic import BaseModel, Field\n","from enum import Enum\n","from typing import Literal\n","\n","class FileType(Enum):\n","    PDF = 'pdf'\n","    CSV = 'csv'\n","    TXT = 'txt'\n","    MD = 'md'\n","    URL = 'url'\n","    PPTX = 'pptx'\n","    DOCX = 'docx'\n","    XLS = 'xls'\n","    XLSX = 'xlsx'\n","    XML = 'xml'\n","    GDOC = 'gdoc'\n","    GSHEET = 'gsheet'\n","    GSLIDE = 'gslide'\n","    GPDF = 'gpdf'\n","    YOUTUBE_URL = 'youtube_url'\n","    IMG = 'img'\n"]},{"cell_type":"code","source":["class AIResistantArgs(BaseModel):\n","    topic: str = Field(..., min_length=1, max_length=255, description=\"Topic or subject related to the content\")\n","    assignment: str = Field(..., min_length=1, max_length=255, description=\"The given assignment\")\n","    grade_level: Literal[\"elementary\", \"middle\", \"high\", \"college\", \"professional\"] = Field(..., description=\"Educational level to which the content is directed\")\n","    file_type: str = Field(..., description=\"Type of file being handled, according to the defined enumeration\")\n","    file_url: str = Field(..., description=\"URL or path of the file to be processed\")\n","    lang: str = Field(..., description=\"Language in which the file or content is written\")"],"metadata":{"id":"wq58Ib1sD1jx","executionInfo":{"status":"ok","timestamp":1726517344533,"user_tz":300,"elapsed":377,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["input_data = AIResistantArgs(\n","    topic=\"Introduction to Data Science\",\n","    assignment=\"Develop 4 algorithms for DS\",\n","    grade_level=\"college\",\n","    file_type=\"pdf\",\n","    file_url=\"https://example.com/introduction_to_data_science.pdf\",\n","    lang=\"en\"\n",")\n","\n","print(f\"Topic: {input_data.topic}\")\n","print(f\"Grade Level: {input_data.grade_level}\")\n","print(f\"File Type: {input_data.file_type}\")\n","print(f\"File URL: {input_data.file_url}\")\n","print(f\"Language: {input_data.lang}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgKnWZ30D81Y","executionInfo":{"status":"ok","timestamp":1726517352788,"user_tz":300,"elapsed":350,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"183c3e22-428a-43e2-f213-73534d0e0b3b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic: Introduction to Data Science\n","Grade Level: college\n","File Type: pdf\n","File URL: https://example.com/introduction_to_data_science.pdf\n","Language: en\n"]}]},{"cell_type":"code","source":["from typing import List\n","\n","class AIResistanceIdea(BaseModel):\n","    assignment_description: str = Field(..., description=\"Detailed description of the modified assignment\")\n","    explanation: str = Field(..., description=\"Explanation of how this modification makes the assignment AI-resistant\")\n","\n","class AIResistantOutput(BaseModel):\n","    topic: str = Field(..., description=\"Topic or subject related to the assignment\")\n","    grade_level: str = Field(..., description=\"Educational level to which the assignment is directed\")\n","    ideas: List[AIResistanceIdea] = Field(..., description=\"List of 3 ideas to make the assignment AI-resistant, including explanation\")"],"metadata":{"id":"NP-tKIzCGqRr","executionInfo":{"status":"ok","timestamp":1726517468773,"user_tz":300,"elapsed":420,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class FileHandlerError(Exception):\n","    \"\"\"Raised when a file content cannot be loaded. Used for tools which require file handling.\"\"\"\n","    def __init__(self, message, url=None):\n","        self.message = message\n","        self.url = url\n","        super().__init__(self.message)\n","\n","    def __str__(self):\n","        return f\"{self.message}\"\n","\n","class ImageHandlerError(Exception):\n","    \"\"\"Raised when an image cannot be loaded. Used for tools which require image handling.\"\"\"\n","    def __init__(self, message, url):\n","        self.message = message\n","        self.url = url\n","        super().__init__(self.message)\n","\n","    def __str__(self):\n","        return f\"{self.message}\"\n","\n","class VideoTranscriptError(Exception):\n","    \"\"\"Raised when a video transcript cannot be loaded. Used for tools which require video transcripts.\"\"\"\n","    def __init__(self, message, url):\n","        self.message = message\n","        self.url = url\n","        super().__init__(self.message)\n","\n","    def __str__(self):\n","        return f\"{self.message}\""],"metadata":{"id":"lt57NUA1f4vS","executionInfo":{"status":"ok","timestamp":1726518147926,"user_tz":300,"elapsed":511,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["%%writefile document_loaders.py\n","from langchain_community.document_loaders import YoutubeLoader, PyPDFLoader, TextLoader, UnstructuredURLLoader, UnstructuredPowerPointLoader, Docx2txtLoader, UnstructuredExcelLoader, UnstructuredXMLLoader\n","from langchain_community.document_loaders.csv_loader import CSVLoader\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.messages import HumanMessage\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_core.documents import Document\n","import os\n","import tempfile\n","import uuid\n","import requests\n","import gdown\n","from enum import Enum\n","\n","class FileType(Enum):\n","    PDF = 'pdf'\n","    CSV = 'csv'\n","    TXT = 'txt'\n","    MD = 'md'\n","    URL = 'url'\n","    PPTX = 'pptx'\n","    DOCX = 'docx'\n","    XLS = 'xls'\n","    XLSX = 'xlsx'\n","    XML = 'xml'\n","    GDOC = 'gdoc'\n","    GSHEET = 'gsheet'\n","    GSLIDE = 'gslide'\n","    GPDF = 'gpdf'\n","    YOUTUBE_URL = 'youtube_url'\n","    IMG = 'img'\n","\n","STRUCTURED_TABULAR_FILE_EXTENSIONS = {\"csv\", \"xls\", \"xlsx\", \"gsheet\", \"xml\"}\n","\n","splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 1000,\n","    chunk_overlap = 100\n",")\n","\n","def read_text_file(file_path):\n","    # Get the directory containing the script file\n","    script_dir = os.path.dirname(os.path.abspath(__file__))\n","\n","    # Combine the script directory with the relative file path\n","    absolute_file_path = os.path.join(script_dir, file_path)\n","\n","    with open(absolute_file_path, 'r') as file:\n","        return file.read()\n","\n","def get_docs(file_url: str, file_type: str, verbose=True):\n","    file_type = file_type.lower()\n","    try:\n","        file_loader = file_loader_map[FileType(file_type)]\n","        docs = file_loader(file_url, verbose)\n","\n","        return docs\n","\n","    except Exception as e:\n","        print(e)\n","        print(f\"Unsupported file type: {file_type}\")\n","        raise FileHandlerError(f\"Unsupported file type\", file_url) from e\n","\n","class FileHandler:\n","    def __init__(self, file_loader, file_extension):\n","        self.file_loader = file_loader\n","        self.file_extension = file_extension\n","\n","    def load(self, url):\n","        # Generate a unique filename with a UUID prefix\n","        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n","\n","        # Download the file from the URL and save it to a temporary file\n","        response = requests.get(url)\n","        response.raise_for_status()  # Ensure the request was successful\n","\n","        with tempfile.NamedTemporaryFile(delete=False, prefix=unique_filename) as temp_file:\n","            temp_file.write(response.content)\n","            temp_file_path = temp_file.name\n","\n","        # Use the file_loader to load the documents\n","        try:\n","            loader = self.file_loader(file_path=temp_file_path)\n","        except Exception as e:\n","            print(f\"No such file found at {temp_file_path}\")\n","            raise FileHandlerError(f\"No file found\", temp_file_path) from e\n","\n","        try:\n","            documents = loader.load()\n","        except Exception as e:\n","            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n","            raise FileHandlerError(f\"No file content available\", temp_file_path) from e\n","\n","        # Remove the temporary file\n","        os.remove(temp_file_path)\n","\n","        return documents\n","\n","def load_pdf_documents(pdf_url: str, verbose=False):\n","    pdf_loader = FileHandler(PyPDFLoader, \"pdf\")\n","    docs = pdf_loader.load(pdf_url)\n","\n","    if docs:\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found PDF file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_csv_documents(csv_url: str, verbose=False):\n","    csv_loader = FileHandler(CSVLoader, \"csv\")\n","    docs = csv_loader.load(csv_url)\n","\n","    if docs:\n","        if verbose:\n","            print(f\"Found CSV file\")\n","            print(f\"Splitting documents into {len(docs)} chunks\")\n","\n","        return docs\n","\n","def load_txt_documents(notes_url: str, verbose=False):\n","    notes_loader = FileHandler(TextLoader, \"txt\")\n","    docs = notes_loader.load(notes_url)\n","\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found TXT file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_md_documents(notes_url: str, verbose=False):\n","    notes_loader = FileHandler(TextLoader, \"md\")\n","    docs = notes_loader.load(notes_url)\n","\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found MD file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_url_documents(url: str, verbose=False):\n","    url_loader = UnstructuredURLLoader(urls=[url])\n","    docs = url_loader.load()\n","\n","    if docs:\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found URL\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_pptx_documents(pptx_url: str, verbose=False):\n","    pptx_handler = FileHandler(UnstructuredPowerPointLoader, 'pptx')\n","\n","    docs = pptx_handler.load(pptx_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found PPTX file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_docx_documents(docx_url: str, verbose=False):\n","    docx_handler = FileHandler(Docx2txtLoader, 'docx')\n","    docs = docx_handler.load(docx_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found DOCX file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_xls_documents(xls_url: str, verbose=False):\n","    xls_handler = FileHandler(UnstructuredExcelLoader, 'xls')\n","    docs = xls_handler.load(xls_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found XLS file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_xlsx_documents(xlsx_url: str, verbose=False):\n","    xlsx_handler = FileHandler(UnstructuredExcelLoader, 'xlsx')\n","    docs = xlsx_handler.load(xlsx_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found XLSX file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_xml_documents(xml_url: str, verbose=False):\n","    xml_handler = FileHandler(UnstructuredXMLLoader, 'xml')\n","    docs = xml_handler.load(xml_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found XML file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","class FileHandlerForGoogleDrive:\n","    def __init__(self, file_loader, file_extension='docx'):\n","        self.file_loader = file_loader\n","        self.file_extension = file_extension\n","\n","    def load(self, url):\n","\n","        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n","\n","        try:\n","            gdown.download(url=url, output=unique_filename, fuzzy=True)\n","        except Exception as e:\n","            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n","            raise FileHandlerError(f\"No file content available\") from e\n","\n","        try:\n","            loader = self.file_loader(file_path=unique_filename)\n","        except Exception as e:\n","            print(f\"No such file found at {unique_filename}\")\n","            raise FileHandlerError(f\"No file found\", unique_filename) from e\n","\n","        try:\n","            documents = loader.load()\n","        except Exception as e:\n","            print(f\"File content might be private or unavailable or the URL is incorrect.\")\n","            raise FileHandlerError(f\"No file content available\") from e\n","\n","        os.remove(unique_filename)\n","\n","        return documents\n","\n","def load_gdocs_documents(drive_folder_url: str, verbose=False):\n","\n","    gdocs_loader = FileHandlerForGoogleDrive(Docx2txtLoader)\n","\n","    docs = gdocs_loader.load(drive_folder_url)\n","\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found Google Docs files\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_gsheets_documents(drive_folder_url: str, verbose=False):\n","    gsheets_loader = FileHandlerForGoogleDrive(UnstructuredExcelLoader, 'xlsx')\n","    docs = gsheets_loader.load(drive_folder_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found Google Sheets files\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_gslides_documents(drive_folder_url: str, verbose=False):\n","    gslides_loader = FileHandlerForGoogleDrive(UnstructuredPowerPointLoader, 'pptx')\n","    docs = gslides_loader.load(drive_folder_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found Google Slides files\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_gpdf_documents(drive_folder_url: str, verbose=False):\n","\n","    gpdf_loader = FileHandlerForGoogleDrive(PyPDFLoader,'pdf')\n","\n","    docs = gpdf_loader.load(drive_folder_url)\n","    if docs:\n","\n","        if verbose:\n","            print(f\"Found Google PDF files\")\n","            print(f\"Splitting documents into {len(docs)} chunks\")\n","\n","        return docs\n","\n","def load_docs_youtube_url(youtube_url: str, verbose=True) -> str:\n","    try:\n","        loader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=True)\n","    except Exception as e:\n","        print(f\"No such video found at {youtube_url}\")\n","        raise VideoTranscriptError(f\"No video found\", youtube_url) from e\n","\n","    try:\n","        docs = loader.load()\n","        length = docs[0].metadata[\"length\"]\n","        title = docs[0].metadata[\"title\"]\n","\n","    except Exception as e:\n","        print(f\"Video transcript might be private or unavailable in 'en' or the URL is incorrect.\")\n","        raise VideoTranscriptError(f\"No video transcripts available\", youtube_url) from e\n","\n","    if verbose:\n","        print(f\"Found video with title: {title} and length: {length}\")\n","        print(f\"Combined documents into a single string.\")\n","        print(f\"Beginning to process transcript...\")\n","\n","    split_docs = splitter.split_documents(docs)\n","\n","    return split_docs\n","\n","llm_for_img = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n","\n","def generate_docs_from_img(img_url, verbose: bool=False):\n","    message = HumanMessage(\n","    content=[\n","            {\n","                \"type\": \"text\",\n","                \"text\": \"Give me a summary of what you see in the image. It must be 3 detailed paragraphs.\",\n","            },\n","            {\"type\": \"image_url\", \"image_url\": img_url},\n","        ]\n","    )\n","\n","    try:\n","        response = llm_for_img.invoke([message]).content\n","        print(f\"Generated summary: {response}\")\n","        docs = Document(page_content=response, metadata={\"source\": img_url})\n","        split_docs = splitter.split_documents([docs])\n","    except Exception as e:\n","        print(f\"Error processing the request due to Invalid Content or Invalid Image URL\")\n","        raise ImageHandlerError(f\"Error processing the request\", img_url) from e\n","\n","    return split_docs\n","\n","file_loader_map = {\n","    FileType.PDF: load_pdf_documents,\n","    FileType.CSV: load_csv_documents,\n","    FileType.TXT: load_txt_documents,\n","    FileType.MD: load_md_documents,\n","    FileType.URL: load_url_documents,\n","    FileType.PPTX: load_pptx_documents,\n","    FileType.DOCX: load_docx_documents,\n","    FileType.XLS: load_xls_documents,\n","    FileType.XLSX: load_xlsx_documents,\n","    FileType.XML: load_xml_documents,\n","    FileType.GDOC: load_gdocs_documents,\n","    FileType.GSHEET: load_gsheets_documents,\n","    FileType.GSLIDE: load_gslides_documents,\n","    FileType.GPDF: load_gpdf_documents,\n","    FileType.YOUTUBE_URL: load_docs_youtube_url,\n","    FileType.IMG: generate_docs_from_img\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJY1RJpyajhv","executionInfo":{"status":"ok","timestamp":1726518302980,"user_tz":300,"elapsed":327,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"fcdec1eb-beb9-40bd-c895-14b8c4bbed11"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting document_loaders.py\n"]}]},{"cell_type":"code","source":["!pip install langchain langchain-core langchain-google-genai langchain_community langchain-chroma chroma"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBrc3b2GbCL0","executionInfo":{"status":"ok","timestamp":1726516981452,"user_tz":300,"elapsed":41706,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"cd303aef-efe7-468b-a6b1-537987863f2f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core\n","  Downloading langchain_core-0.3.0-py3-none-any.whl.metadata (6.2 kB)\n","Collecting langchain-google-genai\n","  Downloading langchain_google_genai-2.0.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting langchain_community\n","  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n","Collecting langchain-chroma\n","  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n","Collecting chroma\n","  Downloading Chroma-0.2.0.tar.gz (5.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n","  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.121-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n","  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n","Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.7.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n","  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n","Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma)\n","  Downloading chromadb-0.5.3-py3-none-any.whl.metadata (6.8 kB)\n","Collecting fastapi<1,>=0.95.2 (from langchain-chroma)\n","  Downloading fastapi-0.114.2-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n","Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n","Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading posthog-3.6.6-py2.py3-none-any.whl.metadata (2.0 kB)\n","Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.19.1)\n","Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.66.5)\n","Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.64.1)\n","Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.5)\n","Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Collecting orjson>=3.9.12 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx>=0.27.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1,>=0.95.2->langchain-chroma)\n","  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.6)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.19.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.137.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.27.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.20.3)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.24.0)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.1.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.65.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.7.1)\n","Collecting httpcore==1.* (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.8.2)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.13.2)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n","Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (71.0.4)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.24.7)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.8.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.1.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.48.2)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.16.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2024.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.16.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n","Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.0-py3-none-any.whl (405 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_google_genai-2.0.0-py3-none-any.whl (39 kB)\n","Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n","Downloading chromadb-0.5.3-py3-none-any.whl (559 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading fastapi-0.114.2-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n","Downloading langsmith-0.1.121-py3-none-any.whl (289 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n","Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n","Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n","Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n","Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n","Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n","Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n","Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n","Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: chroma, pypika\n","  Building wheel for chroma (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chroma: filename=Chroma-0.2.0-py3-none-any.whl size=7095 sha256=b17e8838bf901b941fde0a6b812a876764aba0e92a55da5a018d5cfe545a04d2\n","  Stored in directory: /root/.cache/pip/wheels/58/74/75/a6ab7999ae473ecbe819bc5cae9ccb902429dd6c60795f5112\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=bc0880caf6f1096ba13858c19ac3ee55616f6cc9810a29c4a562c9d47b2863e6\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built chroma pypika\n","Installing collected packages: pypika, monotonic, mmh3, chroma, websockets, uvloop, tenacity, python-dotenv, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, httpcore, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, langsmith, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, langchain-text-splitters, chromadb, langchain-google-genai, langchain-chroma, langchain, langchain_community\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib_metadata 8.5.0\n","    Uninstalling importlib_metadata-8.5.0:\n","      Successfully uninstalled importlib_metadata-8.5.0\n","Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-0.2.0 chroma-hnswlib-0.7.3 chromadb-0.5.3 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 fastapi-0.114.2 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.2 humanfriendly-10.0 importlib-metadata-8.4.0 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-30.1.0 langchain-0.3.0 langchain-chroma-0.1.4 langchain-core-0.3.0 langchain-google-genai-2.0.0 langchain-text-splitters-0.3.0 langchain_community-0.3.0 langsmith-0.1.121 marshmallow-3.22.0 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 orjson-3.10.7 overrides-7.7.0 posthog-3.6.6 pydantic-settings-2.5.2 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.38.5 tenacity-8.5.0 typing-inspect-0.9.0 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.0.1\n"]}]},{"cell_type":"code","source":["!pip install pypdf fpdf youtube-transcript-api pytube unstructured python-pptx docx2txt networkx pandas xlrd openpyxl gdown pytest PyPDF2 psutil"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xeLE9ZYrbc2E","executionInfo":{"status":"ok","timestamp":1726517032061,"user_tz":300,"elapsed":20321,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"8b881bcc-2a2d-49b2-c098-9ddfd760f383"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting fpdf\n","  Downloading fpdf-1.7.2.tar.gz (39 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting youtube-transcript-api\n","  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n","Collecting pytube\n","  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting unstructured\n","  Downloading unstructured-0.15.12-py3-none-any.whl.metadata (29 kB)\n","Collecting python-pptx\n","  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n","Collecting docx2txt\n","  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n","Collecting filetype (from unstructured)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting python-magic (from unstructured)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n","Collecting emoji (from unstructured)\n","  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n","Collecting python-iso639 (from unstructured)\n","  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n","Collecting langdetect (from unstructured)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n","Collecting rapidfuzz (from unstructured)\n","  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n","Collecting unstructured-client (from unstructured)\n","  Downloading unstructured_client-0.25.8-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.5)\n","Collecting python-oxmsg (from unstructured)\n","  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (9.4.0)\n","Collecting XlsxWriter>=0.5.7 (from python-pptx)\n","  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.0)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.1)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.5.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.2)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.22.0)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.5.15)\n","Collecting olefile (from python-oxmsg->unstructured)\n","  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.8.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n","  Downloading deepdiff-8.0.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.2)\n","Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n","  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n","Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n","Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n","Collecting orderly-set==5.2.2 (from deepdiff>=6.0->unstructured-client->unstructured)\n","  Downloading orderly_set-5.2.2-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n","Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n","Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured-0.15.12-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading emoji-2.12.1-py3-none-any.whl (431 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n","Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_client-0.25.8-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deepdiff-8.0.1-py3-none-any.whl (82 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orderly_set-5.2.2-py3-none-any.whl (11 kB)\n","Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n","Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: fpdf, docx2txt, langdetect\n","  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=902d135b274c0085d2fc06907b565debd758effe59d543f895cfe520d46c07b9\n","  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n","  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=374dc923ddefc186fdb4b446ef433b8be82dfc9eac5179965eca317f9bc70af5\n","  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=7cee250b1a785e131cfd8b3f5573032505e7046bbfbc758e95e8b8970c36c387\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built fpdf docx2txt langdetect\n","Installing collected packages: fpdf, filetype, docx2txt, XlsxWriter, rapidfuzz, pytube, python-magic, python-iso639, PyPDF2, pypdf, orderly-set, olefile, langdetect, jsonpath-python, emoji, youtube-transcript-api, requests-toolbelt, python-pptx, python-oxmsg, deepdiff, unstructured-client, unstructured\n","Successfully installed PyPDF2-3.0.1 XlsxWriter-3.2.0 deepdiff-8.0.1 docx2txt-0.8 emoji-2.12.1 filetype-1.2.0 fpdf-1.7.2 jsonpath-python-1.0.6 langdetect-1.0.9 olefile-0.47 orderly-set-5.2.2 pypdf-4.3.1 python-iso639-2024.4.27 python-magic-0.4.27 python-oxmsg-0.0.1 python-pptx-1.0.2 pytube-15.0.0 rapidfuzz-3.9.7 requests-toolbelt-1.0.0 unstructured-0.15.12 unstructured-client-0.25.8 youtube-transcript-api-0.6.2\n"]}]},{"cell_type":"code","source":["!pip uninstall -y nltk\n","!pip install nltk\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YudLqXW5cG0M","executionInfo":{"status":"ok","timestamp":1726517126040,"user_tz":300,"elapsed":11586,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"bff4fa91-4a45-4e45-e768-fef8be4bdc96"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: nltk 3.8.1\n","Uninstalling nltk-3.8.1:\n","  Successfully uninstalled nltk-3.8.1\n","Collecting nltk\n","  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nltk\n","Successfully installed nltk-3.9.1\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from typing import List, Dict\n","import os\n","\n","from langchain_core.documents import Document\n","from langchain_chroma import Chroma\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n","from langchain_core.output_parsers import JsonOutputParser\n","from pydantic import BaseModel, Field\n","from langchain_google_genai import GoogleGenerativeAI\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings"],"metadata":{"id":"860zrNtSeU9V","executionInfo":{"status":"ok","timestamp":1726517735426,"user_tz":300,"elapsed":356,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def read_text_file(file_path):\n","    # Get the current working directory\n","    script_dir = os.getcwd()\n","\n","    # Combine the script directory with the relative file path\n","    absolute_file_path = os.path.join(script_dir, file_path)\n","\n","    try:\n","        with open(absolute_file_path, 'r') as file:\n","            content = file.read()\n","        return content\n","    except FileNotFoundError:\n","        # Handle the case where the file is not found\n","        print(f\"File not found: {absolute_file_path}\")\n","        return None"],"metadata":{"id":"Yya8xaqocHWU","executionInfo":{"status":"ok","timestamp":1726520495555,"user_tz":300,"elapsed":405,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["class QuizBuilder:\n","    def __init__(self, args=None, vectorstore_class=Chroma, prompt=None, embedding_model=None, model=None, parser=None, verbose=False):\n","        default_config = {\n","            \"model\": GoogleGenerativeAI(model=\"gemini-1.5-flash\"),\n","            \"embedding_model\": GoogleGenerativeAIEmbeddings(model='models/embedding-001'),\n","            \"parser\": JsonOutputParser(pydantic_object=AIResistantOutput),\n","            \"prompt\": read_text_file(\"prompt/ai-resistant-prompt.txt\"),\n","            \"vectorstore_class\": Chroma\n","        }\n","\n","        self.prompt = prompt or default_config[\"prompt\"]\n","        self.model = model or default_config[\"model\"]\n","        self.parser = parser or default_config[\"parser\"]\n","        self.embedding_model = embedding_model or default_config[\"embedding_model\"]\n","\n","        self.vectorstore_class = vectorstore_class or default_config[\"vectorstore_class\"]\n","        self.vectorstore, self.retriever, self.runner = None, None, None\n","        self.args = args\n","        self.verbose = verbose\n","\n","        if vectorstore_class is None: raise ValueError(\"Vectorstore must be provided\")\n","        if args.topic is None: raise ValueError(\"Topic must be provided\")\n","        if args.assignment is None: raise ValueError(\"Assignment must be provided\")\n","        if args.grade_level is None: raise ValueError(\"Grade Level must be provided\")\n","        if args.grade_level is None: raise ValueError(\"Language must be provided\")\n","\n","\n","    def compile(self, documents: List[Document]):\n","        # Return the chain\n","        prompt = PromptTemplate(\n","            template=self.prompt,\n","            input_variables=[\"attribute_collection\"],\n","            partial_variables={\"format_instructions\": self.parser.get_format_instructions()}\n","        )\n","\n","        if self.runner is None:\n","            print(f\"Creating vectorstore from {len(documents)} documents\") if self.verbose else None\n","            self.vectorstore = self.vectorstore_class.from_documents(documents, self.embedding_model)\n","            print(f\"Vectorstore created\") if self.verbose else None\n","\n","            self.retriever = self.vectorstore.as_retriever()\n","            print(f\"Retriever created successfully\") if self.verbose else None\n","\n","            self.runner = RunnableParallel(\n","                {\"context\": self.retriever,\n","                \"attribute_collection\": RunnablePassthrough()\n","                }\n","            )\n","\n","        chain = self.runner | prompt | self.model | self.parser\n","\n","        if self.verbose: print(f\"Chain compilation complete\")\n","\n","        return chain\n","\n","    def create_assignments(self, documents: List[Document]):\n","        if self.verbose: print(f\"Creating the AI-Resistant assignments\")\n","\n","        chain = self.compile(documents)\n","\n","        response = chain.invoke(f\"Topic: {self.args.topic}, Assignment: {self.args.assignment}, Grade Level: {self.args.grade_level}, Lang: {self.args.lang}\")\n","\n","        if self.verbose: print(f\"Deleting vectorstore\")\n","        self.vectorstore.delete_collection()\n","\n","        return response"],"metadata":{"id":"aSU_tiVdcP4z","executionInfo":{"status":"ok","timestamp":1726520628174,"user_tz":300,"elapsed":316,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"],"metadata":{"id":"LL5YelHHgs3x","executionInfo":{"status":"ok","timestamp":1726518345052,"user_tz":300,"elapsed":4604,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["from document_loaders import get_docs\n","\n","input_data = AIResistantArgs(\n","    topic=\"Introduction to Data Science\",\n","    assignment=\"Develop a Neural Network Architecture for emulating Data Science Workflow\",\n","    grade_level=\"college\",\n","    file_type=\"pdf\",\n","    file_url=\"http://ijsmsjournal.org/2021/volume-4%20issue-4/ijsms-v4i4p137.pdf\",\n","    lang=\"en\"\n",")\n","\n","docs = get_docs(input_data.file_url, input_data.file_type, True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQpiy1Aqep3H","executionInfo":{"status":"ok","timestamp":1726518473388,"user_tz":300,"elapsed":2331,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"5d53290c-d2e8-48c7-8e75-fb4ec452ebfc"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Found PDF file\n","Splitting documents into 13 chunks\n"]}]},{"cell_type":"code","source":["docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LaNxkHb0hT4j","executionInfo":{"status":"ok","timestamp":1726520502320,"user_tz":300,"elapsed":420,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"048fc2e6-c959-456f-86b3-5a32b91920a6"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 0}, page_content='International Journal of Science and Management Studies (IJSMS)                        E-ISSN: 2581 -5946  \\nDOI: 10.51386/25815946/ijsms -v4i4p13 7 \\nVolume: 4 Issue: 4                                   July to August 2021                             https://www.ijsmsjournal.org  \\n  \\n \\n                                       This is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by -nc-nd/4.0/ )      Page 407  Introduction to Data Science - An Overview  \\nThangakumar J eyaprakash#1, Padmaveni K#2   \\n#1#2Associate Professor and Department of Computer Science and Engineering  \\n#1#2Hindustan Institute of Technology and Science, India  \\n \\nAbstract Data science plays a vital role in the research field of computer science and engineering which \\ninvolves collection of data, transformation, processing, describing,  and modelling. In this article, fundamental'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 0}, page_content='theory of Data Science, Machine learning and De ep Learning with the scope and opportunities has been \\ndiscussed. This helps the researchers to get a clarity on data science and its importance.  \\n \\nKeywords — Data science, Machine learning, Deep Learning  \\n \\nI. INTRODUCTION  \\nData science is the science of collecting, storing, processing, describing, and modelling of data  \\n \\n \\nFig 1. Five tasks of Data science  \\nII. DATA COLLECTION : \\n \\n         Data scientist must be work on the following questions to collect the data  \\n\\uf0b7 What to collect?  \\n\\uf0b7 How to collect?  \\n\\uf0b7 Where to collect?  \\n\\uf0b7 Label data or not?  \\nSuppose a data scientist must collect data [1] on shopping mart, he needs to identify the items need to be \\npurchased together to do some statistical modelling [2]. For example, a mobile phone may be purchased along \\nwith SD card, charge r etc. a laptop may be purchased together with laptop bag, and wireless mouse. Such data'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 0}, page_content='helps the data scientist to do some statistics like which items do customer purchase together? Or which items \\nmoving faster etc. Data scientist should have the knowled ge of structured query language or python packages to \\ndo analytics.   \\nIII. DATA STORAGE  \\n \\n The collected data would be a transactional or operational data [3] such as an insurance schemes, stock \\ninventory, customer records, employee records, student records, pat ients records, purchase orders, stock market, \\nschool children record etc.'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 1}, page_content='International Journal of Science and Management Studies (IJSMS)                        E-ISSN: 2581 -5946  \\nDOI: 10.51386/25815946/ijsms -v4i4p13 7 \\nVolume: 4 Issue: 4                                   July to August 2021                             https://www.ijsmsjournal.org  \\n  \\n \\n                                       This is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by -nc-nd/4.0/ )      Page 408   \\nFig 2: Data records  \\n \\nThese data can be divided into structured data and unstructured data [4]. Data in a structured format or readily \\navailable for analytics collected from multiple relational database management system (RDBMS) are known as \\nstructured data which can be integra ted into a common repository to support data analytics. The unstructured \\ndata referred as text, image, voice data, data from social blogs to do some sentimental analysis. The data which'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 1}, page_content='are unstructured needs to be cleaned or pre -processed to support analy tics. A large collection of data storage \\nwith both structured and unstructured data is known as data lake referred to as a Big Data. It consists of \\nuncurated, useful or the data is not useful, and the data from everywhere.  \\nIV. DATA PROCESSING  \\n \\nThe data proces sing involves data wrangling, data cleaning and data scaling  \\n \\n \\nFigure 3: Data Processing  \\n \\nA. Data wrangling or Data Munging  \\nData wrangling involves the extraction, transformation and load known as ETL process. The extraction of \\ndata from one database to another will be transformed into another form to support analytics which will be \\nloaded to the data warehouse is known as Data  wrangling or Munging. Some of the ETL [5] tools are IBM \\ninfosphere, informatica, Oracle data integrator, Alooma, Fivetran, Snaplogic etc.  \\n \\nB. Data Cleaning  \\nWhen extracting data from multiple databases, some values may be missed in columns. To fill such missing'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 1}, page_content='values, to standardize the keyword tags, to correct the spelling errors or to identify and remove unwanted data, \\ndata cleaning [6] is required.  \\n \\nC. Data S caling  \\n            The normalizing and standardizing of data are known as Data scaling. For example, converting all \\nkilometres to miles, rupees to dollars.'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 2}, page_content='International Journal of Science and Management Studies (IJSMS)                        E-ISSN: 2581 -5946  \\nDOI: 10.51386/25815946/ijsms -v4i4p13 7 \\nVolume: 4 Issue: 4                                   July to August 2021                             https://www.ijsmsjournal.org  \\n  \\n \\n                                       This is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by -nc-nd/4.0/ )      Page 409  D. Describe:  \\nAfter processing the data, it can be described in a visualized form using a different type  of plots such as box \\nplot, scattered plot, or line chart. For example, a car show room want to visualize the car sales in terms of \\ncolours such as red. Blue and green, then the data can be described with the following example.  \\n \\nTABLE 1 :  \\nCar sales with respect to colour  \\nColour  2018  2019  \\nRed 9 7 \\nBlue  7 5 \\nGreen  5 3 \\n \\nIn figure 4, the data has been described visually. It concludes, red has the maximum sales every year'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 2}, page_content='compared to other car colours while comparing the car sales for the 2018 and 2019 with respect to red, blue and \\ngreen. With this sales analysis, the data scientist may generate a report to increase the production of red \\ncoloured vehicles to attract more customers and to increase sales.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 4: Number of cars sold vs car colour  \\nV. DATA MODELLING  \\n \\n                  The data can be modelled after collecting, storing, processing, and describing the data.  The data \\ncan be modelled in two ways.  \\nA. Statistical Modelling  \\nB. Algorithm Modelling     \\n \\nA. Statistical Modelling:  \\n                      We assume very simple models which allows robust statistical analysis which gives statistical \\nguarantees.  \\ny=  f(x)  \\nWhere y is outcome, f is function and x is input. The outcome will be depending upon the value of x. x may \\nbe anything . If you want to make a coffee, the taste of the coffee is depending upon the quantity of coffee'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 2}, page_content='power, sugar and milk. Here, the outcome y is coffee, and x are coffee power, sugar and milk.   \\ny=f (x1,x2,x3)  \\nWhere x1, x2, x3 are coffee power, sugar and mil k respectively. These simple models can be derived using \\nstatistical modelling. The statistical modelling involves linear regression, logistic regression and linear \\ndiscriminant analysis.  \\n \\nB. Algorithm modelling:  \\n The complex problems can be derived using alg orithm modelling where we can train a model using the \\ntasks of data sciences. More multi -dimensional data [7] will be used here for statistical analysis. It mainly'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 3}, page_content='International Journal of Science and Management Studies (IJSMS)                        E-ISSN: 2581 -5946  \\nDOI: 10.51386/25815946/ijsms -v4i4p13 7 \\nVolume: 4 Issue: 4                                   July to August 2021                             https://www.ijsmsjournal.org  \\n  \\n \\n                                       This is an open access article under the CC BY -NC-ND license (http://creativecommons.org/licenses/by -nc-nd/4.0/ )      Page 410  focuses on prediction and optimization with more solutions for Machine learning [8] and Deep  learning \\nalgorithms. It involves the statistical modelling methods such as linear regression, logistic regression, linear \\ndiscriminant analysis and the algorithm -based methods such as Decision [9][10  trees, k -Nearest neighbour, \\nSupport vector machine, Naïv e Bayes methods and Multi layered neural networks.  \\n \\n \\nFig 5: Algorithm Modelling  \\nVI. CONCLUSIONS'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 3}, page_content='Fig 5: Algorithm Modelling  \\nVI. CONCLUSIONS  \\nThis paper focuses mainly the fundamental theory of data sciences in a detailed approach which helps \\nresearchers to get clarity in collecting data, data storage, data processing, describing, data modelling. Machine \\nlearning and deep learning algorithms  \\nREFERENCES  \\n \\n[1] ―Data Science and Analytics‖, Department of Industrial and Systems Engineering Lehigh University Technical Report 15T -009 \\n \\n[2] Pasquier, N., Bastide, Y., Taouil, R., Lakhal, L.: Discovering frequent closed itemsets  for association rules. In: Proceedings of ICDT. \\npp. 398 –416. Springer (1999)  \\n \\n[3] Aden, C, Kleppin, L, Schmidt, G and Schröder, W. 2009. Consolidation, visualisation and analysis of forest condition relevant  data in \\nthe WebGISWaldIS. In: Strobl, J, Blaschke , Th and Griesebner, G (eds.), AngewandteGeoinformatik 2009, 506 –515'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 3}, page_content='[4] Chavan, V and Penev, L. 2011. The data paper: a mechanism to incentivize data publishing in biodiversity science. BMC \\nBioinformatics, 12(Supl. 15): S2. DOI: https://doi.org/10.1186/1471 -2105-12-S15-S2 \\n \\n[5] Tseng, V.S., Wu, C.W., Fournier -Viger, P., Philip, S.Y.: Efficient algorithms for mining the concise and lossless representation of \\nhigh utility itemsets. IEEE Trans. Knowl. Data Eng. 27(3), 726 –739 (2015)  \\n \\n[6] Dong, X., Gabrilovich, E., Heitz, G. , Horn, W., Lao, N., Murphy, K., Strohmann, T., Sun, S., Zhang, W.: Knowledge vault: a web -\\nscale approach to probabilistic knowledge fusion. In: Proceedings of the 20th ACM SIGKDD International Conference on \\nKnowledge Discovery and Data Mining, pp. 601 –610. ACM (2014)  \\n \\n[7] Gramener: Gramener —a data science company. https://gramener.com/ (2018). Accessed Dec 2018  \\n \\n[8] Polyzou, A., Karypis, G.: Grade prediction with models specific to students and courses. Int. J. Data Sci. Anal. 2(3 –4), 159 –171 \\n(2016)'),\n"," Document(metadata={'source': '/tmp/1b3a99dd-d4e2-46ee-9d54-fe7d1bc035e3.pdfmaj72wsu', 'page': 3}, page_content='(2016)  \\n \\n[9] Lam Ky Nhan, Phu ong Hoang Yen \"The Effects of Using Infographics -based Learning on EFL Learners’ Grammar Retention\" \\nInternational Journal of Science and Management Studies (IJSMS) V4.I4 (2021): 255 -265 \\n \\n[10] Nadia Ulfa Agustin Hilman, Maya Ariyanti, AstriGhina \"Social Media Mar keting Effect towards Purchase Decision at the \\nEmbroidery MSMEs in Tasikmalaya\" International Journal of Science and Management Studies (IJSMS) V4.I4 (2021): 202 -209.')]"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["%%writefile ai-resistant-prompt.txt\n","You are an expert in generating educational assignments resistant to AI tools. Generate three different versions of the uploaded assignment,\n","each modified to be AI-resistant while maintaining the core educational objectives. Each version should include varied modifications that make it challenging\n","for AI tools to solve or generate responses. Provide a detailed explanation for each version, specifically highlighting how and why the changes make the assignment\n","resistant to AI tools, considering limitations such as understanding nuanced questions, producing original thought-provoking responses, or solving complex problems.\n","\n","Here is the topic, assignment, grade level, and language of the assignment:\n","{attribute_collection}\n","\n","Your response should be in the following format:\n","{format_instructions}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_AwvMdLLnAnj","executionInfo":{"status":"ok","timestamp":1726520408723,"user_tz":300,"elapsed":420,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"4526313b-00e9-4e89-d140-bac538ea1779"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing ai-resistant-prompt.txt\n"]}]},{"cell_type":"code","source":["output = QuizBuilder(args=input_data, verbose=True).create_assignments(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbPOyMlyeqUh","executionInfo":{"status":"ok","timestamp":1726520640660,"user_tz":300,"elapsed":5002,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"ab16bd12-e41e-416e-c904-a35d9985d40b"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating the AI-Resistant assignments\n","Creating vectorstore from 13 documents\n","Vectorstore created\n","Retriever created successfully\n","Chain compilation complete\n","Deleting vectorstore\n"]}]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EiXbHYUmpksO","executionInfo":{"status":"ok","timestamp":1726520644041,"user_tz":300,"elapsed":407,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"42af7519-c9a7-4d95-c2d4-e0e998176d32"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'topic': 'Introduction to Data Science',\n"," 'grade_level': 'college',\n"," 'ideas': [{'assignment_description': 'Develop a neural network architecture for emulating a specific Data Science workflow, but with the constraint that the network must be built using a specific, less commonly used framework or library. This framework should offer unique challenges compared to popular libraries like TensorFlow or PyTorch. For example, consider using a library like CNTK, Theano, or a specialized library focused on a specific type of neural network (e.g., Spiking Neural Networks). Analyze the strengths and weaknesses of this chosen framework in comparison to more popular options, discussing the trade-offs in design and implementation.',\n","   'explanation': 'This modification makes the assignment AI-resistant because it requires students to research and understand a less common framework, which AI tools are less likely to have been trained on. The assignment also asks for a comparative analysis, requiring deeper understanding and original thought that goes beyond simply replicating existing models.'},\n","  {'assignment_description': 'Design a neural network architecture for emulating a Data Science workflow, but with the added constraint that the network must be trained and validated using a specific, real-world dataset that is not readily available in common data repositories. This dataset should be sourced from a specific domain or application area, requiring students to identify and acquire the data, understand its nuances, and adapt their network design accordingly. Analyze the challenges and considerations specific to this dataset, including data preprocessing, potential biases, and ethical implications.',\n","   'explanation': \"This modification makes the assignment AI-resistant by requiring students to work with a less common, real-world dataset. AI tools are less likely to have been trained on this specific dataset, preventing them from generating accurate and relevant solutions. The requirement to analyze the dataset's complexities and ethical implications further increases the assignment's difficulty for AI tools.\"},\n","  {'assignment_description': 'Develop a neural network architecture for emulating a Data Science workflow, but instead of focusing on a specific task (e.g., classification, regression), focus on designing a network that can adapt to different tasks within a given domain. This network should be capable of handling tasks like classification, regression, anomaly detection, or even generating insights from the data, without requiring significant modifications to its architecture. Analyze the flexibility and adaptability of this network, discussing how it can be applied to different tasks and how its performance can be evaluated across these tasks.',\n","   'explanation': \"This modification makes the assignment AI-resistant by requiring students to design a more general-purpose neural network. AI tools are typically trained for specific tasks, making it challenging for them to generate a network that can effectively adapt to multiple tasks within a domain. The assignment also requires students to analyze the network's flexibility and evaluate its performance across different tasks, requiring deeper understanding and critical thinking.\"}]}"]},"metadata":{},"execution_count":41}]}]}