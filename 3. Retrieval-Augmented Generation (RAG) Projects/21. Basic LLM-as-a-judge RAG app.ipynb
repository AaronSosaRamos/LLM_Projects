{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMN4uoDoh5szDTQbvbzBSZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmCQDGix_L_r","executionInfo":{"status":"ok","timestamp":1719614276038,"user_tz":300,"elapsed":68330,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"7d777d4e-3d5a-4c85-a642-0725e5f55585"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q langchain langchain_community langchain_chroma langchain-google-genai"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","if \"GOOGLE_API_KEY\" not in os.environ:\n","    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"],"metadata":{"id":"kPU4olKT_pRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import GoogleGenerativeAI\n","\n","llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")"],"metadata":{"id":"6Sy2fchg_5UD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIHSL_mXAeol","executionInfo":{"status":"ok","timestamp":1719614446900,"user_tz":300,"elapsed":7991,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"9635f9c9-e2dd-4fd4-c1ef-1dcf662f86ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/290.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFLoader\n","\n","loader = PyPDFLoader(\"https://arxiv.org/pdf/2306.05685\")\n","pages = loader.load_and_split()"],"metadata":{"id":"PN8rtWP9AM3T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain_chroma import Chroma\n","\n","vectorstore = Chroma.from_documents(documents=pages, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"],"metadata":{"id":"qtVUcra2ASJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever = vectorstore.as_retriever()"],"metadata":{"id":"Yts6oOogA3u1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"\"\"\n","Based on the following initial prompt, generate five optimal prompts that vary in perspective, detail level, scope, and incorporate different elements to stimulate diverse responses. The initial prompt is: [Insert Initial Prompt Here].\n","\n","1. Write a prompt that focuses on the same theme but from a first-person perspective.\n","2. Create a prompt that explores a more specific aspect of the original theme.\n","3. Develop a prompt that broadens the scope to include related topics or wider contexts.\n","4. Construct a prompt that includes a unique constraint or element to add complexity.\n","5. Formulate a prompt that emphasizes detailed descriptions and sensory details.\n","\n","Ensure each new prompt maintains the core theme of the initial prompt while offering a unique angle or approach.\n","Just list the proposed prompts in a comma-separated list.\n","{initial_prompt}\n","\"\"\"\n","\n","from langchain_core.prompts import PromptTemplate\n","optimal_prompts_prompt = PromptTemplate.from_template(prompt)\n","\n","chain_generation_prompts =  optimal_prompts_prompt | llm\n","result = chain_generation_prompts.invoke(\"What is LLM-as-a-judge?\")"],"metadata":{"id":"dHVeSTPMBTVa","executionInfo":{"status":"ok","timestamp":1719618092335,"user_tz":300,"elapsed":4508,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"rVX2pqsaObos","executionInfo":{"status":"ok","timestamp":1719618095216,"user_tz":300,"elapsed":430,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"488771f1-7e28-4e4e-a946-68d4e4254991"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1. As a large language model, how would I feel if I were tasked with being the ultimate judge in a court of law, weighing evidence and delivering verdicts?, 2. What are the ethical implications of using a large language model to determine the outcome of legal cases, particularly in terms of bias and fairness?, 3. How might the advent of LLM-as-a-judge impact the future of the legal profession and the justice system as a whole, considering its potential benefits and drawbacks?, 4. Imagine a world where an LLM judge must decide a case with no legal precedent, relying solely on its understanding of morality and ethics. How might it rule, and what challenges would it face?, 5. Describe the scene in a futuristic courtroom where an LLM judge, with its vast knowledge and analytical capabilities, presides over a complex trial, focusing on the sights, sounds, and atmosphere. \\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","\n","rank_prompts_prompt = \"\"\"\n","Given the context, evaluate and rank the following prompts in order of their effectiveness in addressing the context. Consider factors such as relevance, clarity, creativity, and ability to engage the intended audience.\n","\n","Context: {context}\n","\n","Prompts:\n","{optimal_prompts}\n","\n","Identify and provide only the best prompt based on this evaluation. Don't provide a justification.\n","\"\"\"\n","\n","ranked_prompt_template = PromptTemplate.from_template(rank_prompts_prompt)\n","chain_rank_prompts = (\n","    {\"context\": retriever, \"optimal_prompts\": RunnablePassthrough()}\n","    | ranked_prompt_template\n","    | llm\n",")\n","result2 = chain_rank_prompts.invoke(result)"],"metadata":{"id":"aN7CpsjzDuq4","executionInfo":{"status":"ok","timestamp":1719618627415,"user_tz":300,"elapsed":2144,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["result2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"mdv6tTiSPDE6","executionInfo":{"status":"ok","timestamp":1719618629665,"user_tz":300,"elapsed":369,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"07a31e54-5a64-4022-bde1-9bda48efea40"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3. How might the advent of LLM-as-a-judge impact the future of the legal profession and the justice system as a whole, considering its potential benefits and drawbacks? \\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["qa_prompt = \"\"\"\n","Given the following context, generate a comprehensive and well-informed response to the prompt provided. Ensure that the response is grounded in the latest 2024 scientific literature, and consider dimensions such as relevance, clarity, depth, creativity, accuracy, and engagement. Incorporate up-to-date research, statistics, and expert opinions to enhance the quality and reliability of the response.\n","\n","Context: {context}\n","\n","Prompt: {ranked_prompt}\n","\n","Key Considerations:\n","1. **Relevance**: Ensure the response is directly related to the context and prompt.\n","2. **Clarity**: Present the information in a clear, concise, and understandable manner.\n","3. **Depth**: Provide thorough and detailed information, covering all necessary aspects.\n","4. **Creativity**: Include innovative and original ideas where appropriate.\n","5. **Accuracy**: Base the response on current, verified scientific literature and data from 2024.\n","6. **Engagement**: Make the response engaging and compelling for the intended audience.\n","\n","Use these considerations to produce an optimal response that effectively addresses the prompt within the given context.\n","\"\"\"\n","\n","qa_prompt_template = PromptTemplate.from_template(qa_prompt)\n","chain_qa = (\n","    {\"context\": retriever, \"ranked_prompt\": RunnablePassthrough()}\n","    | qa_prompt_template\n","    | llm\n",")\n","result3 = chain_qa.invoke(result2)"],"metadata":{"id":"gg7lkZvVP9a2","executionInfo":{"status":"ok","timestamp":1719618956731,"user_tz":300,"elapsed":16538,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["result3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"rZ3CEqRnQ7ev","executionInfo":{"status":"ok","timestamp":1719618959605,"user_tz":300,"elapsed":422,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"ef1151ec-2c92-4ec2-b960-9130642e5093"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'## The Gavel and the Algorithm: How LLM-as-a-Judge Could Reshape the Legal Landscape\\n\\nThe emergence of Large Language Models (LLMs) as potential judges, as explored in \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\" (Zheng et al., 2023), presents both intriguing possibilities and complex challenges for the legal profession and the justice system. While still in its nascent stages, this technology has the potential to revolutionize legal proceedings, but not without careful consideration of its implications.\\n\\n**Potential Benefits:**\\n\\n* **Enhanced Efficiency and Accessibility:**  LLM-judges could automate many tasks currently performed by human judges, such as legal research, document review, and even preliminary case assessments. This could significantly expedite legal processes, reduce backlogs, and potentially make legal services more affordable and accessible to a wider population. \\n* **Increased Objectivity and Consistency:** One of the purported advantages of LLMs is their ability to process information and apply legal statutes in a more objective and consistent manner than humans, who can be influenced by biases, emotions, or fatigue. This could lead to fairer and more predictable legal outcomes.\\n* **Data-Driven Insights:** LLMs can analyze vast datasets of legal precedents, statutes, and scholarly articles to identify patterns and generate insights that might not be readily apparent to human judges. This data-driven approach could contribute to more informed and nuanced legal decisions.\\n\\n**Potential Drawbacks:**\\n\\n* **Bias and Fairness Concerns:** While LLMs are often touted as being less biased than humans, they are trained on massive datasets that may contain inherent biases present in historical legal data. This could perpetuate existing inequalities within the justice system if not addressed carefully. \\n* **Lack of Transparency and Explainability:**  The decision-making processes of complex LLMs can be opaque, making it difficult to understand the rationale behind their judgments. This lack of transparency could erode public trust in the legal system, as individuals may not understand or accept decisions made by an AI judge.\\n* **Ethical and Philosophical Considerations:**  The use of LLM-judges raises fundamental questions about the nature of justice, accountability, and the role of human judgment in legal proceedings.  Can an algorithm truly understand the nuances of human behavior and deliver a just outcome?  These are complex questions with no easy answers.\\n\\n**Impact on the Legal Profession:**\\n\\nThe widespread adoption of LLM-judges could lead to a significant shift in the legal profession. While some tasks may be automated, new opportunities will likely emerge. Lawyers may need to develop expertise in AI law, specializing in areas such as algorithmic auditing, data privacy, and AI ethics. The role of legal professionals might evolve to focus more on legal strategy, client counseling, and ensuring the ethical development and deployment of legal AI.\\n\\n**Moving Forward:**\\n\\nThe development and implementation of LLM-as-a-judge must be approached with caution, ensuring that ethical considerations, transparency, and fairness are paramount. Rigorous testing and validation, as exemplified by the MT-bench and Chatbot Arena studies, are crucial to assess the capabilities and limitations of these systems. Public discourse and engagement are also essential to shape the ethical and societal implications of this transformative technology.\\n\\nIt\\'s important to note that as of 2024, there is limited scientific literature specifically addressing the impact of LLMs on the legal profession.  The above analysis draws upon existing research on LLM capabilities, biases in AI, and the potential impact of automation on various professions. Further research is necessary to fully understand the complex interplay between LLMs and the justice system. \\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["full_chain = chain_generation_prompts | chain_rank_prompts | chain_qa\n","final_result = full_chain.invoke(\"What is LLM-as-a-judge?\")"],"metadata":{"id":"AdNUXLS9R2YN","executionInfo":{"status":"ok","timestamp":1719619199733,"user_tz":300,"elapsed":21510,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["final_result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"p9Tr3exoSlsy","executionInfo":{"status":"ok","timestamp":1719619202910,"user_tz":300,"elapsed":415,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"582502bb-9833-4370-fa26-e87157b00252"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"## Using LLMs to Enhance Fairness and Accuracy in Judging Subjective Competitions\\n\\nWhile the document focuses on evaluating LLMs' ability to judge open-ended responses, it hints at a crucial application: using this technology to improve fairness and accuracy in judging subjective competitions like essay writing or art contests. Here's how LLMs can be leveraged, along with potential challenges:\\n\\n**1. Objective Metric Extraction:**\\n\\n* **LLMs can analyze subjective content and extract quantifiable metrics.** For essays, this could include vocabulary complexity, argument structure, originality of ideas, and adherence to grammar and style guidelines. For art, metrics could involve color palettes, composition techniques, emotional impact based on image analysis, and adherence to specific styles or themes.\\n* **Example:** An LLM could be trained on a dataset of highly-rated essays to identify recurring patterns and characteristics that contribute to a strong piece, providing a score based on these objective elements.\\n\\n**2. Bias Mitigation:**\\n\\n* **LLMs can be trained to identify and flag potential biases in human judgment.** This could involve recognizing biases related to writing style, artistic expression, or even unconscious biases towards specific demographics. \\n* **Example:** An LLM could analyze past contest results and identify if certain writing styles or art forms consistently score higher, even if those differences are not outlined in the judging criteria. This information can then be used to calibrate judging rubrics and train judges for greater fairness.\\n\\n**3. Providing Consistent Feedback:**\\n\\n* **LLMs can offer standardized feedback based on pre-defined criteria.** This ensures all participants receive consistent and objective assessments, regardless of the individual judge assigned.\\n* **Example:** An LLM could provide feedback on an essay's clarity, argumentation, and evidence, highlighting areas for improvement based on objective criteria and examples from the text itself.\\n\\n**4. Enhancing Human Judgment:**\\n\\n* **LLMs should not replace human judges but act as a support system.**  LLMs can provide initial assessments, flag potential biases, and offer objective data points. Human judges can then use this information to make more informed and nuanced decisions.\\n* **Example:** An art contest could utilize LLMs to analyze submissions based on technical skill and adherence to the theme. Human judges, equipped with this data, can then focus on aspects like creativity, originality, and emotional impact, leading to a more holistic evaluation.\\n\\n**Challenges and Ethical Considerations:**\\n\\n* **Data Bias:** LLMs are trained on massive datasets, which may contain inherent biases. Ensuring these biases aren't reflected in their judgments requires careful dataset curation and bias mitigation techniques.\\n* **Over-Reliance on Metrics:**  Focusing solely on quantifiable metrics could stifle creativity and originality. Maintaining a balance between objective data and subjective human evaluation is crucial.\\n* **Transparency and Explainability:** LLM decisions should be transparent and explainable to ensure fairness and accountability.  Developing mechanisms to understand and interpret LLM judgments is essential.\\n\\n**Conclusion:**\\n\\nLLMs have the potential to significantly improve the fairness and accuracy of judging subjective competitions. However, careful consideration of ethical implications and potential biases is crucial to ensure responsible and effective implementation.  By combining the strengths of LLMs with the nuanced judgment of human experts, we can strive for greater fairness and objectivity in these fields. \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["full_chain = chain_generation_prompts | chain_rank_prompts | chain_qa\n","final_result = full_chain.invoke(\"What is LLM-as-a-judge?\")"],"metadata":{"id":"OnarTFv-SwtQ","executionInfo":{"status":"ok","timestamp":1719619255809,"user_tz":300,"elapsed":27520,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["final_result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"YT093OzgSz2C","executionInfo":{"status":"ok","timestamp":1719619258327,"user_tz":300,"elapsed":351,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"8f1b85a0-968d-4803-a311-7a14409a811b"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'## The Gavel and the Algorithm: Weighing the Potential of LLMs as Judges\\n\\nThe prospect of using LLMs as judges in the justice system, while seemingly futuristic, presents both exciting opportunities and significant challenges.  Let\\'s examine the potential benefits and drawbacks through the lens of access to justice, bias mitigation, and public trust:\\n\\n**Potential Benefits:**\\n\\n* **Increased Access to Justice:** \\n    * **Speed and Efficiency:** LLMs could process cases significantly faster than human judges, potentially alleviating court backlogs and providing quicker resolutions.  This could be particularly impactful in areas with limited legal resources, providing underrepresented communities with faster access to legal decisions.  \\n    * **Cost Reduction:**  The automation provided by LLMs could lower the cost of legal proceedings, making legal representation more accessible to individuals and small businesses who might otherwise struggle to afford it.\\n* **Mitigating Human Bias:**\\n    * **Data-Driven Decisions:**  Theoretically, LLMs could make more data-driven decisions by analyzing vast quantities of legal precedent and case information, potentially reducing the influence of human biases like prejudice or emotional judgment.  \\n    * **Transparency and Auditability:**  The decision-making process of an LLM can be recorded and audited, offering greater transparency compared to the often opaque reasoning of human judges. This could help identify and rectify biased outcomes more effectively. \\n* **Enhancing Public Trust (Conditional):**\\n    * **Perceived Objectivity:**  If developed and implemented responsibly, LLMs could be perceived as more objective than human judges, potentially increasing public trust in the impartiality of legal proceedings. \\n    * **Consistency in Sentencing:**  LLMs could help ensure greater consistency in sentencing by applying the same legal standards across similar cases, reducing disparities that often arise due to human subjectivity.\\n\\n**Potential Drawbacks:**\\n\\n* **Exacerbating Existing Biases:**\\n    * **Data Bias Amplification:** LLMs are trained on vast datasets, and if these datasets reflect existing societal biases present in legal texts and historical judgments, the LLM could inadvertently perpetuate and even amplify those biases. \\n    * **Lack of Contextual Understanding:**  LLMs may struggle to grasp the nuances of individual cases, such as cultural factors, personal histories, or mitigating circumstances, which are often crucial for fair and just outcomes in human-centric legal systems.\\n* **Transparency and Explainability:**\\n    * **\"Black Box\" Problem:**  While the decision-making process of LLMs can be recorded, understanding *why* an LLM reached a particular decision can be difficult. This lack of transparency can erode public trust, as individuals may not understand the reasoning behind judgments that directly affect their lives.\\n    * **Challenging Accountability:**  Determining liability for errors or biases in an LLM\\'s judgment presents a significant challenge. Is the developer responsible, the party who deployed the LLM, or the training data itself? This lack of clear accountability could undermine faith in the legal system.\\n* **Erosion of Public Trust:**\\n    * **Dehumanization of Justice:**  Relying solely on algorithms for legal judgments could be perceived as dehumanizing, removing the essential element of human empathy and understanding from the justice system. \\n    * **Lack of Adaptability:** Law is not static; it evolves with societal values and emerging challenges. LLMs may struggle to adapt to these changes as effectively as human judges who possess a deeper understanding of legal principles and their application in a dynamic world.\\n\\n**Moving Forward Responsibly:**\\n\\nThe use of LLMs in the justice system is in its infancy. Research into mitigating bias, improving transparency, and ensuring accountability is crucial. A hybrid approach, where LLMs assist human judges with specific tasks (legal research, document analysis) while leaving final judgments to human discretion, could be a more realistic and responsible path forward.  \\n\\n**Critical Considerations for 2024 and Beyond:**\\n\\n* **Ongoing Research and Development:**  Continuous research into LLM fairness, transparency, and explainability is paramount.\\n* **Robust Ethical Frameworks:**  Developing comprehensive ethical guidelines for the development and deployment of LLMs in legal contexts is essential.\\n* **Public Engagement and Dialogue:**  Fostering open public discourse about the potential benefits and risks of LLMs in the justice system is crucial to ensure public acceptance and trust.\\n\\nThe integration of LLMs into the justice system is a complex issue with far-reaching implications.  It is a discussion that demands careful consideration, robust research, and a commitment to ethical development to ensure that technology serves justice, rather than undermines it. \\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}]}]}