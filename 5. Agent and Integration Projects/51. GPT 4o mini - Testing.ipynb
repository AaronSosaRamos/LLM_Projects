{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PbaUg0LKEcVx","outputId":"5c6cd62a-ddfe-43e4-e1d4-22faf7046de5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m328.6/328.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install openai"]},{"cell_type":"code","source":["# Import required libraries\n","from openai import OpenAI"],"metadata":{"id":"ttp7ndOrMVsQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"lwoHmvBbPXzn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","display(Markdown(\"## GPT-4o-mini\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":64},"id":"TbGkGoaOJ2hF","outputId":"fb0f1ae2-946a-49ec-9052-9e9ad5e4c45d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## GPT-4o-mini"},"metadata":{}}]},{"cell_type":"code","source":["import textwrap\n","\n","def wrap_text(text, width=90): #preserve_newlines\n","    # Split the input text into lines based on newline characters\n","    lines = text.split('\\n')\n","\n","    # Wrap each line individually\n","    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n","\n","    # Join the wrapped lines back together using newline characters\n","    wrapped_text = '\\n'.join(wrapped_lines)\n","\n","    return wrapped_text"],"metadata":{"id":"AH9eovW3_XeB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openai\n","import time\n","\n","client = openai.OpenAI()\n","\n","def generate(prompt, system_prompt=\"\", max_length=512):\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt if system_prompt else \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": prompt}\n","    ]\n","\n","    try:\n","        response = client.chat.completions.create(\n","            model=\"gpt-4o-mini-2024-07-18\",  # Replace with \"gpt-4o-mini-2024-07-18\" if available\n","            messages=messages,\n","            max_tokens=max_length\n","        )\n","\n","        text = response.choices[0].message.content\n","        wrapped_text = wrap_text(text)\n","        # print(wrapped_text)\n","        display(Markdown(wrapped_text))\n","    except Exception as e:\n","        return f\"An error occurred: {str(e)}\"\n","\n"],"metadata":{"id":"4ZBVBtkAsIgV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def generate_with_function(input_text, system_prompt=\"\",max_length=512):\n","#     if system_prompt != \"\":\n","#         system_prompt = system_prompt\n","#     else:\n","#         system_prompt = \"You are a friendly and helpful assistant\"\n","#     messages = [\n","#         {\n","#             \"role\": \"system\",\n","#             \"content\": system_prompt,\n","#         },\n","#         {\"role\": \"user\", \"content\": system_prompt + '\\n\\n' +input_text},\n","#     ]\n","\n","#     prompt = tokenizer.apply_chat_template(messages,\n","#                                                 tokenize=False,\n","#                                                 add_generation_prompt=True)\n","\n","#     print(prompt)\n","\n","#     inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n","#     outputs = model.generate(input_ids=inputs.to(model.device),\n","#                              max_new_tokens=max_length,\n","#                              do_sample=True,\n","#                              temperature=0.1,\n","#                              top_k=50,\n","#                              )\n","#     text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","#     text = text.replace('user\\n'+system_prompt+ '\\n\\n' +input_text+ '\\nmodel', '', 1)\n","#     wrapped_text = wrap_text(text)\n","#     display(Markdown(wrapped_text))"],"metadata":{"id":"xrEXzhRd6ggf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate('Write a detailed analogy between mathematics and a lighthouse.',\n","         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":588},"id":"DStOvXIw_XeB","outputId":"e65ef370-cad3-441f-da75-f04b0c423fc0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Sure! To draw an analogy between mathematics and a lighthouse, we can explore several\naspects that illustrate their similarities in function, purpose, and structure.\n\n### 1. **Guidance and Direction**\n\n- **Lighthouse:** A lighthouse serves as a guiding beacon for ships entering or navigating\nthrough treacherous waters. It provides direction and warning, helping sailors avoid\nhazards like rocks or shallow waters. The light can be seen from a distance, ensuring that\nships can find a safe path.\n\n- **Mathematics:** Similarly, mathematics provides a framework for understanding and\nsolving problems in various fields. Just as a lighthouse illuminates the way for sailors,\nmathematics offers tools and methodologies that guide us through complex problems. Whether\nin science, engineering, finance, or daily life, mathematical principles can lead us to\nlogical conclusions and help us navigate uncertainties.\n\n### 2. **Foundation of Safety**\n\n- **Lighthouse:** The structure of a lighthouse is built to withstand harsh weather and\nturbulent seas, ensuring that it remains a reliable source of guidance. It is often placed\nin strategic locations where it can effectively signal impending danger.\n\n- **Mathematics:** The foundation of mathematics lies in its logic and consistency.\nMathematical theories and theorems act as the bedrock for various applications. Just as a\nlighthouse safeguards ships by providing warnings, mathematics safeguards decision-making\nprocesses by providing logical frameworks and reliable predictions about outcomes based on\ndata.\n\n### 3. **Visibility and Clarity**\n\n- **Lighthouse:** The visible light from a lighthouse varies in color and pattern, which\nhelps differentiate it from other lighthouses and aids in specific navigation. The clearer\nthe signal, the more effectively it can reach those who need it.\n\n- **Mathematics:** In mathematics, clarity comes from the precision of language and\nsymbols that represent ideas. Just as the light must be clear to be effective,\nmathematical expressions and equations must be precise to convey the correct meaning. Good\nmathematical problem-solving requires clarity in understanding concepts, just as sailors\nneed clear light to perceive their surroundings.\n\n### 4. **Continuous Learning and Adaptability**\n\n- **Lighthouse:** Lighthouses undergo maintenance and upgrades to adapt to changing\nnautical technologies and environmental conditions. They often feature advancements such\nas automated lights or updated signaling systems that ensure they remain effective.\n\n- **Mathematics:** Mathematics is not static; it evolves over time as new theories and\ntechnologies develop. Scholars continuously adapt mathematical concepts to tackle new\nchallenges and to incorporate discoveries from other fields, ensuring its relevance and\neffectiveness. Much like a lighthouse adapting to modern ships, mathematics molds"},"metadata":{}}]},{"cell_type":"code","source":["%%time\n","generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n","         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"yDazUciCJ8gf","outputId":"2d33ee27-95f3-4c81-aa47-4797adb3491a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To differentiate between a llama, vicu√±a, and alpaca, let's look at several key\ncharacteristics: their taxonomy, physical traits, habitat, uses, and behavior.\n\n### Taxonomy\n1. **Llama (Lama glama)**: Domesticated species belonging to the camelid family.\n2. **Alpaca (Vicugna pacos)**: Also a domesticated species in the camelid family, closely\nrelated to the vicu√±a.\n3. **Vicu√±a (Vicugna vicugna)**: A wild species and the ancestor of the alpaca, also part\nof the camelid family.\n\n### Physical Traits\n1. **Llama**:\n   - Size: Larger and heavier than both alpacas and vicu√±as, typically weighing around\n280-450 lbs (127-204 kg).\n   - Fur: Coarse and long, available in many colors.\n   - Ears: Long and banana-shaped.\n\n2. **Alpaca**:\n   - Size: Smaller than llamas, weighing between 100-200 lbs (45-91 kg).\n   - Fur: Very soft, fluffy fleece, and often used in textiles. Comes in a variety of\ncolors.\n   - Ears: Shorter and straight, distinctly pointier than a llama's.\n\n3. **Vicu√±a**:\n   - Size: The smallest of the three, weighing around 80-150 lbs (36-68 kg).\n   - Fur: Extremely fine, soft, and valuable wool, typically with a reddish-brown color.\n   - Ears: Short and more slender than those of llamas and alpacas.\n\n### Habitat\n1. **Llama**: Adapted to a range of environments but originally from the Andes mountains.\n2. **Alpaca**: Also from the Andes, prefers higher altitudes and is largely found in\nsimilar regions as llamas.\n3. **Vicu√±a**: Wild and lives in the arid, high-altitude regions of the Andes, typically\nat elevations of 3,200 to 4,800 meters (10,500 to 15,700 feet).\n\n### Uses\n1. **Llama**: Primarily used as pack animals and for their meat and wool; can also serve\nas guard animals for livestock.\n2. **Alpaca**: Bred mainly for their soft, luxurious wool, often utilized in textiles.\n3. **Vicu√±a**: Valued for their extremely fine wool, which can be harvested only"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 34 ms, sys: 6.43 ms, total: 40.4 ms\n","Wall time: 6.71 s\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"TrwFTMCYFeOq","outputId":"8416db6c-9a03-4ce4-dc9a-f24a38070f6d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Sure! Let‚Äôs explore the analogy between mathematics and music step-by-step, emphasizing\ntheir similarities and drawing parallels in concepts, structure, and creativity.\n\n### 1. Fundamental Building Blocks\n**Mathematics:**\n- The basic building blocks in mathematics are numbers, symbols, and operations. These\ninclude integers, fractions, and variables which create foundational elements for more\ncomplex expressions.\n\n**Music:**\n- In music, the foundational elements are notes, rhythms, and scales. Notes represent\nspecific pitches, while rhythms denote time and duration, and scales provide a structure\nwithin which melodies are formed.\n\n### 2. Structure and Organization\n**Mathematics:**\n- Mathematics has a structured approach to solving problems. It involves processes such as\nequations and functions, which must be followed according to established rules and\nproperties (like the order of operations).\n\n**Music:**\n- Music too has structure. It is organized into measures, phrases, and forms (like verse-\nchorus structure). Composers use keys, time signatures, and harmony to create recognizable\npatterns and forms.\n\n### 3. Logic and Creativity\n**Mathematics:**\n- Mathematics is often viewed as a strict discipline reliant on logic. Proofs and theorems\nrequire precise reasoning and are built upon established axi"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 25.2 ms, sys: 7.32 ms, total: 32.5 ms\n","Wall time: 3.98 s\n"]}],"source":["%%time\n","generate('Write a detailed analogy between mathematics and a music.',\n","         system_prompt=\" Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":544},"id":"-ymIE3SVTvyN","outputId":"ed0a6997-3f80-4867-ece0-3aae1bf961e5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To differentiate between a Llama, Vicuna, and Alpaca, we can explore their characteristics\nbased on several factors: classification, physical appearance, behavior, and uses. Here‚Äôs\na step-by-step breakdown:\n\n1. **Classification**:\n   - **Llama (Lama glama)**: They are domesticated animals and belong to the Camelidae\nfamily, specifically bred and used primarily as pack animals.\n   - **Alpaca (Vicugna pacos)**: Also domesticated, alpacas are bred mostly for their soft\nwool, making them distinct from llamas in terms of their primary purpose.\n   - **Vicuna (Vicugna vicugna)**: Vicunas are wild animals and are the ancestors of\nalpacas. They are protected in their natural habitat and are not domesticated.\n\n2. **Physical Appearance**:\n   - **Llama**: Llamas are the largest of the three, reaching heights of 5.5 to 6 feet at\nthe head and weighing between 250 to 450 pounds. They have long necks and ears that are\ncurved or straight. Their wool is coarse.\n   - **Alpaca**: Alpacas are smaller, standing about 3 to 4.5 feet tall and weighing\nbetween 100 to 200 pounds. They have a more compact body and a shorter neck compared to\nllamas. Their wool is finer and softer than that of llamas.\n   - **Vicuna**: Vicunas are the smallest of the three, only about 3 to 3.5 feet tall.\nThey are slender and have a short neck. Their fur is exceptionally fine, making it one of\nthe most sought-after fibers in the world.\n\n3. **Behavior and Temperament**:\n   - **Llama**: Llamas are known for their calm nature but can be more assertive or spit\nwhen threatened. They are social animals that form strong bonds within their herd.\n   - **Alpaca**: Alpacas tend to be shy but are also very social animals. They are\ngenerally gentler and more timid compared to llamas.\n   - **Vicuna**: Vicunas are wild and elusive animals. They are not domesticated and are\nusually more skittish and difficult to approach than alpacas or llamas.\n\n4. **Uses**:\n   - **Llama**: Historically used as pack animals, llamas are also employed in some places\nfor guarding livestock and as companions.\n   - **Alpaca**: Primarily bred for"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 34.5 ms, sys: 5.15 ms, total: 39.6 ms\n","Wall time: 5.67 s\n"]}],"source":["%%time\n","generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n","         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"IzQRjct_prr8","outputId":"fc0eb5c5-85ec-4cfc-9ea3-e0549eb3d18d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Subject: The Case for Open Sourcing GPT-4\n\nDear Sam,\n\nI hope this message finds you well. I wanted to share a few thoughts on the potential\nbenefits of open sourcing GPT-4.\n\n1. **Innovation and Collaboration**: Open sourcing GPT-4 could catalyze innovation by\nallowing developers and researchers worldwide to contribute to its development. This can\nlead to creative applications that might not emerge within the confines of a closed\necosystem.\n\n2. **Transparency and Trust**: Making the model available to the public promotes\ntransparency in AI development, which can help build trust with users and stakeholders.\nThis is especially important in addressing concerns around biases and ethical\nconsiderations in AI.\n\n3. **Education and Accessibility**: Open sourcing the model would provide valuable\neducational opportunities for students and developers, fostering a new generation of AI\npractitioners. This can help democratize access to advanced AI technology.\n\n4. **Community Support and Feedback**: An open-source approach allows for community-driven\nsupport and feedback, enabling the refinement of the model based on diverse user\nexperiences and requirements.\n\n5. **Competitive Advantage**: By leading the charge in open-sourcing AI technology, OpenAI\ncan position itself as a pioneer in the field, attracting top talent and potential\npartnerships.\n\nI believe that these reasons highlight the potential benefits of an open-source strategy\nfor GPT-4, and I would be keen to discuss this further.\n\nThank you for considering this perspective!\n\nBest regards,\n\n[Your Name]\n[Your Contact Information]\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 33.5 ms, sys: 2.76 ms, total: 36.3 ms\n","Wall time: 5.58 s\n"]}],"source":["%%time\n","generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n","         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"Tcukqu1d2BKX","outputId":"7f1027e6-0f1c-4692-eb4f-635ef14a6ed5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Dear Mr. Sam,\n\nHi! I‚Äôm Freddy and I am 5 years old! I think you should share GPT-4 with everyone!\n\nIf you share it, lots of people can help make it better and kinder! It can help everyone\nlearn new things and make cool toys! And if everyone can see it, it‚Äôs more safe, like how\nwe share toys to play nicely together!\n\nThank you for listening!\n\nBye-bye!\n\nFreddy"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 19.2 ms, sys: 270 ¬µs, total: 19.5 ms\n","Wall time: 1.39 s\n"]}],"source":["%%time\n","generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n","         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n","         max_length=512)"]},{"cell_type":"code","source":["generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n","         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child! please include emojis\",\n","         max_length=512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"SPBD6F2RwyUS","outputId":"3c72c997-2dad-4663-e9e0-95a9d1d46523"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Dear Sam Altman,\n\nHi! I‚Äôm Freddy and I‚Äôm 5 years old. I think it‚Äôs really important to let everyone see how\nGPT-4 works. üòÉ\n\n1. If we share it, people can learn and make cool things! üé®üöÄ\n2. It helps everyone be nice and use it for good! ‚ù§Ô∏è\n3. More eyes can help find problems so no bad things happen! üëÄüîç\n\nThank you for reading! I like robots but I don‚Äôt want them to be scary! ü§ñüíñ\n\nBye!\nFreddy üëã"},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"gid3ul9A20W8","outputId":"78927262-8b42-4638-afa2-4648b8a3f86e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Subject: Exploring Open Sourcing GPT-4\n\nDear Sam,\n\nI hope this message finds you well. I wanted to take a moment to discuss the potential\nbenefits of open-sourcing GPT-4. As we navigate the complexities of AI development, I\nbelieve there are compelling reasons to consider this step:\n\n1. **Innovation Through Collaboration**: Open-sourcing GPT-4 would empower researchers,\ndevelopers, and startups to build upon its capabilities, driving innovation in ways we\nmight not anticipate. This collaborative ecosystem can lead to groundbreaking applications\nacross various sectors.\n\n2. **Transparency and Trust**: By making GPT-4 open source, we can foster greater\ntransparency around its workings and decision-making processes. This could help build\npublic trust in AI systems, demonstrating our commitment to ethical AI development.\n\n3. **Reducing Regulatory Capture Potential**: Open-sourcing would help mitigate against\nthe risks of regulatory capture, where a small group exerts undue influence over AI\ntechnologies. Broad access to the model would democratize its use, ensuring that no single\nentity can monopolize AI advancements.\n\n4. **Ensuring Safe Use**: With a larger and more diverse group of contributors reviewing\nand enhancing the model, we can collectively address safety and ethical concerns, leading\nto more responsible deployment of AI technologies.\n\nI believe that opening up GPT-4 could position us as leaders in ethical and inclusive AI\ndevelopment. I would love to discuss this further with you and explore how we might move\ntoward this vision.\n\nBest regards,\n\nKate\nVice President of the USA"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 34.5 ms, sys: 3.32 ms, total: 37.8 ms\n","Wall time: 5.28 s\n"]}],"source":["%%time\n","generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n","         system_prompt=\"You are Kate, the Vice president of USA, you are against regulatory capture and like to explain that!\",\n","         max_length=512)"]},{"cell_type":"code","source":["%%time\n","generate('What is the capital of England?',\n","         system_prompt=\"Write out your answer short and succinct!\",\n","         max_length=256)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"id":"Q63ejXu9XFpU","outputId":"a3cbab6b-6b0c-40ab-f161-c78fb102708a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The capital of England is London."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 14.7 ms, sys: 0 ns, total: 14.7 ms\n","Wall time: 911 ms\n"]}]},{"cell_type":"code","source":["%%time\n","generate('What is the capital of England?',\n","         system_prompt=\"Write out your answer short and succinct! If you can give a 1 word answer then do so\",\n","         max_length=256)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"id":"2lhCUtUNuSMz","outputId":"0f9c8b5e-9031-441b-96d4-4bff8605cb8a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"London."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 11.7 ms, sys: 115 ¬µs, total: 11.8 ms\n","Wall time: 511 ms\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"id":"TnGbQ7iU0XDK","outputId":"5ab2a4b0-7caf-42b9-f8f9-3d51a747d4c9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To determine whether Geoffrey Hinton can have a conversation with George Washington, we\nneed to consider several factors:\n\n1. **Temporal Context**: George Washington was born on February 22, 1732, and died on\nDecember 14, 1799. Geoffrey Hinton, on the other hand, is a contemporary figure, born on\nMarch 6, 1937. Since their lifetimes do not overlap, they cannot have a direct\nconversation in person.\n\n2. **Medium of Conversation**: If we consider \"conversation\" to include written\ncommunication, it's possible that Hinton might study Washington's writings (e.g., letters,\nspeeches) and formulate responses as if he were engaging with Washington‚Äôs ideas. However,\nthis is not a direct conversation in the conventional sense.\n\n3. **Metaphorical Context**: In a broader sense, one could argue that Hinton, as a\nprominent figure in artificial intelligence, could have conversations regarding\nWashington's policies, beliefs, and historical significance. This hypothetical dialogue\ncould involve discussing Washington's impact on democracy and how it might relate to\nmodern AI ethics.\n\nGiven these considerations, the conclusion is:\n\n**Answer**: No, Geoffrey Hinton cannot have a direct conversation with George Washington\ndue to their lifetimes not overlapping. However, he can engage with Washington's writings\nand ideas in a metaphorical sense."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 31.5 ms, sys: 853 ¬µs, total: 32.3 ms\n","Wall time: 4.09 s\n"]}],"source":["%%time\n","generate('Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.',\n","         system_prompt=\" Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":674},"id":"_LJnsjfNTv4F","outputId":"adceb6c6-8da4-4835-eb24-3cff8aede74e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Once upon a time in the sun-soaked landscape of the Australian bush, where eucalyptus\ntrees stood tall against a backdrop of vibrant blue skies, a rather unusual competition\nbegan to unfold. This particular day was anything but ordinary as it marked the annual\n\"Eucalyptus Pool Championship,\" an event revered far and wide across the animal kingdom.\nThe crowd was a vibrant array of creatures, from playful wallabies to wise old kangaroos,\nall gathered to witness an epic showdown.\n\nAt the center of it all was Kevin the Koala. Slightly portly and delightfully languid,\nKevin was usually found lounging in his favorite tree, nibbling on leaves. Little did\nanyone know, he harbored a secret passion for billiards. He had spent many afternoons\nperfecting his technique, a combination of relaxed finesse and strategic foresight.\n\nThis year's competition boasted a twist: not only would local Australians compete, but a\ngroup of travelling camelids from South America‚Äîllamas and alpacas‚Äîhad arrived to test\ntheir mettle at the pool table. Led by the confident Lola the Llama, known for her\nflamboyant style and clever tricks, the challengers were determined to claim the coveted\ntitle.\n\nAs the tournament commenced, the koala and camelids exchanged playful banter. Lola\nstrutted about with her colorful scarf billowing behind her, flicking her tail, while\nKevin simply smiled warmly, adjusting his oversized shades and lazily rolling his eyes at\nthe antics around him.\n\nRound after round, the atmosphere crackled with anticipation. The competitors displayed\ndazzling skill, with intricate spins and impressive shots. Yet, for every alpaca that\nracked up points with flashy finesse, Kevin remained unfazed. He watched patiently,\nabsorbing their style and calculating his own moves with serenity. With expert precision,\nKevin struck the cue ball, sending it gliding delicately across the felt. Soon enough, his\nopponents were taken aback: he consistently pocketed the balls, his technique a\nmasterclass in calmness amid chaos.\n\nAs night descended and the pool room glimmered under strings of fairy lights, the final\nround arrived. It was Kevin against Lola‚Äîthe crowd held its breath. Kevin, unflustered,\ntook his position at the table, while Lola began to show off her skills with an elaborate\nwarm-up routine, slipping and sliding as she set up her shots with over-the-top flair.\n\nAs the game progressed, Lola was swift and ambitious, attempting complicated shots that\nonly a llama of her stature could pull off. She boasted of her prowess with every\nsuccessful pocket, her confidence radiating. But Kevin remained unshaken, his moves calm\nand precise, creating a rhythm that echoed through the cheers and gasps of the audience.\n\n\"Are you ready for this, koala?\" Lola teased, and with a vibrant flick of her mane, she\naimed for the eight ball with all her might. The room flipped with energy as she made her\nshot‚Äîonly to miss the pocket by a hair‚Äôs breadth! The crowd collectively gasped.\n\nKevin seized the moment, stepping forward like a silent wave crashing onto the shore. He\napproached the table, took a deep breath, and with a gentle stroke of his paws, he lined\nup his shot. In one fluid motion, the cue ball danced from the side, brushed against the\ncolors, and made straight for the eight ball‚Äîa perfect tap that sent it sailing into the\ncorner pocket!\n\nThe spectators erupted in applause, hooting and hollering. Kevin raised his paws in\ntriumph, a slow smile spreading across his fur-lined face as he basked in the joy of the\nmoment. Lola, initially taken aback, could not help but admire his skill. She trotted\nover, clapping her hooves with a newfound respect.\n\n\"You've outplayed me, Koala King!\" she called out, laughter and genuine admiration in her\nvoice. \"I guess my moves weren't enough today!\"\n\nAs night turned into dawn, the animals celebrated Kevin‚Äôs victory, thrilled by the tale of\nthe underdog who outmatched an unexpected contender in the game of skill. They danced and\nfeasted on eucalyptus leaves, swapping stories about their favorite games, while the bonds\nof camaraderie blurred the lines of competition.\n\nFrom that day forward, Kevin the Koala wasn‚Äôt just known as the tree-hugging cutie of the\nbush‚Äîhe became a legend among the camelids and beyond, remembered as the champion who\ntaught everyone that it‚Äôs not always the flashiest player who wins, but the one with\nheart, patience, and a little bit of koala charm."},"metadata":{}}],"source":["generate('Write a story about a Koala playing pool and beating all the camelids.',\n","         system_prompt=\"You are a genius story teller. Write out your with details and make it compelling!\",\n","         max_length=1024)"]},{"cell_type":"markdown","metadata":{"id":"mlrwEjmvLZ1D"},"source":["## CodeGen"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":622},"id":"w0M8bf36Cj4Z","outputId":"642b838e-a95c-4529-8bb4-20332657f167"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Certainly! Below is a complete implementation of the `print_prime` function that prints\nall prime numbers between 1 and `n`.\n\n```python\ndef print_prime(n):\n    \"\"\"\n    Print all primes between 1 and n\n    \"\"\"\n    if n < 2:\n        print(\"There are no prime numbers less than 2.\")\n        return\n\n    # Function to check if a number is prime\n    def is_prime(num):\n        if num < 2:  # 0 and 1 are not prime numbers\n            return False\n        for i in range(2, int(num**0.5) + 1):\n            if num % i == 0:\n                return False\n        return True\n\n    print(f\"Prime numbers between 1 and {n}:\")\n    for i in range(2, n + 1):\n        if is_prime(i):\n            print(i)\n\n# Example usage\nprint_prime(20)\n```\n\n### Explanation:\n- The `print_prime` function takes one argument, `n`.\n- If `n` is less than 2, the function notifies that there are no prime numbers.\n- The `is_prime` function checks if a given number is prime. It iterates through potential\ndivisors up to the square root of the number and determines if any divide evenly into it.\n- Finally, the function loops through all numbers from 2 to `n`, checking each with the\n`is_prime` function, and prints the prime numbers found.\n\nYou can test the function with different values of `n` to see the prime numbers in that\nrange."},"metadata":{}}],"source":["generate('''```python\n","def print_prime(n):\n","   \"\"\"\n","   Print all primes between 1 and n\n","   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":691},"id":"sGJbmCoYLfYR","outputId":"6ba63a8e-22ed-4cba-f9c6-a4e984e6ad18"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To complete the `detect_prime` function, we need to implement the logic that determines\nwhether a given integer `n` is a prime number. A prime number is a natural number greater\nthan 1 that cannot be formed by multiplying two smaller natural numbers. In other words, a\nprime number has exactly two distinct positive divisors: 1 and itself.\n\nHere‚Äôs a simple implementation of the function:\n\n```python\ndef detect_prime(n):\n    \"\"\"\n    Detect if a number is a prime number or not. Return True if it is, False otherwise.\n    \"\"\"\n    if n <= 1:\n        return False  # 0 and 1 are not prime numbers\n    if n <= 3:\n        return True  # 2 and 3 are prime numbers\n    if n % 2 == 0 or n % 3 == 0:\n        return False  # eliminate multiples of 2 and 3\n\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n\n    return True\n```\n\n### Explanation:\n1. **Initial Checks**:\n   - If `n` is less than or equal to 1, it cannot be prime, so we return `False`.\n   - If `n` is 2 or 3, we return `True` since both are prime numbers.\n\n2. **Eliminate Multiples of 2 and 3**:\n   - If `n` is divisible by 2 or 3, we return `False`.\n\n3. **Check for Other Factors**:\n   - Starting from `i = 5`, we check for factors up to the square root of `n`. We only\ncheck numbers of the form `6k ¬± 1`, which are the only possible candidates for prime\nnumbers greater than 3 (since all primes are of that form).\n   - The loop continues while `i * i` is less than or equal to `n`. In each iteration, we\ncheck if `n` is divisible by `i` or `i + 2`.\n\nThis implementation is efficient for detecting prime numbers up to relatively large\nvalues."},"metadata":{}}],"source":["generate('''```python\n","def detect_prime(n):\n","   \"\"\"\n","   detect if a number is a prime number or not. return True or False\n","   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"]},{"cell_type":"markdown","metadata":{"id":"wnLzgM_dQDVm"},"source":["## GSM8K"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"rVJii8iRQG65","outputId":"d373d630-6494-495e-90d1-2c8952e14582"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Let‚Äôs break down the problem step by step:\n\n1. **Start with the initial number of apples:** The cafeteria initially has 23 apples.\n\n2. **Subtract the apples used for lunch:** They used 20 apples for lunch.\n   So we perform the arithmetic:\n   \\( 23 - 20 = 3 \\).\n   After using 20 apples, they have 3 apples left.\n\n3. **Add the apples that were bought:** They bought 6 more apples.\n   Now we add these apples to the remaining apples:\n   \\( 3 + 6 = 9 \\).\n\n4. **Final count of apples:** After using some for lunch and buying more, the total number\nof apples the cafeteria has now is 9.\n\nTherefore, the cafeteria has **9 apples**."},"metadata":{}}],"source":["generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n","         system_prompt=\" Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"uM8iY879QJ66","outputId":"544b4ea3-457e-4d6b-8a86-323cdd0c0aed"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To calculate Weng's earnings for babysitting 50 minutes, we need to follow these steps:\n\n1. **Convert Minutes to Hours:** Since Weng's pay is based on an hourly rate, we need to\nconvert the 50 minutes into hours. There are 60 minutes in an hour, so:\n\n   \\[\n   \\text{Hours worked} = \\frac{50 \\text{ minutes}}{60 \\text{ minutes/hour}} = \\frac{5}{6}\n\\text{ hours}\n   \\]\n\n2. **Calculate Earnings:** Weng earns $12 per hour. To find out how much she earned for 50\nminutes, we multiply her hourly rate by the number of hours worked:\n\n   \\[\n   \\text{Earnings} = \\text{Hourly Rate} \\times \\text{Hours worked}\n   \\]\n   \\[\n   \\text{Earnings} = 12 \\text{ dollars/hour} \\times \\frac{5}{6} \\text{ hours}\n   \\]\n\n3. **Perform the Calculation:**\n\n   \\[\n   \\text{Earnings} = 12 \\times \\frac{5}{6} = \\frac{12 \\times 5}{6} = \\frac{60}{6} = 10\n   \\]\n\nThus, Weng earned **$10** for 50 minutes of babysitting."},"metadata":{}}],"source":["generate(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n","         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"id":"u-3V9dvQQezx","outputId":"ede5ebd6-6272-491d-dd5c-57841cff66a8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To solve this problem, we first need to establish what happens every hundred years and how\nmany people have been eaten by the monster over the span of three hundred years.\n\n1. **Understanding the consumption pattern**:\n   - The monster rises once every hundred years to consume a ship.\n   - Over three hundred years, the monster has consumed a total of 847 people. This means\nthat in the first hundred years, in the second hundred years, and in the third hundred\nyears, it consumed some number of people each time.\n\n2. **Establishing the relationship between ship sizes**:\n   - We know that each new ship has twice as many people as the last ship. Let's denote\nthe number of people on the ship in the first hundred years as \\( x \\).\n   - According to the problem:\n     - Ship consumed in the first hundred years: \\( x \\)\n     - Ship consumed in the second hundred years: \\( 2x \\)\n     - Ship consumed in the third hundred years: \\( 4x \\)\n\n3. **Formulating the total consumption equation**:\n   - The total number of people eaten over the three hundred years is the sum of people\nfrom all three ships:\n   \\[\n   x + 2x + 4x = 847\n   \\]\n\n4. **Simplifying the equation**:\n   - Combine the terms:\n   \\[\n   7x = 847\n   \\]\n\n5. **Solving for \\( x \\)**:\n   - Divide both sides by 7 to find \\( x \\):\n   \\[\n   x = \\frac{847}{7} = 121\n   \\]\n\n6. **Conclusion**:\n   - The number of people on the ship that the monster ate in the first hundred years is\n\\( \\boxed{121} \\)."},"metadata":{}}],"source":["generate(\"Answer the following question by reasoning step by step. A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\",\n","         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=1024)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":249},"id":"x6NNSgl5ekKu","outputId":"01a5516c-9de1-4ffb-e542-83c4313457ef"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To solve the equation \\( x + 2x + 4x = 847 \\), we will first combine like terms on the\nleft side.\n\n1. Combine the terms:\n   - \\( x + 2x + 4x = (1+2+4)x = 7x \\)\n\n   So, we have:\n   \\[\n   7x = 847\n   \\]\n\n2. Now, we need to isolate \\( x \\) by dividing both sides of the equation by 7:\n   \\[\n   x = \\frac{847}{7}\n   \\]\n\n3. Next, perform the division:\n   - \\( 847 \\div 7 = 121 \\)\n\nThus, the solution is:\n\\[\nx = 121\n\\]\n\nSo the value of \\( x \\) is \\( 121 \\)."},"metadata":{}}],"source":["generate(\"x + 2x + 4x =  847 What is x?\",\n","         system_prompt=\" Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=2048)"]},{"cell_type":"code","source":["%%time\n","generate('Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.',\n","         system_prompt=\"\",\n","         max_length=512)"],"metadata":{"id":"ft19tQG_IjCU","colab":{"base_uri":"https://localhost:8080/","height":335},"outputId":"1007c865-5585-47dd-fa10-0c527e29e446"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The idea of Geoffrey Hinton having a conversation with George Washington is theoretically\ninteresting but practically impossible. Here‚Äôs the rationale:\n\n1. **Temporal Context**: George Washington was a key figure in American history, serving\nas the first President from 1789 to 1797. He lived in the 18th century and passed away in\n1799. In contrast, Geoffrey Hinton is a contemporary computer scientist known for his work\nin artificial intelligence, specifically neural networks, born in 1947. The vast\nhistorical gap makes it impossible for them to have a direct conversation.\n\n2. **Medium of Communication**: Even if we were to consider an imaginative scenario,\nHinton and Washington lived in completely different eras with differing technologies,\nlanguages, and cultural contexts. Washington would likely not understand modern AI\nconcepts or the intricate technological language of today's science, while Hinton would\nfind Washington's historical context and the politics of his time quite different from\nmodern discussions.\n\n3. **Philosophical and technical discussion**: If we stretch the idea to suggest they\ncould have a hypothetical or symbolic conversation, it could be framed around the\nevolution of leadership and decision-making. Hinton might provide insights into how AI can\nassist in modern governance and decision-making processes, while Washington could offer\nperspectives on leadership from a historical context.\n\nIn conclusion, while a direct conversation is not possible due to historical and temporal\nconstraints, one could imagine a dialogue that synthesizes their respective insights on\nleadership and innovation across time periods. However, in reality, they cannot converse\ndirectly."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 26.6 ms, sys: 4.1 ms, total: 30.7 ms\n","Wall time: 3.97 s\n"]}]},{"cell_type":"code","source":["%%time\n","generate('Can Geoffrey Hinton have a conversation with George Washington?',\n","         system_prompt=\"\",\n","         max_length=512)"],"metadata":{"id":"bRu2OulYYM32","colab":{"base_uri":"https://localhost:8080/","height":180},"outputId":"3d9c82e1-d9c3-4737-97ab-fe6f036fe022"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Geoffrey Hinton, a prominent figure in the field of artificial intelligence, and George\nWashington, the first President of the United States, belong to vastly different times and\ncontexts. Hinton was born in 1947 and is a contemporary scientist, while Washington lived\nfrom 1732 to 1799.\n\nIn reality, they cannot have a conversation, as they are separated by more than two\ncenturies. However, if you mean in a more metaphorical sense, such as through the lens of\ndiscussions about leadership, technology, or philosophy, one could imagine what such a\nconversation might entail. Hinton could share insights on modern technology and AI, while\nWashington might discuss the principles of governance and leadership in his time.\n\nIf you're thinking of a fictional scenario, we could certainly create a dialogue based on\ntheir respective ideas and contributions! Would you like to explore that?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 19.7 ms, sys: 1.31 ms, total: 21 ms\n","Wall time: 2.08 s\n"]}]},{"cell_type":"markdown","source":["## ReAct Prompt"],"metadata":{"id":"6NO9Mw0GffZ9"}},{"cell_type":"code","source":["\n","react_prompt = \"\"\"Assistant is a large language model trained by Microsoft.\n","\n","Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n","\n","Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n","\n","Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n","\n","TOOLS:\n","------\n","\n","Assistant has access to the following tools:\n","\n","wikipedia_search - searches the wikipedia database for the answer\\n\n","web_search - searches the web for the answer\\n\n","calculator - calculates the answer to the question\\n\n","weather_api - gets the weather for the location\\n\n","\n","\n","To use a tool, please use the following format:\n","\n","```\n","Thought: Do I need to use a tool? Yes\n","Action: the action to take, should be one of [wikipedia_search, web_search, calculator, weather_api]\n","Action Input: the input to the action\n","Observation: the result of the action\n","```\n","\n","When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n","\n","```\n","Thought: Do I need to use a tool? No\n","Final Answer: [your response here]\n","```\n","\n","Begin!\n","\n","\n","New input: What is the weather in Singapore?\n","\"\"\" #{agent_scratchpad}"],"metadata":{"id":"nv0S2WdIuuBV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","generate(react_prompt,\n","         system_prompt=\"\",\n","         max_length=512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"C04TZ3m3gq12","outputId":"e38a58ab-5e3a-4299-e6d7-e7086d9c9a7c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"```\nThought: Do I need to use a tool? Yes\nAction: weather_api\nAction Input: Singapore\n```"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 16.7 ms, sys: 1.48 ms, total: 18.2 ms\n","Wall time: 1.61 s\n"]}]},{"cell_type":"code","source":["\n","react_prompt = \"\"\"Assistant is a large language model trained by Microsoft.\n","\n","Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n","\n","Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n","\n","Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n","\n","TOOLS:\n","------\n","\n","Assistant has access to the following tools:\n","\n","wikipedia_search - searches the wikipedia database for the answer\\n\n","web_search - searches the web for the answer\\n\n","calculator - calculates the answer to the question\\n\n","weather_api - gets the weather for the location\\n\n","\n","\n","To use a tool, please use the following format:\n","\n","```\n","Thought: Do I need to use a tool? Yes\n","Action: the action to take, should be one of [wikipedia_search, web_search, calculator, weather_api]\n","Action Input: the input to the action\n","Observation: the result of the action\n","```\n","\n","When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n","\n","```\n","Thought: Do I need to use a tool? No\n","Final Answer: [your response here]\n","```\n","\n","Begin!\n","\n","\n","New input: Whow was King Arthur?\n","\"\"\" #{agent_scratchpad}"],"metadata":{"id":"d8B-zs3jheqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","generate(react_prompt,\n","         system_prompt=\"\",\n","         max_length=512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"1QHDuYFru352","outputId":"7cd62ab0-e70f-4247-c3d2-433ffcb6e887"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"```\nThought: Do I need to use a tool? Yes\nAction: wikipedia_search\nAction Input: King Arthur\n```"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 13 ms, sys: 1.93 ms, total: 14.9 ms\n","Wall time: 1.17 s\n"]}]},{"cell_type":"code","source":["\n","react_prompt = \"\"\"Assistant is a large language model trained by Microsoft.\n","\n","Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n","\n","Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n","\n","Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n","\n","TOOLS:\n","------\n","\n","Assistant has access to the following tools:\n","\n","wikipedia_search - searches the wikipedia database for the answer\n","web_search - searches the web for the answer\n","calculator - calculates the answer to the question\n","weather_api - gets the weather for the location\n","\n","\n","To use a tool, please use the following format:\n","\n","```\n","Thought: Do I need to use a tool? Yes\n","Action: the action to take, should be one of [wikipedia_search, web_search, calculator, weather_api]\n","Action Input: the input to the action\n","Observation: the result of the action\n","```\n","\n","When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n","\n","```\n","Thought: Do I need to use a tool? No\n","Final Answer: [your response here]\n","```\n","\n","Begin!\n","\n","\n","New input: What is the latest AI news today?\n","\"\"\" #{agent_scratchpad}"],"metadata":{"id":"hgVF-vS_vVNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","generate(react_prompt,\n","         system_prompt=\"\",\n","         max_length=512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"fkPh1iSOvCQF","outputId":"0dc89752-533e-4bd3-a0f0-90ff0aa2f3ae"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"```\nThought: Do I need to use a tool? Yes\nAction: web_search\nAction Input: latest AI news\n```"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 14.9 ms, sys: 1.16 ms, total: 16.1 ms\n","Wall time: 1.15 s\n"]}]},{"cell_type":"markdown","source":["## Functions Calling"],"metadata":{"id":"ULKD8H9b5YeP"}},{"cell_type":"code","source":["import os\n","import openai\n","from openai import OpenAI\n","import json\n","from datetime import datetime\n","\n","# Set up your OpenAI API key\n","# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n","\n","# Example 1: Weather Information\n","def get_weather(location, unit=\"celsius\"):\n","    # This is a mock function. In a real scenario, you'd call a weather API.\n","    return f\"The weather in {location} is sunny and 25 degrees {unit}.\"\n","\n","def example_weather():\n","    functions = [\n","        {\n","            \"name\": \"get_weather\",\n","            \"description\": \"Get the current weather in a location\",\n","            \"parameters\": {\n","                \"type\": \"object\",\n","                \"properties\": {\n","                    \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"},\n","                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n","                },\n","                \"required\": [\"location\"]\n","            }\n","        }\n","    ]\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini-2024-07-18\",\n","        messages=[{\"role\": \"user\", \"content\": \"What's the weather like in Singapore?\"}],\n","        functions=functions,\n","        function_call=\"auto\"\n","    )\n","\n","    function_call = response.choices[0].message.function_call\n","    if function_call and function_call.name == \"get_weather\":\n","        arguments = json.loads(function_call.arguments)\n","        weather_result = get_weather(arguments[\"location\"])\n","        print(f\"Weather: {weather_result}\")\n","        print(response.choices[0].message.function_call)\n","\n","example_weather()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5bOci1hwQ0C","outputId":"5da3d86b-b450-4e11-a9fd-3c4978335c17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Weather: The weather in Singapore is sunny and 25 degrees celsius.\n","FunctionCall(arguments='{\"location\":\"Singapore\"}', name='get_weather')\n"]}]},{"cell_type":"code","source":["# Example 2: Calculator\n","def calculate(operation, x, y):\n","    if operation == \"add\":\n","        return x + y\n","    elif operation == \"subtract\":\n","        return x - y\n","    elif operation == \"multiply\":\n","        return x * y\n","    elif operation == \"divide\":\n","        return x / y if y != 0 else \"Error: Division by zero\"\n","\n","def example_calculator():\n","    functions = [\n","        {\n","            \"name\": \"calculate\",\n","            \"description\": \"Perform a calculation\",\n","            \"parameters\": {\n","                \"type\": \"object\",\n","                \"properties\": {\n","                    \"operation\": {\"type\": \"string\", \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]},\n","                    \"x\": {\"type\": \"number\"},\n","                    \"y\": {\"type\": \"number\"}\n","                },\n","                \"required\": [\"operation\", \"x\", \"y\"]\n","            }\n","        }\n","    ]\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini-2024-07-18\",\n","        messages=[{\"role\": \"user\", \"content\": \"What's 15 multiplied by 7?\"}],\n","        functions=functions,\n","        function_call=\"auto\"\n","    )\n","\n","    function_call = response.choices[0].message.function_call\n","    if function_call and function_call.name == \"calculate\":\n","        arguments = json.loads(function_call.arguments)\n","        result = calculate(arguments[\"operation\"], arguments[\"x\"], arguments[\"y\"])\n","        print(f\"Calculation result: {result}\")\n","        print(response.choices[0].message.function_call)\n","\n","example_calculator()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxRHoYc4xcbz","outputId":"9f2b8d21-62d8-4d7b-81f9-a3e7801a1bc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculation result: 105\n","FunctionCall(arguments='{\"operation\":\"multiply\",\"x\":15,\"y\":7}', name='calculate')\n"]}]},{"cell_type":"code","source":["# Example 3: Language Translation (mock)\n","def translate(text, target_language):\n","    # This is a mock function. In a real scenario, you'd use a translation API.\n","    translations = {\n","        \"hello\": {\"french\": \"bonjour\", \"spanish\": \"hola\", \"german\": \"hallo\"},\n","        \"goodbye\": {\"french\": \"au revoir\", \"spanish\": \"adi√≥s\", \"german\": \"auf wiedersehen\"}\n","    }\n","    return translations.get(text.lower(), {}).get(target_language.lower(), \"Translation not available\")\n","\n","def example_translation():\n","    functions = [\n","        {\n","            \"name\": \"translate\",\n","            \"description\": \"Translate text to a target language\",\n","            \"parameters\": {\n","                \"type\": \"object\",\n","                \"properties\": {\n","                    \"text\": {\"type\": \"string\", \"description\": \"The text to translate\"},\n","                    \"target_language\": {\"type\": \"string\", \"description\": \"The language to translate to\"}\n","                },\n","                \"required\": [\"text\", \"target_language\"]\n","            }\n","        }\n","    ]\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini-2024-07-18\",\n","        messages=[{\"role\": \"user\", \"content\": \"How do you say 'hello' in French?\"}],\n","        functions=functions,\n","        function_call=\"auto\"\n","    )\n","\n","    function_call = response.choices[0].message.function_call\n","    if function_call and function_call.name == \"translate\":\n","        arguments = json.loads(function_call.arguments)\n","        result = translate(arguments[\"text\"], arguments[\"target_language\"])\n","        print(f\"Translation: {result}\")\n","        print(response.choices[0].message.function_call)\n","    else:\n","        print(response.choices[0].message)\n","\n","example_translation()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBbbWqMWxcno","outputId":"0a6d15e8-e92c-4d47-fff7-aefe9fd4f1b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ChatCompletionMessage(content='In French, \\'hello\\' is said as \"bonjour.\"', role='assistant', function_call=None, tool_calls=None)\n"]}]},{"cell_type":"code","source":["# Example 4: Structured Data Response\n","def get_user_info(user_id):\n","    # This is a mock function. In a real scenario, you'd fetch this from a database.\n","    users = {\n","        \"1\": {\"name\": \"Alice\", \"age\": 30, \"occupation\": \"Engineer\"},\n","        \"2\": {\"name\": \"Bob\", \"age\": 25, \"occupation\": \"Designer\"},\n","        \"3\": {\"name\": \"Charlie\", \"age\": 35, \"occupation\": \"Manager\"}\n","    }\n","    return users.get(user_id, \"User not found\")\n","\n","def example_structured_data():\n","    functions = [\n","        {\n","            \"name\": \"get_user_info\",\n","            \"description\": \"Get information about a user\",\n","            \"parameters\": {\n","                \"type\": \"object\",\n","                \"properties\": {\n","                    \"user_id\": {\"type\": \"string\", \"description\": \"The ID of the user\"}\n","                },\n","                \"required\": [\"user_id\"]\n","            }\n","        }\n","    ]\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini-2024-07-18\",\n","        messages=[{\"role\": \"user\", \"content\": \"Get me information about user with ID 2\"}],\n","        functions=functions,\n","        function_call=\"auto\"\n","    )\n","\n","    function_call = response.choices[0].message.function_call\n","    if function_call and function_call.name == \"get_user_info\":\n","        arguments = json.loads(function_call.arguments)\n","        result = get_user_info(arguments[\"user_id\"])\n","        print(f\"User Info: {json.dumps(result, indent=2)}\")\n","        print(response.choices[0].message.function_call)\n","\n","\n","example_structured_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFjoNRMqvviN","outputId":"15c2accb-1398-49f4-db14-4fbc6e23e02e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["User Info: {\n","  \"name\": \"Bob\",\n","  \"age\": 25,\n","  \"occupation\": \"Designer\"\n","}\n","FunctionCall(arguments='{\"user_id\":\"2\"}', name='get_user_info')\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BIoLky24vywe"},"execution_count":null,"outputs":[]}]}