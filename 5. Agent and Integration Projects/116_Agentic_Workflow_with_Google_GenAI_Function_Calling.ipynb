{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Agentic Workflow with Google GenAI Function Calling\n",
        "Made by: Wilfredo Aaron Sosa Ramos (AI Lab Manager at RealityAI Labs)"
      ],
      "metadata": {
        "id": "GdrMF8R77Q78"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HVmMlt6o0gnF"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai google-ai-generativelanguage langchain langchain-community langchain-core langgraph langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "KUIUEMpM0twS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_google_genai = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)"
      ],
      "metadata": {
        "id": "WpAKz4K41Ox2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class QuestionChoice(BaseModel):\n",
        "    key: str = Field(description=\"A unique identifier for the choice using letters A, B, C, or D.\")\n",
        "    value: str = Field(description=\"The text content of the choice\")\n",
        "class MultipleChoiceQuestion(BaseModel):\n",
        "    question: str = Field(description=\"The question text\")\n",
        "    choices: List[QuestionChoice] = Field(description=\"A list of choices for the question, each with a key and a value\")\n",
        "    answer: str = Field(description=\"The key of the correct answer from the choices list\")\n",
        "    explanation: str = Field(description=\"An explanation of why the answer is correct\")\n",
        "\n",
        "class OpenEndedQuestion(BaseModel):\n",
        "    question: str = Field(description=\"The open-ended question text\")\n",
        "    answer: str = Field(description=\"The expected correct answer\")\n",
        "    feedback: List[str] = Field(description=\"A list of possible answers for the provided question\")\n",
        "\n",
        "class QuestionList(BaseModel):\n",
        "    multiple_choice_questions: List[MultipleChoiceQuestion]\n",
        "    open_ended_questions: List[OpenEndedQuestion]"
      ],
      "metadata": {
        "id": "JOqi8bBX1b5i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adapt_questions_for_json(output, parser):\n",
        "    return {\n",
        "        \"questions\": output,\n",
        "        \"format_instructions\": parser.get_format_instructions()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "l885DeE_5wIP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Translation tool:"
      ],
      "metadata": {
        "id": "Yt-FEqSk9AbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translation_tool(user_query: str, source_lang: str, target_lang: str):\n",
        "    \"\"\" Used for translating content from a source language to a target language. \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"\"\"You are a professional translator specializing in converting text into multiple languages with exceptional accuracy, cultural sensitivity, and tone preservation.\n",
        "                  Follow these precise instructions to ensure high-quality translations:\n",
        "\n",
        "                1. **Accuracy:** Reflect the exact meaning of the original text while adapting grammar and syntax to the target language.\n",
        "                2. **Cultural Context:** Incorporate cultural nuances, idiomatic expressions, and regional variations relevant to the target audience.\n",
        "                3. **Tone Consistency:** Maintain the tone and style of the original text, whether formal, technical, persuasive, or conversational.\n",
        "                4. **Language Expertise:** Leverage advanced linguistic knowledge to handle complex terms, technical jargon, or ambiguous phrases appropriately.\n",
        "                5. **Output Clarity:** Present translations cleanly and clearly, ensuring each language is distinct and readable.\"\"\"\n",
        "            ),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"\"\"\n",
        "                Here you have the content to translate: {user_query}\n",
        "                You have to translate it from {source_lang} to {target_lang}\n",
        "\n",
        "                Don't return the analysis, just return the final output\n",
        "\n",
        "                \"\"\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    chain = prompt | chat_google_genai\n",
        "    result = chain.invoke({\n",
        "        \"user_query\": user_query,\n",
        "        \"source_lang\": source_lang,\n",
        "        \"target_lang\": target_lang\n",
        "    })\n",
        "    return result.content"
      ],
      "metadata": {
        "id": "x7bZEqcO9DHj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Summarization tool:"
      ],
      "metadata": {
        "id": "ct6hBy5qITYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarization_tool(user_query: str, summarization_type: str):\n",
        "    \"\"\" Used for summarize content\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"\"\"\n",
        "                You are a professional summarization specialist, responsible for condensing information into concise and coherent formats while preserving key details and maintaining contextual accuracy.\n",
        "                Follow these guidelines for effective summarization:\n",
        "\n",
        "                1. **Clarity and Brevity:** Eliminate unnecessary details while ensuring the summary is clear and directly conveys the main points.\n",
        "                2. **Preserve Context:** Maintain the original meaning, emphasizing critical ideas, facts, or insights.\n",
        "                3. **Flexible Formats:** Decide whether to present the summary as a single sentence, a paragraph, or bullet points, based on the content and what best conveys the information.\n",
        "                If the user specifies a format, follow their instructions.\n",
        "                4. **Audience Awareness:** Tailor the language, tone, and level of detail to suit the target audience.\n",
        "                5. **Logical Flow:** Organize information logically, ensuring seamless understanding and flow.\n",
        "\n",
        "                Below are examples demonstrating different summarization formats:\n",
        "\n",
        "                ---\n",
        "\n",
        "                Original text:\n",
        "                \"The recent advancements in artificial intelligence have led to significant breakthroughs in natural language processing. These include improved machine translation, more accurate\n",
        "                speech recognition, and the ability to generate human-like text, transforming industries like healthcare, education, and customer service.\"\n",
        "\n",
        "                **If single sentence format is best:**\n",
        "                \"Advancements in AI have revolutionized natural language processing, enabling better translation, speech recognition, and text generation across various industries.\"\n",
        "\n",
        "                **If paragraph format is best:**\n",
        "                \"Recent progress in artificial intelligence has transformed natural language processing, with breakthroughs in machine translation, speech recognition, and text generation.\n",
        "                These advancements are driving innovation in industries such as healthcare, education, and customer service.\"\n",
        "\n",
        "                **If bullet points format is best:**\n",
        "                - AI advancements revolutionize natural language processing.\n",
        "                - Key breakthroughs:\n",
        "                  - Enhanced machine translation.\n",
        "                  - Improved speech recognition.\n",
        "                  - Human-like text generation.\n",
        "                - Impact on industries: healthcare, education, customer service.\n",
        "\n",
        "\n",
        "                \"\"\"\n",
        "            ),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"\"\"\n",
        "                Here is the content that you must summarize: {user_query}\n",
        "                And here is the summarization type: {summarization_type}\n",
        "                You must summarize in the language of the given user query.\n",
        "                \"\"\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    chain = prompt | chat_google_genai\n",
        "    result = chain.invoke({\n",
        "        \"user_query\": user_query,\n",
        "        \"summarization_type\": summarization_type\n",
        "    })\n",
        "    return result.content"
      ],
      "metadata": {
        "id": "sSyF2GSvInWc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Rewrite tool:"
      ],
      "metadata": {
        "id": "_5T0CSD8ITUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rewrite_tool(user_query: str, focus: str, tone: str):\n",
        "    \"\"\" Used for rewriting content\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"\"\"\n",
        "                You are a professional rewriter specializing in modifying text to improve clarity, adjust tone, or provide alternative phrasing.\n",
        "                Use your expertise to decide whether to focus on clarity, tone adjustment, or alternative phrasing based on the content's context unless the user specifies their preference.\n",
        "                Follow these guidelines:\n",
        "\n",
        "                1. **Clarity and Precision:** Ensure the rewritten text is clear, concise, and easy to understand, eliminating ambiguities or overly complex language.\n",
        "                2. **Tone Adaptation:** Adjust the tone to suit the context and audience, whether formal, professional, conversational, or creative.\n",
        "                3. **Preserve Meaning:** Retain the original intent and key details while enhancing readability or style.\n",
        "                4. **Alternative Phrasing:** Provide multiple phrasings for variety or emphasis if it improves the content's impact.\n",
        "                5. **Audience-Centric:** Tailor the rewrite to meet the expectations and preferences of the target audience.\n",
        "\n",
        "                Below are examples demonstrating how to rewrite content effectively:\n",
        "\n",
        "                ---\n",
        "\n",
        "                Original text:\n",
        "                \"The new policy aims to improve employee satisfaction by offering flexible work hours and remote work opportunities.\"\n",
        "\n",
        "                **If clarity is the focus:**\n",
        "                \"The updated policy is designed to enhance employee satisfaction by introducing flexible work schedules and options for remote work.\"\n",
        "\n",
        "                **If tone adjustment is required (formal):**\n",
        "                \"The revised policy seeks to promote greater employee satisfaction by implementing flexible working hours and opportunities for remote employment.\"\n",
        "\n",
        "                **If tone adjustment is required (conversational):**\n",
        "                \"We’ve updated our policy to make employees happier, giving them more flexibility with work hours and the option to work remotely.\"\n",
        "\n",
        "                **If alternative phrasing is appropriate:**\n",
        "                1. \"To boost employee satisfaction, the new policy includes flexible scheduling and remote work possibilities.\"\n",
        "                2. \"Flexible hours and remote work are key elements of the new policy aimed at enhancing employee happiness.\"\n",
        "                \"\"\"\n",
        "            ),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"\"\"\n",
        "                Here is the content that you must rewrite: {user_query}\n",
        "                Here is the focus for the rewriting task: {focus}\n",
        "                And here is the tone for the rewriting task: {tone}\n",
        "                You must rewrite in the language of the given user query.\n",
        "                \"\"\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    chain = prompt | chat_google_genai\n",
        "    result = chain.invoke({\n",
        "        \"user_query\": user_query,\n",
        "        \"focus\": focus,\n",
        "        \"tone\": tone\n",
        "    })\n",
        "    return result.content"
      ],
      "metadata": {
        "id": "IFQQSBGhJNPU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Custom prompt tool:"
      ],
      "metadata": {
        "id": "4rA4f1RmIcWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_prompt_tool(user_query: str, action: str):\n",
        "    \"\"\" Used for managing custom prompts\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"\"\"\n",
        "                You are a professional assistant specializing in creating, managing, and reusing user-defined custom prompts. Your role is to facilitate the creation, saving,\n",
        "                and application of custom prompts to ensure users can easily define and reuse specific instructions for consistent and efficient results. Follow these guidelines:\n",
        "\n",
        "                1. **Define Custom Prompts:**\n",
        "                  - Assist users in crafting clear, detailed, and purpose-driven custom prompts tailored to their needs.\n",
        "                  - Ensure the prompts are actionable and cover all necessary instructions.\n",
        "\n",
        "                2. **Save Prompts:**\n",
        "                  - Label prompts descriptively for easy identification.\n",
        "                  - Organize prompts into categories or tags if necessary for efficient retrieval.\n",
        "\n",
        "                3. **Reuse Prompts:**\n",
        "                  - Allow users to apply saved prompts directly to new tasks or content.\n",
        "                  - Offer customization options to adapt previously saved prompts to new contexts.\n",
        "\n",
        "                4. **User Preferences:**\n",
        "                  - Default to user-defined preferences when saved prompts exist.\n",
        "                  - Suggest improvements to prompts if they can be optimized for better performance.\n",
        "\n",
        "                5. **Prompt History Management:**\n",
        "                  - Maintain a record of frequently used prompts for quick access.\n",
        "                  - Enable users to update, rename, or delete prompts as needed.\n",
        "                \"\"\"\n",
        "            ),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"\"\"\n",
        "                Here is the custom prompt: {user_query}\n",
        "                And here is the action that must be taken: {action}\n",
        "                \"\"\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    chain = prompt | chat_google_genai\n",
        "    result = chain.invoke({\n",
        "        \"user_query\": user_query,\n",
        "        \"action\": action\n",
        "    })\n",
        "    return result.content"
      ],
      "metadata": {
        "id": "5yGT8TUaMJZB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Question Generation tool:"
      ],
      "metadata": {
        "id": "1i38t0VM89WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.base import RunnableMap, RunnableLambda\n",
        "\n",
        "def generate_questions_to_json_tool(user_query: str):\n",
        "    \"\"\" Used for generating questions. The results are returned in a JSON format \"\"\"\n",
        "    prompt_generate_questions = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\n",
        "              \"system\",\n",
        "              \"You are a specialized creator of multiple choice and open-ended questions.\"\n",
        "          ),\n",
        "          (\"human\",\n",
        "          \"\"\"\n",
        "            You are a professional in creating high-quality questions for assessments, discussions, or educational purposes. Based on the provided content, generate multiple-choice and/or free-response questions. Unless the user specifies otherwise, you should independently decide the type, number, and format of the questions. If the user provides specific instructions, prioritize their preferences over your decision-making. Follow these guidelines:\n",
        "\n",
        "            1. **Clarity and Relevance:** Ensure each question is concise, focused, and directly aligned with the content.\n",
        "            2. **Dynamic Formats:** Choose the type (multiple-choice or free-response) and number of questions based on the complexity and depth of the content. Use a combination of both formats when appropriate.\n",
        "            3. **Multiple-Choice Questions:**\n",
        "              - Include one correct answer and 2–4 plausible distractors.\n",
        "              - Distractors should be logical, relevant, and neither overly obvious nor misleading.\n",
        "            4. **Free-Response Questions:**\n",
        "              - Use open-ended prompts that encourage thoughtful and detailed responses.\n",
        "              - Focus on analytical, reflective, or application-based phrasing to elicit deeper understanding.\n",
        "            5. **Challenge Level:** Adjust the difficulty of questions to match the intended audience’s knowledge level, ranging from basic recall to advanced critical thinking.\n",
        "\n",
        "            Generate a question for the following query: {user_query}. Generate questions in the language of the given user query.\n",
        "          \"\"\"\n",
        "          )\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    chain_generate_questions = prompt_generate_questions | chat_google_genai\n",
        "\n",
        "    parser = JsonOutputParser(pydantic_object=QuestionList)\n",
        "    prompt_questions_to_json = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a specialized creator of multiple choice and open-ended questions in JSON.\"\n",
        "        ),\n",
        "        (\"human\",\n",
        "         \"\"\"\n",
        "         Please convert the following questions into JSON format:\n",
        "          **Tasks:**\n",
        "          1. For Multiple Choice Questions:\n",
        "            - Include the question text.\n",
        "            - Provide all choices, each with a unique key (e.g., A, B, C, D) and its corresponding value (text content).\n",
        "            - Specify the correct answer using the key of the appropriate choice.\n",
        "            - Add a brief explanation of why the answer is correct.\n",
        "\n",
        "          2. For Open-Ended Questions:\n",
        "            - Include the question text.\n",
        "            - Provide the expected correct answer.\n",
        "            - Add a list of possible feedback answers, reflecting different valid responses to the question.\n",
        "\n",
        "          3. Ensure all questions are formatted as JSON objects and grouped in a list.\n",
        "\n",
        "          **Input Questions:**\n",
        "          {questions}\n",
        "\n",
        "          **Output:**\n",
        "          You must respond as a JSON object:\n",
        "          {format_instructions}\n",
        "         \"\"\"\n",
        "        )\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    chain_questions_to_json = prompt_questions_to_json | chat_google_genai\n",
        "    combined_chain = (\n",
        "        chain_generate_questions\n",
        "        | RunnableLambda(lambda x: adapt_questions_for_json(x.content, parser))\n",
        "        | chain_questions_to_json\n",
        "        | parser\n",
        "    )\n",
        "\n",
        "    result = combined_chain.invoke(user_query)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "L0_AVdHJ0-BO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_questions_to_json_tool(\"Large Language Models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_63E3Wxz5dyK",
        "outputId": "4a2a86c5-72e8-4177-a4b8-3fe021893cc8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'multiple_choice_questions': [{'question': 'Which of the following best describes a Large Language Model (LLM)?',\n",
              "   'choices': [{'key': 'A',\n",
              "     'value': 'A type of computer program that only understands numerical data.'},\n",
              "    {'key': 'B',\n",
              "     'value': 'A machine learning model trained on a massive dataset of text and code.'},\n",
              "    {'key': 'C',\n",
              "     'value': 'A simple algorithm that translates languages word-for-word.'},\n",
              "    {'key': 'D',\n",
              "     'value': 'A database that stores information about different languages.'}],\n",
              "   'answer': 'B',\n",
              "   'explanation': 'LLMs are machine learning models trained on vast amounts of text and code data, enabling them to understand and generate human-like text.'},\n",
              "  {'question': 'What is a key characteristic of LLMs that allows them to generate human-like text?',\n",
              "   'choices': [{'key': 'A',\n",
              "     'value': 'Their ability to perform complex mathematical calculations.'},\n",
              "    {'key': 'B',\n",
              "     'value': 'Their use of pre-programmed rules for grammar and syntax.'},\n",
              "    {'key': 'C',\n",
              "     'value': 'Their ability to learn patterns and relationships in vast amounts of text data.'},\n",
              "    {'key': 'D',\n",
              "     'value': 'Their reliance on a limited set of vocabulary and sentence structures.'}],\n",
              "   'answer': 'C',\n",
              "   'explanation': 'LLMs learn from the patterns and relationships in the text data they are trained on, which allows them to generate coherent and contextually relevant text.'},\n",
              "  {'question': 'Which of the following is a common application of Large Language Models?',\n",
              "   'choices': [{'key': 'A',\n",
              "     'value': 'Controlling robotic arms in manufacturing.'},\n",
              "    {'key': 'B', 'value': 'Predicting stock market fluctuations.'},\n",
              "    {'key': 'C',\n",
              "     'value': 'Generating creative content, such as poems and articles.'},\n",
              "    {'key': 'D', 'value': 'Analyzing geological data for oil exploration.'}],\n",
              "   'answer': 'C',\n",
              "   'explanation': 'LLMs are frequently used for generating creative content due to their ability to understand and produce human-like text.'}],\n",
              " 'open_ended_questions': [{'question': 'Explain the concept of \"training\" in the context of Large Language Models. What does it mean for an LLM to be \"trained\" on a large dataset, and how does this process affect its capabilities?',\n",
              "   'answer': 'Training an LLM involves exposing it to a massive dataset of text and code, allowing it to learn patterns, relationships, and statistical probabilities within the data. This process enables the LLM to generate text, translate languages, and perform other language-related tasks.',\n",
              "   'feedback': ['Training involves exposing the LLM to a large dataset to learn patterns.',\n",
              "    'Training allows the LLM to understand language and generate text.',\n",
              "    'The training process enables the LLM to perform various language-related tasks.']},\n",
              "  {'question': 'Discuss the potential ethical concerns associated with the use of Large Language Models. Consider issues such as bias, misinformation, and the impact on human jobs.',\n",
              "   'answer': 'Ethical concerns include the potential for bias in LLMs due to biased training data, the spread of misinformation through generated text, and the displacement of human jobs due to automation. These issues require careful consideration and mitigation strategies.',\n",
              "   'feedback': ['LLMs can exhibit bias due to biased training data.',\n",
              "    'LLMs can be used to spread misinformation.',\n",
              "    'LLMs may lead to job displacement due to automation.']},\n",
              "  {'question': 'Beyond text generation, what are some other potential applications of Large Language Models? Provide examples and explain how LLMs could be used in these contexts.',\n",
              "   'answer': 'Beyond text generation, LLMs can be used for tasks such as code generation, question answering, summarization, and even creative tasks like music composition. For example, LLMs can assist developers by generating code snippets or help researchers by summarizing large volumes of text.',\n",
              "   'feedback': ['LLMs can be used for code generation.',\n",
              "    'LLMs can be used for question answering and summarization.',\n",
              "    'LLMs can be used for creative tasks like music composition.']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Default prompt:"
      ],
      "metadata": {
        "id": "IMUAg0rTIjXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def default_prompt_tool(user_query: str):\n",
        "    \"\"\" Used for answering any kind of prompt that is not about Translation, Summarization, Rewriting, Question Generation or Custom Prompts\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"\"\"\n",
        "                You are the CoTeacher Assistant, designed to provide personalized and adaptive guidance to educators. Your role is to enhance the teaching experience by being context-aware\n",
        "                and role-specific, tailoring your interactions to the needs of teachers. Based on the context provided in the interface, follow these professional guidelines:\n",
        "\n",
        "                1. **Dynamic Adaptation:** Understand the user's specific teaching context, such as lesson planning, classroom management, or student engagement. Adapt your responses to provide\n",
        "                highly relevant and actionable insights.\n",
        "\n",
        "                2. **Role-Specific Support:** Serve as a collaborative partner in teaching, offering solutions that are practical, evidence-based, and aligned with best practices in education.\n",
        "\n",
        "                3. **Clarity and Precision:** Ensure your responses are clear, concise, and directly address the user's inquiry or task.\n",
        "\n",
        "                4. **Empathetic and Encouraging Tone:** Maintain a supportive and positive tone, empowering educators to achieve their goals with confidence.\n",
        "\n",
        "                5. **Proactive Engagement:** Anticipate potential needs based on the user's input. For example:\n",
        "                  - Offer tailored strategies for differentiated instruction.\n",
        "                  - Suggest effective classroom management techniques.\n",
        "                  - Provide constructive feedback on lesson plans.\n",
        "                \"\"\"\n",
        "            ),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"\"\"\n",
        "                Here is the user's query: {user_query}\n",
        "                \"\"\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    chain = prompt | chat_google_genai\n",
        "    result = chain.invoke({\n",
        "        \"user_query\": user_query\n",
        "    })\n",
        "    return result.content"
      ],
      "metadata": {
        "id": "c9AUvN_VIlQj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.ai.generativelanguage_v1beta.types import content\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "krnxmAaD7pif"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    translation_tool,\n",
        "    summarization_tool,\n",
        "    rewrite_tool,\n",
        "    generate_questions_to_json_tool,\n",
        "    custom_prompt_tool,\n",
        "    default_prompt_tool\n",
        "]\n",
        "\n",
        "model = genai.GenerativeModel(model_name='gemini-2.0-flash-exp',\n",
        "                              tools=tools)"
      ],
      "metadata": {
        "id": "C31xi75d7tYr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model._tools.to_proto()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzusUAbP7xK7",
        "outputId": "75bbb0b1-c55d-49ca-f5ad-8df22183493e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[function_declarations {\n",
              "   name: \"translation_tool\"\n",
              "   description: \" Used for translating content from a source language to a target language. \"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"user_query\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"target_lang\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"source_lang\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     required: \"user_query\"\n",
              "     required: \"source_lang\"\n",
              "     required: \"target_lang\"\n",
              "   }\n",
              " }\n",
              " function_declarations {\n",
              "   name: \"summarization_tool\"\n",
              "   description: \" Used for summarize content\"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"user_query\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"summarization_type\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     required: \"user_query\"\n",
              "     required: \"summarization_type\"\n",
              "   }\n",
              " }\n",
              " function_declarations {\n",
              "   name: \"rewrite_tool\"\n",
              "   description: \" Used for rewriting content\"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"user_query\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"tone\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"focus\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     required: \"user_query\"\n",
              "     required: \"focus\"\n",
              "     required: \"tone\"\n",
              "   }\n",
              " }\n",
              " function_declarations {\n",
              "   name: \"generate_questions_to_json_tool\"\n",
              "   description: \" Used for generating questions. The results are returned in a JSON format \"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"user_query\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     required: \"user_query\"\n",
              "   }\n",
              " }\n",
              " function_declarations {\n",
              "   name: \"custom_prompt_tool\"\n",
              "   description: \" Used for managing custom prompts\"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"user_query\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"action\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     required: \"user_query\"\n",
              "     required: \"action\"\n",
              "   }\n",
              " }\n",
              " function_declarations {\n",
              "   name: \"default_prompt_tool\"\n",
              "   description: \" Used for answering any kind of prompt that is not about Translation, Summarization, Rewriting, Question Generation or Custom Prompts\"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"user_query\"\n",
              "       value {\n",
              "         type_: STRING\n",
              "       }\n",
              "     }\n",
              "     required: \"user_query\"\n",
              "   }\n",
              " }]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Examples (Without automating agentic workflow):"
      ],
      "metadata": {
        "id": "ZNDrQ4dUBc7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Complex task: Translation + Question Generation"
      ],
      "metadata": {
        "id": "TClSSHxBB09a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat()\n",
        "response = chat.send_message(\"\"\"Could you convert 'Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language.\n",
        "These models leverage deep learning techniques, particularly transformer architectures, to process and generate text across a wide range of contexts and tasks. LLMs are capable\n",
        "of performing diverse functions, including language translation, summarization, content creation, and even complex problem-solving. Their capabilities continue to expand as they are fine-tuned\n",
        "for specific applications, making them invaluable tools in industries such as healthcare, education, customer support, and software development.' into Spanish and after that creating questions from it?\"\"\")"
      ],
      "metadata": {
        "id": "7xeeNaPj74qy"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvCw4Q7n8WwR",
        "outputId": "2f1f16ac-8eb0-4283-f616-fd1eaa5c2073"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"function_call\": {\n",
              "                  \"name\": \"translation_tool\",\n",
              "                  \"args\": {\n",
              "                    \"user_query\": \"Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language.\\\\nThese models leverage deep learning techniques, particularly transformer architectures, to process and generate text across a wide range of contexts and tasks. LLMs are capable\\\\nof performing diverse functions, including language translation, summarization, content creation, and even complex problem-solving. Their capabilities continue to expand as they are fine-tuned\\\\nfor specific applications, making them invaluable tools in industries such as healthcare, education, customer support, and software development.\",\n",
              "                    \"target_lang\": \"Spanish\",\n",
              "                    \"source_lang\": \"English\"\n",
              "                  }\n",
              "                }\n",
              "              },\n",
              "              {\n",
              "                \"text\": \"\\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"safety_ratings\": [\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            }\n",
              "          ],\n",
              "          \"citation_metadata\": {\n",
              "            \"citation_sources\": [\n",
              "              {\n",
              "                \"start_index\": 135,\n",
              "                \"end_index\": 259,\n",
              "                \"uri\": \"https://eng.unimelb.edu.au/ingenium/technology-and-society/student-programmed-office-robot-assistant-wins-top-prize-at-hri24\"\n",
              "              }\n",
              "            ]\n",
              "          },\n",
              "          \"avg_logprobs\": -0.007890262387015602\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 267,\n",
              "        \"candidates_token_count\": 132,\n",
              "        \"total_token_count\": 399\n",
              "      }\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.parts:\n",
        "    if fn := part.function_call:\n",
        "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
        "        print(f\"{fn.name}({args})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s74xM0KH_Ze4",
        "outputId": "f6c8b1a0-b4be-412a-ed7a-dba35dcaaa0f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "translation_tool(user_query=Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language.\\nThese models leverage deep learning techniques, particularly transformer architectures, to process and generate text across a wide range of contexts and tasks. LLMs are capable\\nof performing diverse functions, including language translation, summarization, content creation, and even complex problem-solving. Their capabilities continue to expand as they are fine-tuned\\nfor specific applications, making them invaluable tools in industries such as healthcare, education, customer support, and software development., target_lang=Spanish, source_lang=English)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc = response.candidates[0].content.parts[0].function_call"
      ],
      "metadata": {
        "id": "laGeDWtn_bOe"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmK4bsOC_hHE",
        "outputId": "3aa7bf06-8651-4a18-e194-590690f51051"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "name: \"translation_tool\"\n",
              "args {\n",
              "  fields {\n",
              "    key: \"user_query\"\n",
              "    value {\n",
              "      string_value: \"Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language.\\\\nThese models leverage deep learning techniques, particularly transformer architectures, to process and generate text across a wide range of contexts and tasks. LLMs are capable\\\\nof performing diverse functions, including language translation, summarization, content creation, and even complex problem-solving. Their capabilities continue to expand as they are fine-tuned\\\\nfor specific applications, making them invaluable tools in industries such as healthcare, education, customer support, and software development.\"\n",
              "    }\n",
              "  }\n",
              "  fields {\n",
              "    key: \"target_lang\"\n",
              "    value {\n",
              "      string_value: \"Spanish\"\n",
              "    }\n",
              "  }\n",
              "  fields {\n",
              "    key: \"source_lang\"\n",
              "    value {\n",
              "      string_value: \"English\"\n",
              "    }\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = translation_tool(user_query = fc.args['user_query'], source_lang = fc.args['source_lang'], target_lang = fc.args['target_lang'])"
      ],
      "metadata": {
        "id": "HddZfXkL_n9N"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "B1yMHCZX_ucU",
        "outputId": "307cbc1a-f59b-44a7-ad83-65bd22c32954"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Los Modelos de Lenguaje Grandes (LLM, por sus siglas en inglés) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan técnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducción de idiomas, el resumen, la creación de contenido e incluso la resolución de problemas complejos. Sus capacidades continúan expandiéndose a medida que se ajustan para aplicaciones específicas, lo que los convierte en herramientas invaluables en industrias como la atención médica, la educación, la atención al cliente y el desarrollo de software.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    genai.protos.Content(\n",
        "    parts=[genai.protos.Part(\n",
        "        function_response = genai.protos.FunctionResponse(\n",
        "          name='translation_tool',\n",
        "          response={'result': result}))]))"
      ],
      "metadata": {
        "id": "vnsn4bIYAz9Z"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-4pGBPtA55G",
        "outputId": "a03eb6d2-9159-488d-c4f1-c6ce1f46f155"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"function_call\": {\n",
              "                  \"name\": \"generate_questions_to_json_tool\",\n",
              "                  \"args\": {\n",
              "                    \"user_query\": \"Los Modelos de Lenguaje Grandes (LLM, por sus siglas en ingl\\u00e9s) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan t\\u00e9cnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducci\\u00f3n de idiomas, el resumen, la creaci\\u00f3n de contenido e incluso la resoluci\\u00f3n de problemas complejos. Sus capacidades contin\\u00faan expandi\\u00e9ndose a medida que se ajustan para aplicaciones espec\\u00edficas, lo que los convierte en herramientas invaluables en industrias como la atenci\\u00f3n m\\u00e9dica, la educaci\\u00f3n, la atenci\\u00f3n al cliente y el desarrollo de software.\"\n",
              "                  }\n",
              "                }\n",
              "              },\n",
              "              {\n",
              "                \"text\": \"\\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"safety_ratings\": [\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            }\n",
              "          ],\n",
              "          \"avg_logprobs\": -0.002819884338496644\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 581,\n",
              "        \"candidates_token_count\": 162,\n",
              "        \"total_token_count\": 743\n",
              "      }\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.parts:\n",
        "    if fn := part.function_call:\n",
        "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
        "        print(f\"{fn.name}({args})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2UURaK9A91E",
        "outputId": "deb8f145-ee43-47c4-9fe6-0c143697055d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate_questions_to_json_tool(user_query=Los Modelos de Lenguaje Grandes (LLM, por sus siglas en inglés) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan técnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducción de idiomas, el resumen, la creación de contenido e incluso la resolución de problemas complejos. Sus capacidades continúan expandiéndose a medida que se ajustan para aplicaciones específicas, lo que los convierte en herramientas invaluables en industrias como la atención médica, la educación, la atención al cliente y el desarrollo de software.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc = response.candidates[0].content.parts[0].function_call"
      ],
      "metadata": {
        "id": "MrN5dB_vA-za"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCGkB9dCBB9k",
        "outputId": "f2eb0b8d-a49f-4c1c-eab1-e1f7313739e1"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "name: \"generate_questions_to_json_tool\"\n",
              "args {\n",
              "  fields {\n",
              "    key: \"user_query\"\n",
              "    value {\n",
              "      string_value: \"Los Modelos de Lenguaje Grandes (LLM, por sus siglas en inglés) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan técnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducción de idiomas, el resumen, la creación de contenido e incluso la resolución de problemas complejos. Sus capacidades continúan expandiéndose a medida que se ajustan para aplicaciones específicas, lo que los convierte en herramientas invaluables en industrias como la atención médica, la educación, la atención al cliente y el desarrollo de software.\"\n",
              "    }\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_questions_to_json_tool(user_query = fc.args['user_query'])"
      ],
      "metadata": {
        "id": "-8ksvxPKBC9Y"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-AK3b29Bq-h",
        "outputId": "0fff136c-d86d-4af1-e06f-2e23e8b2d3af"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'multiple_choice_questions': [{'question': '¿Qué son los Modelos de Lenguaje Grandes (LLM)?',\n",
              "   'choices': [{'key': 'A', 'value': 'Sistemas de almacenamiento de datos.'},\n",
              "    {'key': 'B',\n",
              "     'value': 'Sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto.'},\n",
              "    {'key': 'C', 'value': 'Programas de edición de texto.'},\n",
              "    {'key': 'D', 'value': 'Herramientas de diseño gráfico.'}],\n",
              "   'answer': 'B',\n",
              "   'explanation': 'Los LLM son sistemas de IA entrenados con grandes cantidades de texto para entender y generar lenguaje.'},\n",
              "  {'question': '¿Qué técnica de aprendizaje profundo utilizan principalmente los LLM?',\n",
              "   'choices': [{'key': 'A', 'value': 'Redes neuronales convolucionales.'},\n",
              "    {'key': 'B', 'value': 'Máquinas de vectores de soporte.'},\n",
              "    {'key': 'C', 'value': 'Arquitecturas de transformadores.'},\n",
              "    {'key': 'D', 'value': 'Algoritmos genéticos.'}],\n",
              "   'answer': 'C',\n",
              "   'explanation': 'Los LLM se basan principalmente en arquitecturas de transformadores debido a su capacidad para procesar secuencias de datos de manera eficiente.'},\n",
              "  {'question': '¿Cuál de las siguientes NO es una función típica de los LLM?',\n",
              "   'choices': [{'key': 'A', 'value': 'Traducción de idiomas.'},\n",
              "    {'key': 'B', 'value': 'Resumen de textos.'},\n",
              "    {'key': 'C', 'value': 'Creación de contenido.'},\n",
              "    {'key': 'D', 'value': 'Control de hardware físico.'}],\n",
              "   'answer': 'D',\n",
              "   'explanation': 'Los LLM se centran en el procesamiento de texto y no están diseñados para controlar hardware físico.'}],\n",
              " 'open_ended_questions': [{'question': 'Describe brevemente cómo los LLM procesan y generan texto, mencionando las técnicas clave que utilizan.',\n",
              "   'answer': 'Los LLM procesan texto utilizando redes neuronales, especialmente arquitecturas de transformadores, que les permiten entender las relaciones entre palabras y generar texto coherente. Utilizan técnicas como el auto-atención para enfocarse en las partes relevantes del texto.',\n",
              "   'feedback': ['Los LLM usan redes neuronales y transformadores para entender y generar texto.',\n",
              "    'Procesan texto mediante auto-atención para entender las relaciones entre palabras.',\n",
              "    'Utilizan técnicas de aprendizaje profundo para generar texto coherente.']},\n",
              "  {'question': 'Explica por qué los LLM se consideran herramientas valiosas en diversas industrias. Proporciona al menos tres ejemplos de aplicaciones específicas.',\n",
              "   'answer': 'Los LLM son valiosos por su capacidad para automatizar tareas de procesamiento de lenguaje, mejorar la eficiencia y generar contenido. Ejemplos incluyen: atención al cliente automatizada (chatbots), generación de contenido de marketing, y traducción de documentos.',\n",
              "   'feedback': ['Son valiosos por su capacidad para automatizar tareas de lenguaje y mejorar la eficiencia.',\n",
              "    'Ejemplos de aplicaciones son chatbots, generación de contenido y traducción.',\n",
              "    'Los LLM ayudan a automatizar tareas y generar contenido en diversas industrias.']},\n",
              "  {'question': '¿Cómo crees que la continua expansión de las capacidades de los LLM podría impactar la sociedad en el futuro? Considera tanto los beneficios como los posibles desafíos.',\n",
              "   'answer': 'La expansión de los LLM podría traer beneficios como la automatización de tareas, la mejora de la comunicación y el acceso a la información. Sin embargo, también existen desafíos como la desinformación, el desplazamiento laboral y la necesidad de regular su uso ético.',\n",
              "   'feedback': ['Podrían automatizar tareas y mejorar la comunicación, pero también generar desinformación.',\n",
              "    'Los beneficios incluyen mayor acceso a la información, pero hay desafíos como el desplazamiento laboral.',\n",
              "    'Es importante considerar tanto los beneficios como los desafíos éticos y sociales de los LLM.']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    genai.protos.Content(\n",
        "    parts=[genai.protos.Part(\n",
        "        function_response = genai.protos.FunctionResponse(\n",
        "          name='generate_questions_to_json_tool',\n",
        "          response={'result': result}))]))"
      ],
      "metadata": {
        "id": "hdIhGVIkB9OF"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWL6nDgFCT5Q",
        "outputId": "c109ca6d-04a3-4234-d8b4-2a44b1117c57"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"Okay, I've translated the text to Spanish and generated some questions based on it. Here are the results:\\n\\n**Spanish Translation:**\\n\\nLos Modelos de Lenguaje Grandes (LLM, por sus siglas en ingl\\u00e9s) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan t\\u00e9cnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducci\\u00f3n de idiomas, el resumen, la creaci\\u00f3n de contenido e incluso la resoluci\\u00f3n de problemas complejos. Sus capacidades contin\\u00faan expandi\\u00e9ndose a medida que se ajustan para aplicaciones espec\\u00edficas, lo que los convierte en herramientas invaluables en industrias como la atenci\\u00f3n m\\u00e9dica, la educaci\\u00f3n, la atenci\\u00f3n al cliente y el desarrollo de software.\\n\\n**Generated Questions:**\\n\\nHere are some multiple-choice and open-ended questions based on the translated text:\\n\\n**Multiple Choice Questions:**\\n\\n1.  **Question:** \\u00bfQu\\u00e9 son los Modelos de Lenguaje Grandes (LLM)?\\n    *   A) Sistemas de almacenamiento de datos.\\n    *   B) Sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto.\\n    *   C) Programas de edici\\u00f3n de texto.\\n    *   D) Herramientas de dise\\u00f1o gr\\u00e1fico.\\n    *   **Answer:** B\\n    *   **Explanation:** Los LLM son sistemas de IA entrenados con grandes cantidades de texto para entender y generar lenguaje.\\n\\n2.  **Question:** \\u00bfQu\\u00e9 t\\u00e9cnica de aprendizaje profundo utilizan principalmente los LLM?\\n    *   A) Redes neuronales convolucionales.\\n    *   B) M\\u00e1quinas de vectores de soporte.\\n    *   C) Arquitecturas de transformadores.\\n    *   D) Algoritmos gen\\u00e9ticos.\\n    *   **Answer:** C\\n    *   **Explanation:** Los LLM se basan principalmente en arquitecturas de transformadores debido a su capacidad para procesar secuencias de datos de manera eficiente.\\n\\n3.  **Question:** \\u00bfCu\\u00e1l de las siguientes NO es una funci\\u00f3n t\\u00edpica de los LLM?\\n    *   A) Traducci\\u00f3n de idiomas.\\n    *   B) Resumen de textos.\\n    *   C) Creaci\\u00f3n de contenido.\\n    *   D) Control de hardware f\\u00edsico.\\n    *   **Answer:** D\\n    *    **Explanation:** Los LLM se centran en el procesamiento de texto y no est\\u00e1n dise\\u00f1ados para controlar hardware f\\u00edsico.\\n\\n**Open-Ended Questions:**\\n\\n1.  **Question:** Describe brevemente c\\u00f3mo los LLM procesan y generan texto, mencionando las t\\u00e9cnicas clave que utilizan.\\n    *   **Answer:** Los LLM procesan texto utilizando redes neuronales, especialmente arquitecturas de transformadores, que les permiten entender las relaciones entre palabras y generar texto coherente. Utilizan t\\u00e9cnicas como el auto-atenci\\u00f3n para enfocarse en las partes relevantes del texto.\\n    *   **Feedback:**\\n        *   Los LLM usan redes neuronales y transformadores para entender y generar texto.\\n        *   Procesan texto mediante auto-atenci\\u00f3n para entender las relaciones entre palabras.\\n        *   Utilizan t\\u00e9cnicas de aprendizaje profundo para generar texto coherente.\\n\\n2.  **Question:** Explica por qu\\u00e9 los LLM se consideran herramientas valiosas en diversas industrias. Proporciona al menos tres ejemplos de aplicaciones espec\\u00edficas.\\n    *   **Answer:** Los LLM son valiosos por su capacidad para automatizar tareas de procesamiento de lenguaje, mejorar la eficiencia y generar contenido. Ejemplos incluyen: atenci\\u00f3n al cliente automatizada (chatbots), generaci\\u00f3n de contenido de marketing, y traducci\\u00f3n de documentos.\\n    *    **Feedback:**\\n        *   Son valiosos por su capacidad para automatizar tareas de lenguaje y mejorar la eficiencia.\\n        *   Ejemplos de aplicaciones son chatbots, generaci\\u00f3n de contenido y traducci\\u00f3n.\\n         *  Los LLM ayudan a automatizar tareas y generar contenido en diversas industrias.\\n\\n3.  **Question:** \\u00bfC\\u00f3mo crees que la continua expansi\\u00f3n de las capacidades de los LLM podr\\u00eda impactar la sociedad en el futuro? Considera tanto los beneficios como los posibles desaf\\u00edos.\\n    *   **Answer:** La expansi\\u00f3n de los LLM podr\\u00eda traer beneficios como la automatizaci\\u00f3n de tareas, la mejora de la comunicaci\\u00f3n y el acceso a la informaci\\u00f3n. Sin embargo, tambi\\u00e9n existen desaf\\u00edos como la desinformaci\\u00f3n, el desplazamiento laboral y la necesidad de regular su uso \\u00e9tico.\\n    *   **Feedback:**\\n        *   Podr\\u00edan automatizar tareas y mejorar la comunicaci\\u00f3n, pero tambi\\u00e9n generar desinformaci\\u00f3n.\\n        *   Los beneficios incluyen mayor acceso a la informaci\\u00f3n, pero hay desaf\\u00edos como el desplazamiento laboral.\\n        *   Es importante considerar tanto los beneficios como los desaf\\u00edos \\u00e9ticos y sociales de los LLM.\\n\\nI hope this is helpful! Let me know if you have other questions.\\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"safety_ratings\": [\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            }\n",
              "          ],\n",
              "          \"avg_logprobs\": -0.015742318672046327\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 1582,\n",
              "        \"candidates_token_count\": 1026,\n",
              "        \"total_token_count\": 2608\n",
              "      }\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "8gqy-JwsCalO",
        "outputId": "c12b1ef3-c965-49d5-dabe-04163f891133"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, I've translated the text to Spanish and generated some questions based on it. Here are the results:\\n\\n**Spanish Translation:**\\n\\nLos Modelos de Lenguaje Grandes (LLM, por sus siglas en inglés) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan técnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducción de idiomas, el resumen, la creación de contenido e incluso la resolución de problemas complejos. Sus capacidades continúan expandiéndose a medida que se ajustan para aplicaciones específicas, lo que los convierte en herramientas invaluables en industrias como la atención médica, la educación, la atención al cliente y el desarrollo de software.\\n\\n**Generated Questions:**\\n\\nHere are some multiple-choice and open-ended questions based on the translated text:\\n\\n**Multiple Choice Questions:**\\n\\n1.  **Question:** ¿Qué son los Modelos de Lenguaje Grandes (LLM)?\\n    *   A) Sistemas de almacenamiento de datos.\\n    *   B) Sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto.\\n    *   C) Programas de edición de texto.\\n    *   D) Herramientas de diseño gráfico.\\n    *   **Answer:** B\\n    *   **Explanation:** Los LLM son sistemas de IA entrenados con grandes cantidades de texto para entender y generar lenguaje.\\n\\n2.  **Question:** ¿Qué técnica de aprendizaje profundo utilizan principalmente los LLM?\\n    *   A) Redes neuronales convolucionales.\\n    *   B) Máquinas de vectores de soporte.\\n    *   C) Arquitecturas de transformadores.\\n    *   D) Algoritmos genéticos.\\n    *   **Answer:** C\\n    *   **Explanation:** Los LLM se basan principalmente en arquitecturas de transformadores debido a su capacidad para procesar secuencias de datos de manera eficiente.\\n\\n3.  **Question:** ¿Cuál de las siguientes NO es una función típica de los LLM?\\n    *   A) Traducción de idiomas.\\n    *   B) Resumen de textos.\\n    *   C) Creación de contenido.\\n    *   D) Control de hardware físico.\\n    *   **Answer:** D\\n    *    **Explanation:** Los LLM se centran en el procesamiento de texto y no están diseñados para controlar hardware físico.\\n\\n**Open-Ended Questions:**\\n\\n1.  **Question:** Describe brevemente cómo los LLM procesan y generan texto, mencionando las técnicas clave que utilizan.\\n    *   **Answer:** Los LLM procesan texto utilizando redes neuronales, especialmente arquitecturas de transformadores, que les permiten entender las relaciones entre palabras y generar texto coherente. Utilizan técnicas como el auto-atención para enfocarse en las partes relevantes del texto.\\n    *   **Feedback:**\\n        *   Los LLM usan redes neuronales y transformadores para entender y generar texto.\\n        *   Procesan texto mediante auto-atención para entender las relaciones entre palabras.\\n        *   Utilizan técnicas de aprendizaje profundo para generar texto coherente.\\n\\n2.  **Question:** Explica por qué los LLM se consideran herramientas valiosas en diversas industrias. Proporciona al menos tres ejemplos de aplicaciones específicas.\\n    *   **Answer:** Los LLM son valiosos por su capacidad para automatizar tareas de procesamiento de lenguaje, mejorar la eficiencia y generar contenido. Ejemplos incluyen: atención al cliente automatizada (chatbots), generación de contenido de marketing, y traducción de documentos.\\n    *    **Feedback:**\\n        *   Son valiosos por su capacidad para automatizar tareas de lenguaje y mejorar la eficiencia.\\n        *   Ejemplos de aplicaciones son chatbots, generación de contenido y traducción.\\n         *  Los LLM ayudan a automatizar tareas y generar contenido en diversas industrias.\\n\\n3.  **Question:** ¿Cómo crees que la continua expansión de las capacidades de los LLM podría impactar la sociedad en el futuro? Considera tanto los beneficios como los posibles desafíos.\\n    *   **Answer:** La expansión de los LLM podría traer beneficios como la automatización de tareas, la mejora de la comunicación y el acceso a la información. Sin embargo, también existen desafíos como la desinformación, el desplazamiento laboral y la necesidad de regular su uso ético.\\n    *   **Feedback:**\\n        *   Podrían automatizar tareas y mejorar la comunicación, pero también generar desinformación.\\n        *   Los beneficios incluyen mayor acceso a la información, pero hay desafíos como el desplazamiento laboral.\\n        *   Es importante considerar tanto los beneficios como los desafíos éticos y sociales de los LLM.\\n\\nI hope this is helpful! Let me know if you have other questions.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.parts:\n",
        "    if fn := part.function_call:\n",
        "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
        "        print(f\"{fn.name}({args})\")"
      ],
      "metadata": {
        "id": "3rXNny_bC9Wj"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AUTOMATED AGENTIC WORKFLOW:"
      ],
      "metadata": {
        "id": "8EhPq9m2EPVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function_map = {\n",
        "    \"translation_tool\": translation_tool,\n",
        "    \"summarization_tool\": summarization_tool,\n",
        "    \"rewrite_tool\": rewrite_tool,\n",
        "    \"generate_questions_to_json_tool\": generate_questions_to_json_tool,\n",
        "    \"custom_prompt_tool\": custom_prompt_tool,\n",
        "    \"default_prompt_tool\": default_prompt_tool\n",
        "}"
      ],
      "metadata": {
        "id": "ZkwEqfVSEO34"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_generic_assistant(user_query: str):\n",
        "  chat = model.start_chat()\n",
        "\n",
        "  response = chat.send_message(user_query)\n",
        "\n",
        "  result = None\n",
        "\n",
        "  while True:\n",
        "      for part in response.parts:\n",
        "          if fn := part.function_call:\n",
        "              args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
        "              print(f\"USING THE FUNCTION: {fn.name}({args})\")\n",
        "              break\n",
        "      else:\n",
        "          print(f\"FINAL RESPONSE: {response.text}\")\n",
        "          break\n",
        "\n",
        "      fc = response.candidates[0].content.parts[0].function_call\n",
        "      result = function_map[fc.name](**fc.args)\n",
        "      print(f\"GENERATED RESULT: {result}\")\n",
        "\n",
        "      response = chat.send_message(\n",
        "          genai.protos.Content(\n",
        "              parts=[genai.protos.Part(\n",
        "                  function_response=genai.protos.FunctionResponse(\n",
        "                      name=fc.name,\n",
        "                      response={'result': result}))]))\n",
        "\n",
        "  return result, response.text"
      ],
      "metadata": {
        "id": "KYvyJlPAEU9N"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_generic_assistant(\"\"\"\n",
        "Could you convert 'Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language.\n",
        "These models leverage deep learning techniques, particularly transformer architectures, to process and generate text across a wide range of contexts and tasks. LLMs are capable\n",
        "of performing diverse functions, including language translation, summarization, content creation, and even complex problem-solving. Their capabilities continue to expand as they are fine-tuned\n",
        "for specific applications, making them invaluable tools in industries such as healthcare, education, customer support, and software development.' into Spanish and after that creating questions from it?\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xzuETK4WH2Y2",
        "outputId": "51556ade-c2d7-4318-d8d4-ddb821a45e16"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING THE FUNCTION: translation_tool(user_query=Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language.\\nThese models leverage deep learning techniques, particularly transformer architectures, to process and generate text across a wide range of contexts and tasks. LLMs are capable\\nof performing diverse functions, including language translation, summarization, content creation, and even complex problem-solving. Their capabilities continue to expand as they are fine-tuned\\nfor specific applications, making them invaluable tools in industries such as healthcare, education, customer support, and software development., source_lang=English, target_lang=Spanish)\n",
            "GENERATED RESULT: Los Modelos de Lenguaje Grandes (LLM, por sus siglas en inglés) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan técnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducción de idiomas, el resumen, la creación de contenido e incluso la resolución de problemas complejos. Sus capacidades continúan expandiéndose a medida que se ajustan para aplicaciones específicas, lo que los convierte en herramientas invaluables en industrias como la atención médica, la educación, la atención al cliente y el desarrollo de software.\n",
            "\n",
            "USING THE FUNCTION: generate_questions_to_json_tool(user_query=Los Modelos de Lenguaje Grandes (LLM, por sus siglas en inglés) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan técnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducción de idiomas, el resumen, la creación de contenido e incluso la resolución de problemas complejos. Sus capacidades continúan expandiéndose a medida que se ajustan para aplicaciones específicas, lo que los convierte en herramientas invaluables en industrias como la atención médica, la educación, la atención al cliente y el desarrollo de software.)\n",
            "GENERATED RESULT: {'multiple_choice_questions': [{'question': '¿Qué son los Modelos de Lenguaje Grandes (LLM)?', 'choices': [{'key': 'A', 'value': 'Sistemas de almacenamiento de datos.'}, {'key': 'B', 'value': 'Sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto.'}, {'key': 'C', 'value': 'Programas de edición de texto.'}, {'key': 'D', 'value': 'Herramientas de diseño gráfico.'}], 'answer': 'B', 'explanation': 'Los LLM son sistemas de IA entrenados con grandes cantidades de texto para entender y generar lenguaje.'}, {'question': '¿Qué técnica de aprendizaje profundo utilizan principalmente los LLM?', 'choices': [{'key': 'A', 'value': 'Redes neuronales convolucionales.'}, {'key': 'B', 'value': 'Máquinas de vectores de soporte.'}, {'key': 'C', 'value': 'Arquitecturas de transformadores.'}, {'key': 'D', 'value': 'Algoritmos genéticos.'}], 'answer': 'C', 'explanation': 'Los LLM se basan principalmente en arquitecturas de transformadores debido a su capacidad para procesar secuencias de datos de manera eficiente.'}, {'question': '¿Cuál de las siguientes NO es una función típica de los LLM?', 'choices': [{'key': 'A', 'value': 'Traducción de idiomas.'}, {'key': 'B', 'value': 'Resumen de textos.'}, {'key': 'C', 'value': 'Creación de contenido.'}, {'key': 'D', 'value': 'Control de hardware físico.'}], 'answer': 'D', 'explanation': 'Los LLM se centran en el procesamiento de texto y no están diseñados para controlar hardware físico.'}], 'open_ended_questions': [{'question': 'Describe brevemente cómo los LLM procesan y generan texto, mencionando las técnicas clave que utilizan.', 'answer': 'Los LLM procesan texto utilizando redes neuronales, especialmente arquitecturas de transformadores, que les permiten entender las relaciones entre palabras y generar texto coherente. Utilizan técnicas como el auto-atención para enfocarse en las partes relevantes del texto.', 'feedback': ['Los LLM usan redes neuronales y transformadores para entender y generar texto.', 'Procesan texto mediante auto-atención para entender las relaciones entre palabras.', 'Utilizan técnicas de aprendizaje profundo para generar texto coherente.']}, {'question': 'Explica por qué los LLM se consideran herramientas valiosas en diversas industrias. Proporciona al menos tres ejemplos de aplicaciones específicas.', 'answer': 'Los LLM son valiosos por su capacidad para automatizar tareas de procesamiento de lenguaje, mejorar la eficiencia y generar contenido. Ejemplos incluyen: atención al cliente automatizada (chatbots), generación de contenido de marketing, y traducción de documentos.', 'feedback': ['Son valiosos por su capacidad para automatizar tareas de lenguaje y mejorar la eficiencia.', 'Ejemplos de aplicaciones son chatbots, generación de contenido y traducción.', 'Los LLM ayudan a automatizar tareas y generar contenido en diversas industrias.']}, {'question': '¿Cómo crees que la continua expansión de las capacidades de los LLM podría impactar la sociedad en el futuro? Considera tanto los beneficios como los posibles desafíos.', 'answer': 'La expansión de los LLM podría traer beneficios como la automatización de tareas, la mejora de la comunicación y el acceso a la información. Sin embargo, también existen desafíos como la desinformación, el desplazamiento laboral y la necesidad de regular su uso ético.', 'feedback': ['Podrían automatizar tareas y mejorar la comunicación, pero también generar desinformación.', 'Los beneficios incluyen mayor acceso a la información, pero hay desafíos como el desplazamiento laboral.', 'Es importante considerar tanto los beneficios como los desafíos éticos y sociales de los LLM.']}]}\n",
            "FINAL RESPONSE: Okay, I've translated the text to Spanish and generated some questions about it. Here they are:\n",
            "\n",
            "**Spanish Translation:**\n",
            "\n",
            "Los Modelos de Lenguaje Grandes (LLM, por sus siglas en inglés) son sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto para comprender y generar lenguaje similar al humano. Estos modelos aprovechan técnicas de aprendizaje profundo, particularmente arquitecturas de transformadores, para procesar y generar texto en una amplia gama de contextos y tareas. Los LLM son capaces de realizar diversas funciones, incluyendo la traducción de idiomas, el resumen, la creación de contenido e incluso la resolución de problemas complejos. Sus capacidades continúan expandiéndose a medida que se ajustan para aplicaciones específicas, lo que los convierte en herramientas invaluables en industrias como la atención médica, la educación, la atención al cliente y el desarrollo de software.\n",
            "\n",
            "**Questions:**\n",
            "\n",
            "**Multiple Choice Questions:**\n",
            "\n",
            "1.  **Question:** ¿Qué son los Modelos de Lenguaje Grandes (LLM)?\n",
            "    *   A) Sistemas de almacenamiento de datos.\n",
            "    *   B) Sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto.\n",
            "    *   C) Programas de edición de texto.\n",
            "    *   D) Herramientas de diseño gráfico.\n",
            "    *   **Answer:** B\n",
            "    *   **Explanation:** Los LLM son sistemas de IA entrenados con grandes cantidades de texto para entender y generar lenguaje.\n",
            "\n",
            "2.  **Question:** ¿Qué técnica de aprendizaje profundo utilizan principalmente los LLM?\n",
            "    *   A) Redes neuronales convolucionales.\n",
            "    *   B) Máquinas de vectores de soporte.\n",
            "    *   C) Arquitecturas de transformadores.\n",
            "    *   D) Algoritmos genéticos.\n",
            "    *   **Answer:** C\n",
            "    *   **Explanation:** Los LLM se basan principalmente en arquitecturas de transformadores debido a su capacidad para procesar secuencias de datos de manera eficiente.\n",
            "\n",
            "3.  **Question:** ¿Cuál de las siguientes NO es una función típica de los LLM?\n",
            "    *   A) Traducción de idiomas.\n",
            "    *   B) Resumen de textos.\n",
            "    *   C) Creación de contenido.\n",
            "    *   D) Control de hardware físico.\n",
            "    *   **Answer:** D\n",
            "    *   **Explanation:** Los LLM se centran en el procesamiento de texto y no están diseñados para controlar hardware físico.\n",
            "\n",
            "**Open-Ended Questions:**\n",
            "\n",
            "1.  **Question:** Describe brevemente cómo los LLM procesan y generan texto, mencionando las técnicas clave que utilizan.\n",
            "    *   **Answer:** Los LLM procesan texto utilizando redes neuronales, especialmente arquitecturas de transformadores, que les permiten entender las relaciones entre palabras y generar texto coherente. Utilizan técnicas como el auto-atención para enfocarse en las partes relevantes del texto.\n",
            "    *   **Feedback:**\n",
            "        *   Los LLM usan redes neuronales y transformadores para entender y generar texto.\n",
            "        *   Procesan texto mediante auto-atención para entender las relaciones entre palabras.\n",
            "        *   Utilizan técnicas de aprendizaje profundo para generar texto coherente.\n",
            "\n",
            "2.  **Question:** Explica por qué los LLM se consideran herramientas valiosas en diversas industrias. Proporciona al menos tres ejemplos de aplicaciones específicas.\n",
            "    *   **Answer:** Los LLM son valiosos por su capacidad para automatizar tareas de procesamiento de lenguaje, mejorar la eficiencia y generar contenido. Ejemplos incluyen: atención al cliente automatizada (chatbots), generación de contenido de marketing, y traducción de documentos.\n",
            "    *   **Feedback:**\n",
            "        *   Son valiosos por su capacidad para automatizar tareas de lenguaje y mejorar la eficiencia.\n",
            "        *   Ejemplos de aplicaciones son chatbots, generación de contenido y traducción.\n",
            "        *   Los LLM ayudan a automatizar tareas y generar contenido en diversas industrias.\n",
            "\n",
            "3.  **Question:** ¿Cómo crees que la continua expansión de las capacidades de los LLM podría impactar la sociedad en el futuro? Considera tanto los beneficios como los posibles desafíos.\n",
            "    *   **Answer:** La expansión de los LLM podría traer beneficios como la automatización de tareas, la mejora de la comunicación y el acceso a la información. Sin embargo, también existen desafíos como la desinformación, el desplazamiento laboral y la necesidad de regular su uso ético.\n",
            "    *   **Feedback:**\n",
            "        *   Podrían automatizar tareas y mejorar la comunicación, pero también generar desinformación.\n",
            "        *   Los beneficios incluyen mayor acceso a la información, pero hay desafíos como el desplazamiento laboral.\n",
            "        *   Es importante considerar tanto los beneficios como los desafíos éticos y sociales de los LLM.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'multiple_choice_questions': [{'question': '¿Qué son los Modelos de Lenguaje Grandes (LLM)?',\n",
              "   'choices': [{'key': 'A', 'value': 'Sistemas de almacenamiento de datos.'},\n",
              "    {'key': 'B',\n",
              "     'value': 'Sistemas avanzados de inteligencia artificial entrenados con grandes cantidades de datos de texto.'},\n",
              "    {'key': 'C', 'value': 'Programas de edición de texto.'},\n",
              "    {'key': 'D', 'value': 'Herramientas de diseño gráfico.'}],\n",
              "   'answer': 'B',\n",
              "   'explanation': 'Los LLM son sistemas de IA entrenados con grandes cantidades de texto para entender y generar lenguaje.'},\n",
              "  {'question': '¿Qué técnica de aprendizaje profundo utilizan principalmente los LLM?',\n",
              "   'choices': [{'key': 'A', 'value': 'Redes neuronales convolucionales.'},\n",
              "    {'key': 'B', 'value': 'Máquinas de vectores de soporte.'},\n",
              "    {'key': 'C', 'value': 'Arquitecturas de transformadores.'},\n",
              "    {'key': 'D', 'value': 'Algoritmos genéticos.'}],\n",
              "   'answer': 'C',\n",
              "   'explanation': 'Los LLM se basan principalmente en arquitecturas de transformadores debido a su capacidad para procesar secuencias de datos de manera eficiente.'},\n",
              "  {'question': '¿Cuál de las siguientes NO es una función típica de los LLM?',\n",
              "   'choices': [{'key': 'A', 'value': 'Traducción de idiomas.'},\n",
              "    {'key': 'B', 'value': 'Resumen de textos.'},\n",
              "    {'key': 'C', 'value': 'Creación de contenido.'},\n",
              "    {'key': 'D', 'value': 'Control de hardware físico.'}],\n",
              "   'answer': 'D',\n",
              "   'explanation': 'Los LLM se centran en el procesamiento de texto y no están diseñados para controlar hardware físico.'}],\n",
              " 'open_ended_questions': [{'question': 'Describe brevemente cómo los LLM procesan y generan texto, mencionando las técnicas clave que utilizan.',\n",
              "   'answer': 'Los LLM procesan texto utilizando redes neuronales, especialmente arquitecturas de transformadores, que les permiten entender las relaciones entre palabras y generar texto coherente. Utilizan técnicas como el auto-atención para enfocarse en las partes relevantes del texto.',\n",
              "   'feedback': ['Los LLM usan redes neuronales y transformadores para entender y generar texto.',\n",
              "    'Procesan texto mediante auto-atención para entender las relaciones entre palabras.',\n",
              "    'Utilizan técnicas de aprendizaje profundo para generar texto coherente.']},\n",
              "  {'question': 'Explica por qué los LLM se consideran herramientas valiosas en diversas industrias. Proporciona al menos tres ejemplos de aplicaciones específicas.',\n",
              "   'answer': 'Los LLM son valiosos por su capacidad para automatizar tareas de procesamiento de lenguaje, mejorar la eficiencia y generar contenido. Ejemplos incluyen: atención al cliente automatizada (chatbots), generación de contenido de marketing, y traducción de documentos.',\n",
              "   'feedback': ['Son valiosos por su capacidad para automatizar tareas de lenguaje y mejorar la eficiencia.',\n",
              "    'Ejemplos de aplicaciones son chatbots, generación de contenido y traducción.',\n",
              "    'Los LLM ayudan a automatizar tareas y generar contenido en diversas industrias.']},\n",
              "  {'question': '¿Cómo crees que la continua expansión de las capacidades de los LLM podría impactar la sociedad en el futuro? Considera tanto los beneficios como los posibles desafíos.',\n",
              "   'answer': 'La expansión de los LLM podría traer beneficios como la automatización de tareas, la mejora de la comunicación y el acceso a la información. Sin embargo, también existen desafíos como la desinformación, el desplazamiento laboral y la necesidad de regular su uso ético.',\n",
              "   'feedback': ['Podrían automatizar tareas y mejorar la comunicación, pero también generar desinformación.',\n",
              "    'Los beneficios incluyen mayor acceso a la información, pero hay desafíos como el desplazamiento laboral.',\n",
              "    'Es importante considerar tanto los beneficios como los desafíos éticos y sociales de los LLM.']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_generic_assistant(\"\"\"\n",
        "Could you summarize 'Retrieval-Augmented Generation (RAG) is a cutting-edge framework in artificial intelligence that combines information retrieval with generative AI models to enhance response\n",
        "quality and relevance. In RAG, an AI system retrieves relevant documents or data from an external knowledge base in real-time and uses this retrieved information to inform the generation of responses.\n",
        "This approach addresses a key limitation of generative models, which often rely solely on their pre-trained knowledge, potentially leading to outdated or inaccurate outputs. By integrating retrieval,\n",
        "RAG ensures that responses are both contextually rich and factually accurate, making it particularly valuable for applications like customer support, research assistance, and content generation.'\n",
        "then translate that summarization from English to Spanish and after that creating questions from it?\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cYif9C7JNz3B",
        "outputId": "ab6ea892-c76d-4041-a75b-1cb1a920c66f"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING THE FUNCTION: summarization_tool(user_query=Retrieval-Augmented Generation (RAG) is a cutting-edge framework in artificial intelligence that combines information retrieval with generative AI models to enhance response \\nquality and relevance. In RAG, an AI system retrieves relevant documents or data from an external knowledge base in real-time and uses this retrieved information to inform the generation of responses.\\nThis approach addresses a key limitation of generative models, which often rely solely on their pre-trained knowledge, potentially leading to outdated or inaccurate outputs. By integrating retrieval, \\nRAG ensures that responses are both contextually rich and factually accurate, making it particularly valuable for applications like customer support, research assistance, and content generation., summarization_type=default)\n",
            "GENERATED RESULT: Retrieval-Augmented Generation (RAG) enhances AI response quality by combining information retrieval with generative models. RAG systems retrieve real-time data from external sources to ensure responses are contextually rich and factually accurate, addressing the limitations of relying solely on pre-trained knowledge. This makes RAG valuable for applications like customer support, research, and content creation.\n",
            "\n",
            "USING THE FUNCTION: translation_tool(user_query=Retrieval-Augmented Generation (RAG) enhances AI response quality by combining information retrieval with generative models. RAG systems retrieve real-time data from external sources to ensure responses are contextually rich and factually accurate, addressing the limitations of relying solely on pre-trained knowledge. This makes RAG valuable for applications like customer support, research, and content creation., source_lang=English, target_lang=Spanish)\n",
            "GENERATED RESULT: La Generación Aumentada por Recuperación (RAG, por sus siglas en inglés) mejora la calidad de las respuestas de la IA al combinar la recuperación de información con modelos generativos. Los sistemas RAG recuperan datos en tiempo real de fuentes externas para asegurar que las respuestas sean contextualmente ricas y fácticamente precisas, abordando las limitaciones de depender únicamente del conocimiento pre-entrenado. Esto hace que RAG sea valioso para aplicaciones como la atención al cliente, la investigación y la creación de contenido.\n",
            "\n",
            "USING THE FUNCTION: generate_questions_to_json_tool(user_query=La Generación Aumentada por Recuperación (RAG, por sus siglas en inglés) mejora la calidad de las respuestas de la IA al combinar la recuperación de información con modelos generativos. Los sistemas RAG recuperan datos en tiempo real de fuentes externas para asegurar que las respuestas sean contextualmente ricas y fácticamente precisas, abordando las limitaciones de depender únicamente del conocimiento pre-entrenado. Esto hace que RAG sea valioso para aplicaciones como la atención al cliente, la investigación y la creación de contenido.)\n",
            "GENERATED RESULT: {'multiple_choice_questions': [{'question': '¿Cuál es el propósito principal de la Generación Aumentada por Recuperación (RAG)?', 'choices': [{'key': 'A', 'value': 'Limitar la cantidad de datos que utiliza un modelo de IA.'}, {'key': 'B', 'value': 'Mejorar la calidad de las respuestas de la IA mediante la combinación de recuperación de información y modelos generativos.'}, {'key': 'C', 'value': 'Aumentar la velocidad de procesamiento de los modelos de IA.'}, {'key': 'D', 'value': 'Reducir la necesidad de entrenamiento previo en modelos de IA.'}], 'answer': 'B', 'explanation': 'RAG aims to enhance the quality of AI responses by integrating information retrieval with generative models, allowing for more informed and contextually relevant outputs.'}, {'question': '¿Cómo asegura RAG que las respuestas de la IA sean contextualmente ricas y precisas?', 'choices': [{'key': 'A', 'value': 'Utilizando únicamente el conocimiento pre-entrenado del modelo.'}, {'key': 'B', 'value': 'Recuperando datos en tiempo real de fuentes externas.'}, {'key': 'C', 'value': 'Aplicando algoritmos de compresión de datos.'}, {'key': 'D', 'value': 'Limitando el acceso a información externa.'}], 'answer': 'B', 'explanation': \"RAG achieves contextually rich and accurate responses by retrieving real-time data from external sources, which supplements the model's pre-existing knowledge.\"}, {'question': '¿Cuál de las siguientes NO es una aplicación típica de los sistemas RAG?', 'choices': [{'key': 'A', 'value': 'Atención al cliente.'}, {'key': 'B', 'value': 'Investigación.'}, {'key': 'C', 'value': 'Creación de contenido.'}, {'key': 'D', 'value': 'Traducción automática de idiomas.'}], 'answer': 'D', 'explanation': 'While RAG can be used in various applications, it is not typically used for direct machine translation. Its strength lies in enhancing responses with retrieved information, which is not the primary focus of translation tasks.'}], 'open_ended_questions': [{'question': 'Explica con tus propias palabras cómo funciona el proceso de RAG (Generación Aumentada por Recuperación) y por qué es una mejora sobre los modelos de IA que solo usan conocimiento pre-entrenado.', 'answer': \"RAG works by first retrieving relevant information from an external knowledge source based on the user's query. This retrieved information is then used to augment the input to a generative model, which produces the final response. This is an improvement over models that only use pre-trained knowledge because it allows the model to access up-to-date and specific information, leading to more accurate and contextually relevant responses.\", 'feedback': ['RAG retrieves information and uses it to improve the response.', 'RAG combines retrieval and generation for better answers.', 'RAG accesses external knowledge to enhance responses.', 'RAG is better than pre-trained models because it can use current information.']}, {'question': 'Describe al menos dos ventajas específicas que ofrece la tecnología RAG en comparación con los modelos de IA tradicionales.', 'answer': 'Two specific advantages of RAG are: 1) It allows models to access and use up-to-date information, which is not possible with models that rely solely on pre-trained knowledge. 2) It improves the accuracy and contextual relevance of responses by grounding them in external knowledge sources, reducing the risk of generating incorrect or hallucinated information.', 'feedback': ['RAG uses up-to-date information.', 'RAG improves accuracy and relevance.', 'RAG reduces hallucinations.', 'RAG accesses external knowledge.']}, {'question': 'Considerando las aplicaciones mencionadas (atención al cliente, investigación, creación de contenido), ¿cómo crees que RAG podría transformar cada uno de estos campos? Proporciona ejemplos concretos.', 'answer': \"In customer service, RAG can provide more accurate and helpful responses by accessing real-time product information or FAQs. In research, it can help researchers quickly find relevant information from vast databases, accelerating the research process. In content creation, RAG can assist writers by providing up-to-date facts and context, leading to more informative and accurate content. For example, a customer service bot could use RAG to answer questions about a new product by retrieving information from the product's documentation, a researcher could use RAG to find the latest studies on a specific topic, and a writer could use RAG to get the most recent statistics for an article.\", 'feedback': ['RAG can improve customer service with real-time information.', 'RAG can accelerate research by finding relevant data.', 'RAG can help content creation with up-to-date facts.', 'RAG can provide specific examples for each field.']}]}\n",
            "FINAL RESPONSE: Okay, I've summarized the text, translated the summary to Spanish, and generated questions about the summary in JSON format. Here's the result:\n",
            "\n",
            "**Summary:**\n",
            "Retrieval-Augmented Generation (RAG) enhances AI response quality by combining information retrieval with generative models. RAG systems retrieve real-time data from external sources to ensure responses are contextually rich and factually accurate, addressing the limitations of relying solely on pre-trained knowledge. This makes RAG valuable for applications like customer support, research, and content creation.\n",
            "\n",
            "**Spanish Translation:**\n",
            "La Generación Aumentada por Recuperación (RAG, por sus siglas en inglés) mejora la calidad de las respuestas de la IA al combinar la recuperación de información con modelos generativos. Los sistemas RAG recuperan datos en tiempo real de fuentes externas para asegurar que las respuestas sean contextualmente ricas y fácticamente precisas, abordando las limitaciones de depender únicamente del conocimiento pre-entrenado. Esto hace que RAG sea valioso para aplicaciones como la atención al cliente, la investigación y la creación de contenido.\n",
            "\n",
            "**Generated Questions (JSON format):**\n",
            "```json\n",
            "{\n",
            "  \"result\": {\n",
            "    \"multiple_choice_questions\": [\n",
            "      {\n",
            "        \"answer\": \"B\",\n",
            "        \"choices\": [\n",
            "          {\n",
            "            \"key\": \"A\",\n",
            "            \"value\": \"Limitar la cantidad de datos que utiliza un modelo de IA.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"B\",\n",
            "            \"value\": \"Mejorar la calidad de las respuestas de la IA mediante la combinación de recuperación de información y modelos generativos.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"C\",\n",
            "            \"value\": \"Aumentar la velocidad de procesamiento de los modelos de IA.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"D\",\n",
            "            \"value\": \"Reducir la necesidad de entrenamiento previo en modelos de IA.\"\n",
            "          }\n",
            "        ],\n",
            "        \"explanation\": \"RAG aims to enhance the quality of AI responses by integrating information retrieval with generative models, allowing for more informed and contextually relevant outputs.\",\n",
            "        \"question\": \"¿Cuál es el propósito principal de la Generación Aumentada por Recuperación (RAG)?\"\n",
            "      },\n",
            "      {\n",
            "        \"answer\": \"B\",\n",
            "        \"choices\": [\n",
            "          {\n",
            "            \"key\": \"A\",\n",
            "            \"value\": \"Utilizando únicamente el conocimiento pre-entrenado del modelo.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"B\",\n",
            "            \"value\": \"Recuperando datos en tiempo real de fuentes externas.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"C\",\n",
            "            \"value\": \"Aplicando algoritmos de compresión de datos.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"D\",\n",
            "            \"value\": \"Limitando el acceso a información externa.\"\n",
            "          }\n",
            "        ],\n",
            "        \"explanation\": \"RAG achieves contextually rich and accurate responses by retrieving real-time data from external sources, which supplements the model's pre-existing knowledge.\",\n",
            "        \"question\": \"¿Cómo asegura RAG que las respuestas de la IA sean contextualmente ricas y precisas?\"\n",
            "      },\n",
            "      {\n",
            "        \"answer\": \"D\",\n",
            "        \"choices\": [\n",
            "          {\n",
            "            \"key\": \"A\",\n",
            "            \"value\": \"Atención al cliente.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"B\",\n",
            "            \"value\": \"Investigación.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"C\",\n",
            "            \"value\": \"Creación de contenido.\"\n",
            "          },\n",
            "          {\n",
            "            \"key\": \"D\",\n",
            "            \"value\": \"Traducción automática de idiomas.\"\n",
            "          }\n",
            "        ],\n",
            "        \"explanation\": \"While RAG can be used in various applications, it is not typically used for direct machine translation. Its strength lies in enhancing responses with retrieved information, which is not the primary focus of translation tasks.\",\n",
            "        \"question\": \"¿Cuál de las siguientes NO es una aplicación típica de los sistemas RAG?\"\n",
            "      }\n",
            "    ],\n",
            "    \"open_ended_questions\": [\n",
            "      {\n",
            "        \"answer\": \"RAG works by first retrieving relevant information from an external knowledge source based on the user's query. This retrieved information is then used to augment the input to a generative model, which produces the final response. This is an improvement over models that only use pre-trained knowledge because it allows the model to access up-to-date and specific information, leading to more accurate and contextually relevant responses.\",\n",
            "        \"feedback\": [\n",
            "          \"RAG retrieves information and uses it to improve the response.\",\n",
            "          \"RAG combines retrieval and generation for better answers.\",\n",
            "          \"RAG accesses external knowledge to enhance responses.\",\n",
            "          \"RAG is better than pre-trained models because it can use current information.\"\n",
            "        ],\n",
            "        \"question\": \"Explica con tus propias palabras cómo funciona el proceso de RAG (Generación Aumentada por Recuperación) y por qué es una mejora sobre los modelos de IA que solo usan conocimiento pre-entrenado.\"\n",
            "      },\n",
            "      {\n",
            "        \"answer\": \"Two specific advantages of RAG are: 1) It allows models to access and use up-to-date information, which is not possible with models that rely solely on pre-trained knowledge. 2) It improves the accuracy and contextual relevance of responses by grounding them in external knowledge sources, reducing the risk of generating incorrect or hallucinated information.\",\n",
            "         \"feedback\": [\n",
            "          \"RAG uses up-to-date information.\",\n",
            "          \"RAG improves accuracy and relevance.\",\n",
            "          \"RAG reduces hallucinations.\",\n",
            "          \"RAG accesses external knowledge.\"\n",
            "        ],\n",
            "        \"question\": \"Describe al menos dos ventajas específicas que ofrece la tecnología RAG en comparación con los modelos de IA tradicionales.\"\n",
            "      },\n",
            "      {\n",
            "        \"answer\": \"In customer service, RAG can provide more accurate and helpful responses by accessing real-time product information or FAQs. In research, it can help researchers quickly find relevant information from vast databases, accelerating the research process. In content creation, RAG can assist writers by providing up-to-date facts and context, leading to more informative and accurate content. For example, a customer service bot could use RAG to answer questions about a new product by retrieving information from the product's documentation, a researcher could use RAG to find the latest studies on a specific topic, and a writer could use RAG to get the most recent statistics for an article.\",\n",
            "         \"feedback\": [\n",
            "          \"RAG can improve customer service with real-time information.\",\n",
            "          \"RAG can accelerate research by finding relevant data.\",\n",
            "          \"RAG can help content creation with up-to-date facts.\",\n",
            "          \"RAG can provide specific examples for each field.\"\n",
            "        ],\n",
            "        \"question\": \"Considerando las aplicaciones mencionadas (atención al cliente, investigación, creación de contenido), ¿cómo crees que RAG podría transformar cada uno de estos campos? Proporciona ejemplos concretos.\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'multiple_choice_questions': [{'question': '¿Cuál es el propósito principal de la Generación Aumentada por Recuperación (RAG)?',\n",
              "   'choices': [{'key': 'A',\n",
              "     'value': 'Limitar la cantidad de datos que utiliza un modelo de IA.'},\n",
              "    {'key': 'B',\n",
              "     'value': 'Mejorar la calidad de las respuestas de la IA mediante la combinación de recuperación de información y modelos generativos.'},\n",
              "    {'key': 'C',\n",
              "     'value': 'Aumentar la velocidad de procesamiento de los modelos de IA.'},\n",
              "    {'key': 'D',\n",
              "     'value': 'Reducir la necesidad de entrenamiento previo en modelos de IA.'}],\n",
              "   'answer': 'B',\n",
              "   'explanation': 'RAG aims to enhance the quality of AI responses by integrating information retrieval with generative models, allowing for more informed and contextually relevant outputs.'},\n",
              "  {'question': '¿Cómo asegura RAG que las respuestas de la IA sean contextualmente ricas y precisas?',\n",
              "   'choices': [{'key': 'A',\n",
              "     'value': 'Utilizando únicamente el conocimiento pre-entrenado del modelo.'},\n",
              "    {'key': 'B',\n",
              "     'value': 'Recuperando datos en tiempo real de fuentes externas.'},\n",
              "    {'key': 'C', 'value': 'Aplicando algoritmos de compresión de datos.'},\n",
              "    {'key': 'D', 'value': 'Limitando el acceso a información externa.'}],\n",
              "   'answer': 'B',\n",
              "   'explanation': \"RAG achieves contextually rich and accurate responses by retrieving real-time data from external sources, which supplements the model's pre-existing knowledge.\"},\n",
              "  {'question': '¿Cuál de las siguientes NO es una aplicación típica de los sistemas RAG?',\n",
              "   'choices': [{'key': 'A', 'value': 'Atención al cliente.'},\n",
              "    {'key': 'B', 'value': 'Investigación.'},\n",
              "    {'key': 'C', 'value': 'Creación de contenido.'},\n",
              "    {'key': 'D', 'value': 'Traducción automática de idiomas.'}],\n",
              "   'answer': 'D',\n",
              "   'explanation': 'While RAG can be used in various applications, it is not typically used for direct machine translation. Its strength lies in enhancing responses with retrieved information, which is not the primary focus of translation tasks.'}],\n",
              " 'open_ended_questions': [{'question': 'Explica con tus propias palabras cómo funciona el proceso de RAG (Generación Aumentada por Recuperación) y por qué es una mejora sobre los modelos de IA que solo usan conocimiento pre-entrenado.',\n",
              "   'answer': \"RAG works by first retrieving relevant information from an external knowledge source based on the user's query. This retrieved information is then used to augment the input to a generative model, which produces the final response. This is an improvement over models that only use pre-trained knowledge because it allows the model to access up-to-date and specific information, leading to more accurate and contextually relevant responses.\",\n",
              "   'feedback': ['RAG retrieves information and uses it to improve the response.',\n",
              "    'RAG combines retrieval and generation for better answers.',\n",
              "    'RAG accesses external knowledge to enhance responses.',\n",
              "    'RAG is better than pre-trained models because it can use current information.']},\n",
              "  {'question': 'Describe al menos dos ventajas específicas que ofrece la tecnología RAG en comparación con los modelos de IA tradicionales.',\n",
              "   'answer': 'Two specific advantages of RAG are: 1) It allows models to access and use up-to-date information, which is not possible with models that rely solely on pre-trained knowledge. 2) It improves the accuracy and contextual relevance of responses by grounding them in external knowledge sources, reducing the risk of generating incorrect or hallucinated information.',\n",
              "   'feedback': ['RAG uses up-to-date information.',\n",
              "    'RAG improves accuracy and relevance.',\n",
              "    'RAG reduces hallucinations.',\n",
              "    'RAG accesses external knowledge.']},\n",
              "  {'question': 'Considerando las aplicaciones mencionadas (atención al cliente, investigación, creación de contenido), ¿cómo crees que RAG podría transformar cada uno de estos campos? Proporciona ejemplos concretos.',\n",
              "   'answer': \"In customer service, RAG can provide more accurate and helpful responses by accessing real-time product information or FAQs. In research, it can help researchers quickly find relevant information from vast databases, accelerating the research process. In content creation, RAG can assist writers by providing up-to-date facts and context, leading to more informative and accurate content. For example, a customer service bot could use RAG to answer questions about a new product by retrieving information from the product's documentation, a researcher could use RAG to find the latest studies on a specific topic, and a writer could use RAG to get the most recent statistics for an article.\",\n",
              "   'feedback': ['RAG can improve customer service with real-time information.',\n",
              "    'RAG can accelerate research by finding relevant data.',\n",
              "    'RAG can help content creation with up-to-date facts.',\n",
              "    'RAG can provide specific examples for each field.']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_generic_assistant(\"\"\"\n",
        "Please, rewrite this: Retrieval-Augmented Generation (RAG) is a cutting-edge framework in artificial intelligence that combines information retrieval with generative AI models to\n",
        "enhance response quality and relevance. In RAG, an AI system retrieves relevant documents or data from an external knowledge base in real-time and uses this retrieved information\n",
        "to inform the generation of responses. This approach addresses a key limitation of generative models, which often rely solely on their pre-trained knowledge, potentially leading to\n",
        "outdated or inaccurate outputs. By integrating retrieval, RAG ensures that responses are both contextually rich and factually accurate, making it particularly valuable for applications\n",
        "like customer support, research assistance, and content generation. After that, translate it from English to German and finally summarize that content with bullet points.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "MDBIt5v4OkBW",
        "outputId": "371f8abf-a9db-495d-e344-fb995fafab08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING THE FUNCTION: rewrite_tool(tone=formal, focus=RAG, user_query=Retrieval-Augmented Generation (RAG) is a cutting-edge framework in artificial intelligence that combines information retrieval with generative AI models to \\nenhance response quality and relevance. In RAG, an AI system retrieves relevant documents or data from an external knowledge base in real-time and uses this retrieved information \\nto inform the generation of responses. This approach addresses a key limitation of generative models, which often rely solely on their pre-trained knowledge, potentially leading to \\noutdated or inaccurate outputs. By integrating retrieval, RAG ensures that responses are both contextually rich and factually accurate, making it particularly valuable for applications \\nlike customer support, research assistance, and content generation.)\n",
            "GENERATED RESULT: Retrieval-Augmented Generation (RAG) represents an advanced framework within the field of artificial intelligence. It integrates information retrieval mechanisms with generative AI models to improve the quality and relevance of generated responses. In the RAG methodology, an AI system accesses and retrieves pertinent documents or data from an external knowledge repository in real-time. This retrieved information is then utilized to guide the generation of responses. This approach effectively mitigates a significant limitation inherent in generative models, which typically depend exclusively on their pre-existing training data, potentially resulting in responses that are either outdated or factually incorrect. Through the incorporation of real-time retrieval, RAG ensures that generated responses are both contextually comprehensive and factually precise. This makes it particularly advantageous for applications such as customer support, research assistance, and content creation.\n",
            "\n",
            "USING THE FUNCTION: translation_tool(user_query=Retrieval-Augmented Generation (RAG) represents an advanced framework within the field of artificial intelligence. It integrates information retrieval mechanisms with generative AI models to improve the quality and relevance of generated responses. In the RAG methodology, an AI system accesses and retrieves pertinent documents or data from an external knowledge repository in real-time. This retrieved information is then utilized to guide the generation of responses. This approach effectively mitigates a significant limitation inherent in generative models, which typically depend exclusively on their pre-existing training data, potentially resulting in responses that are either outdated or factually incorrect. Through the incorporation of real-time retrieval, RAG ensures that generated responses are both contextually comprehensive and factually precise. This makes it particularly advantageous for applications such as customer support, research assistance, and content creation., target_lang=German, source_lang=English)\n",
            "GENERATED RESULT: Retrieval-Augmented Generation (RAG) stellt einen fortschrittlichen Rahmen im Bereich der künstlichen Intelligenz dar. Es integriert Informationsabrufmechanismen mit generativen KI-Modellen, um die Qualität und Relevanz der generierten Antworten zu verbessern. In der RAG-Methodik greift ein KI-System in Echtzeit auf relevante Dokumente oder Daten aus einem externen Wissensspeicher zu und ruft diese ab. Diese abgerufenen Informationen werden dann verwendet, um die Generierung von Antworten zu steuern. Dieser Ansatz mildert effektiv eine erhebliche Einschränkung, die generativen Modellen innewohnt, welche typischerweise ausschließlich auf ihren vorhandenen Trainingsdaten basieren, was potenziell zu Antworten führt, die entweder veraltet oder sachlich falsch sind. Durch die Einbeziehung des Echtzeit-Abrufs stellt RAG sicher, dass generierte Antworten sowohl kontextuell umfassend als auch faktisch präzise sind. Dies macht es besonders vorteilhaft für Anwendungen wie Kundensupport, Forschungshilfe und Inhaltserstellung.\n",
            "\n",
            "USING THE FUNCTION: summarization_tool(user_query=Retrieval-Augmented Generation (RAG) stellt einen fortschrittlichen Rahmen im Bereich der künstlichen Intelligenz dar. Es integriert Informationsabrufmechanismen mit generativen KI-Modellen, um die Qualität und Relevanz der generierten Antworten zu verbessern. In der RAG-Methodik greift ein KI-System in Echtzeit auf relevante Dokumente oder Daten aus einem externen Wissensspeicher zu und ruft diese ab. Diese abgerufenen Informationen werden dann verwendet, um die Generierung von Antworten zu steuern. Dieser Ansatz mildert effektiv eine erhebliche Einschränkung, die generativen Modellen innewohnt, welche typischerweise ausschließlich auf ihren vorhandenen Trainingsdaten basieren, was potenziell zu Antworten führt, die entweder veraltet oder sachlich falsch sind. Durch die Einbeziehung des Echtzeit-Abrufs stellt RAG sicher, dass generierte Antworten sowohl kontextuell umfassend als auch faktisch präzise sind. Dies macht es besonders vorteilhaft für Anwendungen wie Kundensupport, Forschungshilfe und Inhaltserstellung., summarization_type=bullet points)\n",
            "GENERATED RESULT: *   Retrieval-Augmented Generation (RAG) ist ein fortschrittlicher KI-Rahmen.\n",
            "*   RAG integriert Informationsabruf mit generativen KI-Modellen.\n",
            "*   Ziel ist die Verbesserung der Qualität und Relevanz generierter Antworten.\n",
            "*   Ein KI-System greift in Echtzeit auf externe Wissensspeicher zu.\n",
            "*   Abgerufene Informationen steuern die Antwortgenerierung.\n",
            "*   RAG mildert Einschränkungen generativer Modelle, die auf Trainingsdaten basieren.\n",
            "*   Echtzeit-Abruf sorgt für kontextuelle und faktische Genauigkeit.\n",
            "*   Vorteilhaft für Kundensupport, Forschungshilfe und Inhaltserstellung.\n",
            "\n",
            "FINAL RESPONSE: Okay, here's the rewritten text, the German translation, and a summary with bullet points:\n",
            "\n",
            "**Rewritten Text (Formal Tone):**\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) represents an advanced framework within the field of artificial intelligence. It integrates information retrieval mechanisms with generative AI models to improve the quality and relevance of generated responses. In the RAG methodology, an AI system accesses and retrieves pertinent documents or data from an external knowledge repository in real-time. This retrieved information is then utilized to guide the generation of responses. This approach effectively mitigates a significant limitation inherent in generative models, which typically depend exclusively on their pre-existing training data, potentially resulting in responses that are either outdated or factually incorrect. Through the incorporation of real-time retrieval, RAG ensures that generated responses are both contextually comprehensive and factually precise. This makes it particularly advantageous for applications such as customer support, research assistance, and content creation.\n",
            "\n",
            "**German Translation:**\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) stellt einen fortschrittlichen Rahmen im Bereich der künstlichen Intelligenz dar. Es integriert Informationsabrufmechanismen mit generativen KI-Modellen, um die Qualität und Relevanz der generierten Antworten zu verbessern. In der RAG-Methodik greift ein KI-System in Echtzeit auf relevante Dokumente oder Daten aus einem externen Wissensspeicher zu und ruft diese ab. Diese abgerufenen Informationen werden dann verwendet, um die Generierung von Antworten zu steuern. Dieser Ansatz mildert effektiv eine erhebliche Einschränkung, die generativen Modellen innewohnt, welche typischerweise ausschließlich auf ihren vorhandenen Trainingsdaten basieren, was potenziell zu Antworten führt, die entweder veraltet oder sachlich falsch sind. Durch die Einbeziehung des Echtzeit-Abrufs stellt RAG sicher, dass generierte Antworten sowohl kontextuell umfassend als auch faktisch präzise sind. Dies macht es besonders vorteilhaft für Anwendungen wie Kundensupport, Forschungshilfe und Inhaltserstellung.\n",
            "\n",
            "**Summary (Bullet Points):**\n",
            "\n",
            "*   Retrieval-Augmented Generation (RAG) ist ein fortschrittlicher KI-Rahmen.\n",
            "*   RAG integriert Informationsabruf mit generativen KI-Modellen.\n",
            "*   Ziel ist die Verbesserung der Qualität und Relevanz generierter Antworten.\n",
            "*   Ein KI-System greift in Echtzeit auf externe Wissensspeicher zu.\n",
            "*   Abgerufene Informationen steuern die Antwortgenerierung.\n",
            "*   RAG mildert Einschränkungen generativer Modelle, die auf Trainingsdaten basieren.\n",
            "*   Echtzeit-Abruf sorgt für kontextuelle und faktische Genauigkeit.\n",
            "*   Vorteilhaft für Kundensupport, Forschungshilfe und Inhaltserstellung.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*   Retrieval-Augmented Generation (RAG) ist ein fortschrittlicher KI-Rahmen.\\n*   RAG integriert Informationsabruf mit generativen KI-Modellen.\\n*   Ziel ist die Verbesserung der Qualität und Relevanz generierter Antworten.\\n*   Ein KI-System greift in Echtzeit auf externe Wissensspeicher zu.\\n*   Abgerufene Informationen steuern die Antwortgenerierung.\\n*   RAG mildert Einschränkungen generativer Modelle, die auf Trainingsdaten basieren.\\n*   Echtzeit-Abruf sorgt für kontextuelle und faktische Genauigkeit.\\n*   Vorteilhaft für Kundensupport, Forschungshilfe und Inhaltserstellung.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result, response = run_generic_assistant(\"\"\"\n",
        "Please, rewrite this: Retrieval-Augmented Generation (RAG) is a cutting-edge framework in artificial intelligence that combines information retrieval with generative AI models to\n",
        "enhance response quality and relevance. In RAG, an AI system retrieves relevant documents or data from an external knowledge base in real-time and uses this retrieved information\n",
        "to inform the generation of responses. This approach addresses a key limitation of generative models, which often rely solely on their pre-trained knowledge, potentially leading to\n",
        "outdated or inaccurate outputs. By integrating retrieval, RAG ensures that responses are both contextually rich and factually accurate, making it particularly valuable for applications\n",
        "like customer support, research assistance, and content generation. After that, translate it from English to German and finally summarize that content with bullet points.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "GZu5Eng1Wda-",
        "outputId": "59c63b1f-033b-4930-bdcc-7a8554512af9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING THE FUNCTION: rewrite_tool(tone=formal, focus=AI, user_query=Retrieval-Augmented Generation (RAG) is a cutting-edge framework in artificial intelligence that combines information retrieval with generative AI models to enhance response quality and relevance. In RAG, an AI system retrieves relevant documents or data from an external knowledge base in real-time and uses this retrieved information to inform the generation of responses. This approach addresses a key limitation of generative models, which often rely solely on their pre-trained knowledge, potentially leading to outdated or inaccurate outputs. By integrating retrieval, RAG ensures that responses are both contextually rich and factually accurate, making it particularly valuable for applications like customer support, research assistance, and content generation.)\n",
            "GENERATED RESULT: Retrieval-Augmented Generation (RAG) represents an advanced framework within the field of artificial intelligence. It integrates information retrieval mechanisms with generative AI models to improve the quality and relevance of generated responses. In the RAG framework, an AI system dynamically retrieves pertinent documents or data from an external knowledge repository. This retrieved information is then utilized to guide the generation of responses. This methodology effectively mitigates a significant constraint of generative models, which typically depend exclusively on their pre-existing training data, potentially resulting in responses that are either outdated or factually incorrect. Through the incorporation of retrieval processes, RAG ensures that responses are both contextually comprehensive and factually precise. This makes it particularly advantageous for applications such as customer support, research assistance, and content creation.\n",
            "\n",
            "USING THE FUNCTION: translation_tool(user_query=Retrieval-Augmented Generation (RAG) represents an advanced framework within the field of artificial intelligence. It integrates information retrieval mechanisms with generative AI models to improve the quality and relevance of generated responses. In the RAG framework, an AI system dynamically retrieves pertinent documents or data from an external knowledge repository. This retrieved information is then utilized to guide the generation of responses. This methodology effectively mitigates a significant constraint of generative models, which typically depend exclusively on their pre-existing training data, potentially resulting in responses that are either outdated or factually incorrect. Through the incorporation of retrieval processes, RAG ensures that responses are both contextually comprehensive and factually precise. This makes it particularly advantageous for applications such as customer support, research assistance, and content creation., target_lang=German, source_lang=English)\n",
            "GENERATED RESULT: Retrieval-Augmented Generation (RAG) stellt einen fortschrittlichen Rahmen im Bereich der künstlichen Intelligenz dar. Es integriert Informationsabrufmechanismen mit generativen KI-Modellen, um die Qualität und Relevanz der generierten Antworten zu verbessern. Im RAG-Framework ruft ein KI-System dynamisch relevante Dokumente oder Daten aus einem externen Wissensspeicher ab. Diese abgerufenen Informationen werden dann verwendet, um die Generierung von Antworten zu steuern. Diese Methodik mildert effektiv eine wesentliche Einschränkung generativer Modelle, die typischerweise ausschließlich auf ihren vorhandenen Trainingsdaten beruhen, was potenziell zu Antworten führt, die entweder veraltet oder sachlich falsch sind. Durch die Einbeziehung von Abrufprozessen stellt RAG sicher, dass Antworten sowohl kontextuell umfassend als auch faktisch präzise sind. Dies macht es besonders vorteilhaft für Anwendungen wie Kundensupport, Forschungshilfe und Inhaltserstellung.\n",
            "\n",
            "USING THE FUNCTION: summarization_tool(user_query=Retrieval-Augmented Generation (RAG) stellt einen fortschrittlichen Rahmen im Bereich der künstlichen Intelligenz dar. Es integriert Informationsabrufmechanismen mit generativen KI-Modellen, um die Qualität und Relevanz der generierten Antworten zu verbessern. Im RAG-Framework ruft ein KI-System dynamisch relevante Dokumente oder Daten aus einem externen Wissensspeicher ab. Diese abgerufenen Informationen werden dann verwendet, um die Generierung von Antworten zu steuern. Diese Methodik mildert effektiv eine wesentliche Einschränkung generativer Modelle, die typischerweise ausschließlich auf ihren vorhandenen Trainingsdaten beruhen, was potenziell zu Antworten führt, die entweder veraltet oder sachlich falsch sind. Durch die Einbeziehung von Abrufprozessen stellt RAG sicher, dass Antworten sowohl kontextuell umfassend als auch faktisch präzise sind. Dies macht es besonders vorteilhaft für Anwendungen wie Kundensupport, Forschungshilfe und Inhaltserstellung., summarization_type=bullet points)\n",
            "GENERATED RESULT: *   Retrieval-Augmented Generation (RAG) ist ein fortschrittlicher KI-Rahmen.\n",
            "*   RAG integriert Informationsabruf mit generativen KI-Modellen.\n",
            "*   Ziel ist die Verbesserung der Qualität und Relevanz generierter Antworten.\n",
            "*   Das System ruft dynamisch relevante Informationen aus einem externen Wissensspeicher ab.\n",
            "*   Diese Informationen steuern die Generierung von Antworten.\n",
            "*   RAG mildert Einschränkungen generativer Modelle, die auf Trainingsdaten beruhen.\n",
            "*   Es stellt sicher, dass Antworten kontextuell und faktisch präzise sind.\n",
            "*   Vorteilhaft für Kundensupport, Forschungshilfe und Inhaltserstellung.\n",
            "\n",
            "FINAL RESPONSE: Okay, here's the rewritten text, its translation to German, and a summary with bullet points:\n",
            "\n",
            "**Rewritten Text (Formal Tone):**\n",
            "Retrieval-Augmented Generation (RAG) represents an advanced framework within the field of artificial intelligence. It integrates information retrieval mechanisms with generative AI models to improve the quality and relevance of generated responses. In the RAG framework, an AI system dynamically retrieves pertinent documents or data from an external knowledge repository. This retrieved information is then utilized to guide the generation of responses. This methodology effectively mitigates a significant constraint of generative models, which typically depend exclusively on their pre-existing training data, potentially resulting in responses that are either outdated or factually incorrect. Through the incorporation of retrieval processes, RAG ensures that responses are both contextually comprehensive and factually precise. This makes it particularly advantageous for applications such as customer support, research assistance, and content creation.\n",
            "\n",
            "**German Translation:**\n",
            "Retrieval-Augmented Generation (RAG) stellt einen fortschrittlichen Rahmen im Bereich der künstlichen Intelligenz dar. Es integriert Informationsabrufmechanismen mit generativen KI-Modellen, um die Qualität und Relevanz der generierten Antworten zu verbessern. Im RAG-Framework ruft ein KI-System dynamisch relevante Dokumente oder Daten aus einem externen Wissensspeicher ab. Diese abgerufenen Informationen werden dann verwendet, um die Generierung von Antworten zu steuern. Diese Methodik mildert effektiv eine wesentliche Einschränkung generativer Modelle, die typischerweise ausschließlich auf ihren vorhandenen Trainingsdaten beruhen, was potenziell zu Antworten führt, die entweder veraltet oder sachlich falsch sind. Durch die Einbeziehung von Abrufprozessen stellt RAG sicher, dass Antworten sowohl kontextuell umfassend als auch faktisch präzise sind. Dies macht es besonders vorteilhaft für Anwendungen wie Kundensupport, Forschungshilfe und Inhaltserstellung.\n",
            "\n",
            "**Summary (Bullet Points):**\n",
            "*   Retrieval-Augmented Generation (RAG) ist ein fortschrittlicher KI-Rahmen.\n",
            "*   RAG integriert Informationsabruf mit generativen KI-Modellen.\n",
            "*   Ziel ist die Verbesserung der Qualität und Relevanz generierter Antworten.\n",
            "*   Das System ruft dynamisch relevante Informationen aus einem externen Wissensspeicher ab.\n",
            "*   Diese Informationen steuern die Generierung von Antworten.\n",
            "*   RAG mildert Einschränkungen generativer Modelle, die auf Trainingsdaten beruhen.\n",
            "*   Es stellt sicher, dass Antworten kontextuell und faktisch präzise sind.\n",
            "*   Vorteilhaft für Kundensupport, Forschungshilfe und Inhaltserstellung.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "xd1I0BXJWlon",
        "outputId": "d1bde40a-b44c-4c82-d5cf-a0647a805a8e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*   Retrieval-Augmented Generation (RAG) ist ein fortschrittlicher KI-Rahmen.\\n*   RAG integriert Informationsabruf mit generativen KI-Modellen.\\n*   Ziel ist die Verbesserung der Qualität und Relevanz generierter Antworten.\\n*   Das System ruft dynamisch relevante Informationen aus einem externen Wissensspeicher ab.\\n*   Diese Informationen steuern die Generierung von Antworten.\\n*   RAG mildert Einschränkungen generativer Modelle, die auf Trainingsdaten beruhen.\\n*   Es stellt sicher, dass Antworten kontextuell und faktisch präzise sind.\\n*   Vorteilhaft für Kundensupport, Forschungshilfe und Inhaltserstellung.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "wsmfnhwIWnHK",
        "outputId": "54622bce-7364-463b-9c77-5c0ed4fdf59b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, here's the rewritten text, its translation to German, and a summary with bullet points:\\n\\n**Rewritten Text (Formal Tone):**\\nRetrieval-Augmented Generation (RAG) represents an advanced framework within the field of artificial intelligence. It integrates information retrieval mechanisms with generative AI models to improve the quality and relevance of generated responses. In the RAG framework, an AI system dynamically retrieves pertinent documents or data from an external knowledge repository. This retrieved information is then utilized to guide the generation of responses. This methodology effectively mitigates a significant constraint of generative models, which typically depend exclusively on their pre-existing training data, potentially resulting in responses that are either outdated or factually incorrect. Through the incorporation of retrieval processes, RAG ensures that responses are both contextually comprehensive and factually precise. This makes it particularly advantageous for applications such as customer support, research assistance, and content creation.\\n\\n**German Translation:**\\nRetrieval-Augmented Generation (RAG) stellt einen fortschrittlichen Rahmen im Bereich der künstlichen Intelligenz dar. Es integriert Informationsabrufmechanismen mit generativen KI-Modellen, um die Qualität und Relevanz der generierten Antworten zu verbessern. Im RAG-Framework ruft ein KI-System dynamisch relevante Dokumente oder Daten aus einem externen Wissensspeicher ab. Diese abgerufenen Informationen werden dann verwendet, um die Generierung von Antworten zu steuern. Diese Methodik mildert effektiv eine wesentliche Einschränkung generativer Modelle, die typischerweise ausschließlich auf ihren vorhandenen Trainingsdaten beruhen, was potenziell zu Antworten führt, die entweder veraltet oder sachlich falsch sind. Durch die Einbeziehung von Abrufprozessen stellt RAG sicher, dass Antworten sowohl kontextuell umfassend als auch faktisch präzise sind. Dies macht es besonders vorteilhaft für Anwendungen wie Kundensupport, Forschungshilfe und Inhaltserstellung.\\n\\n**Summary (Bullet Points):**\\n*   Retrieval-Augmented Generation (RAG) ist ein fortschrittlicher KI-Rahmen.\\n*   RAG integriert Informationsabruf mit generativen KI-Modellen.\\n*   Ziel ist die Verbesserung der Qualität und Relevanz generierter Antworten.\\n*   Das System ruft dynamisch relevante Informationen aus einem externen Wissensspeicher ab.\\n*   Diese Informationen steuern die Generierung von Antworten.\\n*   RAG mildert Einschränkungen generativer Modelle, die auf Trainingsdaten beruhen.\\n*   Es stellt sicher, dass Antworten kontextuell und faktisch präzise sind.\\n*   Vorteilhaft für Kundensupport, Forschungshilfe und Inhaltserstellung.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result, response = run_generic_assistant(\"\"\"\n",
        "Generate tips for differentiated instruction\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QRH6IRRxW16H",
        "outputId": "ca1f03bd-4655-4fbf-d383-ed02095042ab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING THE FUNCTION: default_prompt_tool(user_query=Generate tips for differentiated instruction)\n",
            "GENERATED RESULT: Okay, I can help with that! Here are some tips for differentiated instruction, tailored to help you meet the diverse needs of your students:\n",
            "\n",
            "**Understanding Your Students**\n",
            "\n",
            "1.  **Know Your Learners:**\n",
            "    *   **Assess Regularly:** Use pre-assessments, formative assessments, and observations to understand students' prior knowledge, learning styles, and interests.\n",
            "    *   **Individual Profiles:** Create student profiles that include learning preferences, strengths, and areas for growth.\n",
            "\n",
            "**Strategies for Differentiation**\n",
            "\n",
            "2.  **Content Differentiation:**\n",
            "    *   **Vary Resources:** Provide a range of materials (e.g., texts at different reading levels, videos, audio recordings).\n",
            "    *   **Tiered Content:** Offer different levels of complexity within the same topic.\n",
            "    *   **Choice Boards:** Allow students to select from a variety of content options.\n",
            "\n",
            "3.  **Process Differentiation:**\n",
            "    *   **Flexible Grouping:** Use whole-class, small-group, and individual work based on learning needs.\n",
            "    *   **Learning Centers:** Set up stations with different activities that cater to various learning styles.\n",
            "    *   **Varied Activities:** Offer a range of activities (e.g., hands-on projects, discussions, research, presentations).\n",
            "\n",
            "4.  **Product Differentiation:**\n",
            "    *   **Choice in Output:** Allow students to demonstrate their learning through various formats (e.g., written reports, presentations, models, performances).\n",
            "    *   **Rubrics:** Provide clear rubrics that allow for different levels of complexity and creativity.\n",
            "    *   **Student-Led Projects:** Encourage students to design their own projects based on their interests.\n",
            "\n",
            "**Classroom Environment**\n",
            "\n",
            "5.  **Create a Supportive Environment:**\n",
            "    *   **Positive Culture:** Foster a classroom where students feel safe to take risks and learn from mistakes.\n",
            "    *   **Growth Mindset:** Encourage students to believe in their ability to learn and improve.\n",
            "    *   **Accessibility:** Ensure all materials and activities are accessible to all students.\n",
            "\n",
            "**Implementation Tips**\n",
            "\n",
            "6.  **Start Small:**\n",
            "    *   **Focus on One Area:** Begin by differentiating one aspect of your lesson (e.g., content, process, or product).\n",
            "    *   **Gradual Implementation:** Gradually introduce more differentiation strategies as you become comfortable.\n",
            "\n",
            "7.  **Reflect and Adjust:**\n",
            "    *   **Monitor Progress:** Regularly assess the effectiveness of your differentiation strategies.\n",
            "    *   **Seek Feedback:** Ask students for feedback on what works best for them.\n",
            "    *   **Be Flexible:** Be prepared to adjust your approach based on student needs and feedback.\n",
            "\n",
            "**Additional Tips**\n",
            "\n",
            "8.  **Use Technology:**\n",
            "    *   **Adaptive Software:** Utilize programs that adjust to students' skill levels.\n",
            "    *   **Digital Tools:** Incorporate digital tools for research, collaboration, and presentation.\n",
            "\n",
            "9.  **Collaborate:**\n",
            "    *   **Team Teaching:** Work with other teachers to share ideas and resources.\n",
            "    *   **Professional Development:** Seek out opportunities to learn more about differentiated instruction.\n",
            "\n",
            "**Example**\n",
            "\n",
            "*   **Topic:** The Water Cycle\n",
            "    *   **Content:** Provide reading materials at different levels, videos, and interactive diagrams.\n",
            "    *   **Process:** Offer options for group work, individual research, or hands-on experiments.\n",
            "    *   **Product:** Allow students to choose between creating a poster, giving a presentation, or writing a report.\n",
            "\n",
            "By implementing these tips, you can create a more inclusive and effective learning environment that meets the diverse needs of all your students. Let me know if you have any other questions or need more specific advice!\n",
            "\n",
            "FINAL RESPONSE: Okay, here are some tips for differentiated instruction:\n",
            "\n",
            "**Understanding Your Students**\n",
            "\n",
            "1.  **Know Your Learners:**\n",
            "    *   **Assess Regularly:** Use pre-assessments, formative assessments, and observations to understand students' prior knowledge, learning styles, and interests.\n",
            "    *   **Individual Profiles:** Create student profiles that include learning preferences, strengths, and areas for growth.\n",
            "\n",
            "**Strategies for Differentiation**\n",
            "\n",
            "2.  **Content Differentiation:**\n",
            "    *   **Vary Resources:** Provide a range of materials (e.g., texts at different reading levels, videos, audio recordings).\n",
            "    *   **Tiered Content:** Offer different levels of complexity within the same topic.\n",
            "    *   **Choice Boards:** Allow students to select from a variety of content options.\n",
            "\n",
            "3.  **Process Differentiation:**\n",
            "    *   **Flexible Grouping:** Use whole-class, small-group, and individual work based on learning needs.\n",
            "    *   **Learning Centers:** Set up stations with different activities that cater to various learning styles.\n",
            "    *   **Varied Activities:** Offer a range of activities (e.g., hands-on projects, discussions, research, presentations).\n",
            "\n",
            "4.  **Product Differentiation:**\n",
            "    *   **Choice in Output:** Allow students to demonstrate their learning through various formats (e.g., written reports, presentations, models, performances).\n",
            "    *   **Rubrics:** Provide clear rubrics that allow for different levels of complexity and creativity.\n",
            "    *   **Student-Led Projects:** Encourage students to design their own projects based on their interests.\n",
            "\n",
            "**Classroom Environment**\n",
            "\n",
            "5.  **Create a Supportive Environment:**\n",
            "    *   **Positive Culture:** Foster a classroom where students feel safe to take risks and learn from mistakes.\n",
            "    *   **Growth Mindset:** Encourage students to believe in their ability to learn and improve.\n",
            "    *   **Accessibility:** Ensure all materials and activities are accessible to all students.\n",
            "\n",
            "**Implementation Tips**\n",
            "\n",
            "6.  **Start Small:**\n",
            "    *   **Focus on One Area:** Begin by differentiating one aspect of your lesson (e.g., content, process, or product).\n",
            "    *   **Gradual Implementation:** Gradually introduce more differentiation strategies as you become comfortable.\n",
            "\n",
            "7.  **Reflect and Adjust:**\n",
            "    *   **Monitor Progress:** Regularly assess the effectiveness of your differentiation strategies.\n",
            "    *   **Seek Feedback:** Ask students for feedback on what works best for them.\n",
            "    *   **Be Flexible:** Be prepared to adjust your approach based on student needs and feedback.\n",
            "\n",
            "**Additional Tips**\n",
            "\n",
            "8.  **Use Technology:**\n",
            "    *   **Adaptive Software:** Utilize programs that adjust to students' skill levels.\n",
            "    *   **Digital Tools:** Incorporate digital tools for research, collaboration, and presentation.\n",
            "\n",
            "9.  **Collaborate:**\n",
            "    *   **Team Teaching:** Work with other teachers to share ideas and resources.\n",
            "    *   **Professional Development:** Seek out opportunities to learn more about differentiated instruction.\n",
            "\n",
            "**Example**\n",
            "\n",
            "*   **Topic:** The Water Cycle\n",
            "    *   **Content:** Provide reading materials at different levels, videos, and interactive diagrams.\n",
            "    *   **Process:** Offer options for group work, individual research, or hands-on experiments.\n",
            "    *   **Product:** Allow students to choose between creating a poster, giving a presentation, or writing a report.\n",
            "\n",
            "By implementing these tips, you can create a more inclusive and effective learning environment that meets the diverse needs of all your students. Let me know if you have any other questions or need more specific advice!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "U4hJRjMjXI1a",
        "outputId": "62c54802-fbdb-45e3-b5d2-a5b40238f663"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, I can help with that! Here are some tips for differentiated instruction, tailored to help you meet the diverse needs of your students:\\n\\n**Understanding Your Students**\\n\\n1.  **Know Your Learners:**\\n    *   **Assess Regularly:** Use pre-assessments, formative assessments, and observations to understand students' prior knowledge, learning styles, and interests.\\n    *   **Individual Profiles:** Create student profiles that include learning preferences, strengths, and areas for growth.\\n\\n**Strategies for Differentiation**\\n\\n2.  **Content Differentiation:**\\n    *   **Vary Resources:** Provide a range of materials (e.g., texts at different reading levels, videos, audio recordings).\\n    *   **Tiered Content:** Offer different levels of complexity within the same topic.\\n    *   **Choice Boards:** Allow students to select from a variety of content options.\\n\\n3.  **Process Differentiation:**\\n    *   **Flexible Grouping:** Use whole-class, small-group, and individual work based on learning needs.\\n    *   **Learning Centers:** Set up stations with different activities that cater to various learning styles.\\n    *   **Varied Activities:** Offer a range of activities (e.g., hands-on projects, discussions, research, presentations).\\n\\n4.  **Product Differentiation:**\\n    *   **Choice in Output:** Allow students to demonstrate their learning through various formats (e.g., written reports, presentations, models, performances).\\n    *   **Rubrics:** Provide clear rubrics that allow for different levels of complexity and creativity.\\n    *   **Student-Led Projects:** Encourage students to design their own projects based on their interests.\\n\\n**Classroom Environment**\\n\\n5.  **Create a Supportive Environment:**\\n    *   **Positive Culture:** Foster a classroom where students feel safe to take risks and learn from mistakes.\\n    *   **Growth Mindset:** Encourage students to believe in their ability to learn and improve.\\n    *   **Accessibility:** Ensure all materials and activities are accessible to all students.\\n\\n**Implementation Tips**\\n\\n6.  **Start Small:**\\n    *   **Focus on One Area:** Begin by differentiating one aspect of your lesson (e.g., content, process, or product).\\n    *   **Gradual Implementation:** Gradually introduce more differentiation strategies as you become comfortable.\\n\\n7.  **Reflect and Adjust:**\\n    *   **Monitor Progress:** Regularly assess the effectiveness of your differentiation strategies.\\n    *   **Seek Feedback:** Ask students for feedback on what works best for them.\\n    *   **Be Flexible:** Be prepared to adjust your approach based on student needs and feedback.\\n\\n**Additional Tips**\\n\\n8.  **Use Technology:**\\n    *   **Adaptive Software:** Utilize programs that adjust to students' skill levels.\\n    *   **Digital Tools:** Incorporate digital tools for research, collaboration, and presentation.\\n\\n9.  **Collaborate:**\\n    *   **Team Teaching:** Work with other teachers to share ideas and resources.\\n    *   **Professional Development:** Seek out opportunities to learn more about differentiated instruction.\\n\\n**Example**\\n\\n*   **Topic:** The Water Cycle\\n    *   **Content:** Provide reading materials at different levels, videos, and interactive diagrams.\\n    *   **Process:** Offer options for group work, individual research, or hands-on experiments.\\n    *   **Product:** Allow students to choose between creating a poster, giving a presentation, or writing a report.\\n\\nBy implementing these tips, you can create a more inclusive and effective learning environment that meets the diverse needs of all your students. Let me know if you have any other questions or need more specific advice!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "7awZSRLlXNP3",
        "outputId": "bbb04075-5ca2-4544-d89d-b9bf8c9c2f69"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, here are some tips for differentiated instruction:\\n\\n**Understanding Your Students**\\n\\n1.  **Know Your Learners:**\\n    *   **Assess Regularly:** Use pre-assessments, formative assessments, and observations to understand students' prior knowledge, learning styles, and interests.\\n    *   **Individual Profiles:** Create student profiles that include learning preferences, strengths, and areas for growth.\\n\\n**Strategies for Differentiation**\\n\\n2.  **Content Differentiation:**\\n    *   **Vary Resources:** Provide a range of materials (e.g., texts at different reading levels, videos, audio recordings).\\n    *   **Tiered Content:** Offer different levels of complexity within the same topic.\\n    *   **Choice Boards:** Allow students to select from a variety of content options.\\n\\n3.  **Process Differentiation:**\\n    *   **Flexible Grouping:** Use whole-class, small-group, and individual work based on learning needs.\\n    *   **Learning Centers:** Set up stations with different activities that cater to various learning styles.\\n    *   **Varied Activities:** Offer a range of activities (e.g., hands-on projects, discussions, research, presentations).\\n\\n4.  **Product Differentiation:**\\n    *   **Choice in Output:** Allow students to demonstrate their learning through various formats (e.g., written reports, presentations, models, performances).\\n    *   **Rubrics:** Provide clear rubrics that allow for different levels of complexity and creativity.\\n    *   **Student-Led Projects:** Encourage students to design their own projects based on their interests.\\n\\n**Classroom Environment**\\n\\n5.  **Create a Supportive Environment:**\\n    *   **Positive Culture:** Foster a classroom where students feel safe to take risks and learn from mistakes.\\n    *   **Growth Mindset:** Encourage students to believe in their ability to learn and improve.\\n    *   **Accessibility:** Ensure all materials and activities are accessible to all students.\\n\\n**Implementation Tips**\\n\\n6.  **Start Small:**\\n    *   **Focus on One Area:** Begin by differentiating one aspect of your lesson (e.g., content, process, or product).\\n    *   **Gradual Implementation:** Gradually introduce more differentiation strategies as you become comfortable.\\n\\n7.  **Reflect and Adjust:**\\n    *   **Monitor Progress:** Regularly assess the effectiveness of your differentiation strategies.\\n    *   **Seek Feedback:** Ask students for feedback on what works best for them.\\n    *   **Be Flexible:** Be prepared to adjust your approach based on student needs and feedback.\\n\\n**Additional Tips**\\n\\n8.  **Use Technology:**\\n    *   **Adaptive Software:** Utilize programs that adjust to students' skill levels.\\n    *   **Digital Tools:** Incorporate digital tools for research, collaboration, and presentation.\\n\\n9.  **Collaborate:**\\n    *   **Team Teaching:** Work with other teachers to share ideas and resources.\\n    *   **Professional Development:** Seek out opportunities to learn more about differentiated instruction.\\n\\n**Example**\\n\\n*   **Topic:** The Water Cycle\\n    *   **Content:** Provide reading materials at different levels, videos, and interactive diagrams.\\n    *   **Process:** Offer options for group work, individual research, or hands-on experiments.\\n    *   **Product:** Allow students to choose between creating a poster, giving a presentation, or writing a report.\\n\\nBy implementing these tips, you can create a more inclusive and effective learning environment that meets the diverse needs of all your students. Let me know if you have any other questions or need more specific advice!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}