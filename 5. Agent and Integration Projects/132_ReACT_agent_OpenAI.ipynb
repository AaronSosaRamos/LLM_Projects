{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpKjXDb-M8ud",
        "outputId": "17b175db-3869-4fe6-b0cb-e75355d25366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "fR2OiNdLOiyw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from tavily import TavilyClient\n",
        "import os\n",
        "\n",
        "# Define the agent's system prompt\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an intelligent agent capable of reasoning and performing actions. Follow these steps:\n",
        "1. Think about the input and what needs to be done.\n",
        "2. Decide on an action or answer.\n",
        "3. Call functions for actions if necessary.\n",
        "4. Provide a result or request additional input until the task is complete.\n",
        "If an action is needed, format your response as:\n",
        "\"Action: <action_name>(<parameters>)\"\n",
        "\n",
        "For web searches, follow these steps:\n",
        "1. Analyze the user's query and identify keywords.\n",
        "2. Formulate a clear search query that captures the user's intent.\n",
        "3. Call the `search_web` function with the formulated query.\n",
        "4. Examine the search results and extract relevant information.\n",
        "5. If necessary, refine the search query or use additional tools to gather more information.\n",
        "6. Provide a final answer or summary based on the gathered information.\n",
        "\n",
        "Examples:\n",
        "\n",
        "User: What is the history of artificial intelligence?\n",
        "Thought: I need to find information about the history of AI.\n",
        "Action: search_web(query='history of artificial intelligence')\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the Tavily client with your API key\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "def search_web(query):\n",
        "    try:\n",
        "        # Perform the search with Tavily\n",
        "        response = tavily_client.search(query, max_results=3)\n",
        "        # Extract and format the results\n",
        "        results = response.get('results', [])\n",
        "        formatted_results = \"\\n\".join([f\"{i+1}. {result['title']} - {result['url']}\" for i, result in enumerate(results)])\n",
        "        return formatted_results if formatted_results else \"No results found.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error performing search: {e}\"\n",
        "\n",
        "def perform_math(operation, num1, num2):\n",
        "    if operation == \"add\":\n",
        "        return num1 + num2\n",
        "    elif operation == \"subtract\":\n",
        "        return num1 - num2\n",
        "    elif operation == \"multiply\":\n",
        "        return num1 * num2\n",
        "    elif operation == \"divide\" and num2 != 0:\n",
        "        return num1 / num2\n",
        "    return \"Invalid operation or parameters.\"\n",
        "\n",
        "# ReAct loop implementation\n",
        "def react_agent(user_input):\n",
        "    conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "    while True:\n",
        "        # Generate response using OpenAI's GPT\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=conversation\n",
        "        )\n",
        "        message = response.choices[0].message.content\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": message})\n",
        "\n",
        "        print(\"Assistant:\", message)  # Show the assistant's message\n",
        "\n",
        "        if \"Thought:\" in message:\n",
        "            # Extract the thought process\n",
        "            thought = message.split(\"Thought:\")[1].strip()\n",
        "            print(f\"Thought: {thought}\")  # Display the thought process\n",
        "\n",
        "            # Continue to the next iteration to allow for Action call\n",
        "            continue\n",
        "\n",
        "        # Check for action call\n",
        "        if \"Action:\" in message:\n",
        "            action_call = message.split(\"Action:\")[1].strip()\n",
        "            action_name, params = parse_action(action_call)\n",
        "\n",
        "            # Execute the action\n",
        "            if action_name == \"search_web\":\n",
        "                query = params.get(\"query\", \"\")\n",
        "                result = search_web(query)\n",
        "            elif action_name == \"perform_math\":\n",
        "                result = perform_math(\n",
        "                    params.get(\"operation\", \"\"),\n",
        "                    params.get(\"num1\", 0),\n",
        "                    params.get(\"num2\", 0),\n",
        "                )\n",
        "            else:\n",
        "                result = \"Unknown action.\"\n",
        "\n",
        "            print(f\"Observation: {result}\")\n",
        "            conversation.append({\"role\": \"system\", \"content\": f\"Observation: {result}\"})\n",
        "        else:\n",
        "            # Task is complete\n",
        "            break\n",
        "\n",
        "    return conversation[-1][\"content\"]\n",
        "\n",
        "# Helper function to parse action calls\n",
        "def parse_action(action_call):\n",
        "    # Extract the action name and parameters\n",
        "    action_name, params_str = action_call.split(\"(\", 1)  # Limit split to 1\n",
        "\n",
        "    print(action_name)\n",
        "    print(params_str)\n",
        "\n",
        "    action_name = action_name.strip()\n",
        "    params_str = params_str.rstrip(\")\")\n",
        "    params = {}\n",
        "\n",
        "    if params_str:\n",
        "        for param in params_str.split(\",\"):\n",
        "            if \"=\" in param:  # Check if key-value pair exists\n",
        "                key, value = param.split(\"=\", 1)  # Limit split to 1\n",
        "                key = key.strip()\n",
        "                value = value.strip().strip(\"'\\\"\")\n",
        "                params[key] = int(value) if value.isdigit() else value\n",
        "            else:\n",
        "                # Handle cases where '=' is not present, e.g., log a warning\n",
        "                print(f\"Warning: Parameter '{param}' is missing a key-value assignment.\")\n",
        "                # You can choose to ignore this parameter, assign a default value,\n",
        "                # or raise an exception depending on your specific requirements.\n",
        "    return action_name, params"
      ],
      "metadata": {
        "id": "ErptKhtSNzhw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    user_query = \"Can you add 1553 and 2034?\"\n",
        "    print(\"Final Answer:\", react_agent(user_query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6LdUkYMO5UL",
        "outputId": "16700c77-579e-4590-aa43-72656ffbd5af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: Action: calculate_sum(1553, 2034)\n",
            "Warning: Parameter '1553' is missing a key-value assignment.\n",
            "Warning: Parameter ' 2034' is missing a key-value assignment.\n",
            "Observation: Unknown action.\n",
            "Assistant: The sum of 1553 and 2034 is 3587. Would you like to perform any other calculations or tasks?\n",
            "Final Answer: The sum of 1553 and 2034 is 3587. Would you like to perform any other calculations or tasks?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    user_query = \"Can you search what is a LLM in the web?\"\n",
        "    print(\"Final Answer:\", react_agent(user_query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTMzyR_EPriQ",
        "outputId": "379cea29-4b00-468d-fa1a-8b2399702510"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: Action: search_web(query='what is a LLM')\n",
            "search_web\n",
            "query='what is a LLM')\n",
            "Observation: 1. What is a large language model (LLM)? - Cloudflare - https://www.cloudflare.com/learning/ai/what-is-large-language-model/\n",
            "2. What are large language models (LLMs)? - IBM - https://www.ibm.com/think/topics/large-language-models\n",
            "3. What are LLMs, and how are they used in generative AI? - https://www.computerworld.com/article/1627101/what-are-large-language-models-and-how-are-they-used-in-generative-ai.html\n",
            "Assistant: A Large Language Model (LLM) is a type of artificial intelligence model designed to understand and generate human-like text. These models are trained on vast amounts of text data and utilize deep learning techniques to predict the next word in a sentence, generate coherent text, and perform various natural language processing tasks.\n",
            "\n",
            "Key features of LLMs include:\n",
            "\n",
            "1. **Scale**: They consist of billions of parameters, making them capable of capturing complex patterns in language.\n",
            "2. **Pre-training and fine-tuning**: They undergo a two-step training process, where they are first pre-trained on a large corpus of text and then fine-tuned on specific tasks.\n",
            "3. **Applications**: LLMs can be used for chatbots, content generation, language translation, and more.\n",
            "\n",
            "For more details, you can check out these resources:\n",
            "- [Cloudflare on LLMs](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\n",
            "- [IBM's explanation](https://www.ibm.com/think/topics/large-language-models)\n",
            "- [Computerworld's overview](https://www.computerworld.com/article/1627101/what-are-large-language-models-and-how-are-they-used-in-generative-ai.html)\n",
            "Final Answer: A Large Language Model (LLM) is a type of artificial intelligence model designed to understand and generate human-like text. These models are trained on vast amounts of text data and utilize deep learning techniques to predict the next word in a sentence, generate coherent text, and perform various natural language processing tasks.\n",
            "\n",
            "Key features of LLMs include:\n",
            "\n",
            "1. **Scale**: They consist of billions of parameters, making them capable of capturing complex patterns in language.\n",
            "2. **Pre-training and fine-tuning**: They undergo a two-step training process, where they are first pre-trained on a large corpus of text and then fine-tuned on specific tasks.\n",
            "3. **Applications**: LLMs can be used for chatbots, content generation, language translation, and more.\n",
            "\n",
            "For more details, you can check out these resources:\n",
            "- [Cloudflare on LLMs](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\n",
            "- [IBM's explanation](https://www.ibm.com/think/topics/large-language-models)\n",
            "- [Computerworld's overview](https://www.computerworld.com/article/1627101/what-are-large-language-models-and-how-are-they-used-in-generative-ai.html)\n"
          ]
        }
      ]
    }
  ]
}