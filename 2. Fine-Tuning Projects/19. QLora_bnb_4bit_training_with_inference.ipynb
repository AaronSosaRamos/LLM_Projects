{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"78103416c77e4df690864225850fe5d7":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ab86ec2efdec4fbea73e62a7a2a6872c","IPY_MODEL_ff3a24f4356e475788d05ce35a4e6110","IPY_MODEL_0d4653dee8fd4af997fe7d5a44c05a88","IPY_MODEL_5ac262b05f2542c5a63636087517b08c"],"layout":"IPY_MODEL_32a0c5873e364332b81e96542be99632"}},"534d40d694d341289a1d8088113b3990":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_377ca665254847e3b26ebe4f5b06ee47","placeholder":"​","style":"IPY_MODEL_ef7b9c48b5e34460a9ef294e0ee8dbeb","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"0f1dfb1d062a4adbabeadf34265b538f":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_d604dc72a57c4b6bade0a24410c571a8","placeholder":"​","style":"IPY_MODEL_535b7a5d2ea441d2bfcd4dcefc18fce7","value":""}},"742b8a9703ec415aa244d87b595750da":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_f6dfbc0ada76456c88ebffab19bf0719","style":"IPY_MODEL_b8ccaca9f0d94fdcae6b2b8fd2a2f707","value":true}},"9ffb4af6122245058b879852a0ac70b0":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_bfadcba349a7436fbc45c85eb8bb59cb","style":"IPY_MODEL_c0f415df449a4d5ca9577d24bd67105f","tooltip":""}},"a9402ea402cc43beaae32590d7ee140d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_982c3f2684da43199147106c4e1f9e45","placeholder":"​","style":"IPY_MODEL_b2e76357a15147d0b72be8d217d98ad1","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"32a0c5873e364332b81e96542be99632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"377ca665254847e3b26ebe4f5b06ee47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef7b9c48b5e34460a9ef294e0ee8dbeb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d604dc72a57c4b6bade0a24410c571a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"535b7a5d2ea441d2bfcd4dcefc18fce7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6dfbc0ada76456c88ebffab19bf0719":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8ccaca9f0d94fdcae6b2b8fd2a2f707":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfadcba349a7436fbc45c85eb8bb59cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0f415df449a4d5ca9577d24bd67105f":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"982c3f2684da43199147106c4e1f9e45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2e76357a15147d0b72be8d217d98ad1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6c45600947849148e184eff811490b2":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68e9f915b1c5413e95ba308c50031f23","placeholder":"​","style":"IPY_MODEL_7b4b3de6d30a47d3b2c03081ee98fbd1","value":"Connecting..."}},"68e9f915b1c5413e95ba308c50031f23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b4b3de6d30a47d3b2c03081ee98fbd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab86ec2efdec4fbea73e62a7a2a6872c":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9346fa62396433f9356e27080d566b9","placeholder":"​","style":"IPY_MODEL_06989eef0d734fa1b7cc83789b502a14","value":"Token is valid (permission: write)."}},"ff3a24f4356e475788d05ce35a4e6110":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8ed6677d4974fec83e42ba94c573bc5","placeholder":"​","style":"IPY_MODEL_7328bee5776d4ccc8061beb53ed5079a","value":"Your token has been saved in your configured git credential helpers (store)."}},"0d4653dee8fd4af997fe7d5a44c05a88":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_627bd6820a7c42b082adfe2663f8d760","placeholder":"​","style":"IPY_MODEL_8e45fbe793254218aa6590cb184ba421","value":"Your token has been saved to /root/.cache/huggingface/token"}},"5ac262b05f2542c5a63636087517b08c":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_317816a31c8d442ca0e396efa14f700a","placeholder":"​","style":"IPY_MODEL_ef5b8ac85a0b48eeb1c5271bdca1a7d5","value":"Login successful"}},"f9346fa62396433f9356e27080d566b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06989eef0d734fa1b7cc83789b502a14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8ed6677d4974fec83e42ba94c573bc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7328bee5776d4ccc8061beb53ed5079a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"627bd6820a7c42b082adfe2663f8d760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e45fbe793254218aa6590cb184ba421":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"317816a31c8d442ca0e396efa14f700a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef5b8ac85a0b48eeb1c5271bdca1a7d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ef29f2ddb3e475dbb93764cac1d4045":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f696a894b96497182abbf901c6d6dcf","IPY_MODEL_6ee470b8ebe64ec7bea92f51162edadf","IPY_MODEL_2ebfcb1252b44aaf83c682df9056d784"],"layout":"IPY_MODEL_26c32c2cad2f4639b6c15c29ddc6e2aa"}},"1f696a894b96497182abbf901c6d6dcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acbc739cd6884b7a93875d8ab45c7b24","placeholder":"​","style":"IPY_MODEL_f8e69ca6b5594e13911ca99d84582b0c","value":"adapter_model.safetensors: 100%"}},"6ee470b8ebe64ec7bea92f51162edadf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_325ec8373cd24798b75e3a5a87f3428e","max":9459448,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22366bac488c43eabdf384a0f5b9c98d","value":9459448}},"2ebfcb1252b44aaf83c682df9056d784":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6d7bcfe2a574f9b822cf4dbdcf5e1b9","placeholder":"​","style":"IPY_MODEL_2e64a4f5a6874bfeb7202ac5254bdda6","value":" 9.46M/9.46M [00:00&lt;00:00, 30.9MB/s]"}},"26c32c2cad2f4639b6c15c29ddc6e2aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acbc739cd6884b7a93875d8ab45c7b24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8e69ca6b5594e13911ca99d84582b0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"325ec8373cd24798b75e3a5a87f3428e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22366bac488c43eabdf384a0f5b9c98d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6d7bcfe2a574f9b822cf4dbdcf5e1b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e64a4f5a6874bfeb7202ac5254bdda6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# `transformers` meets `bitsandbytes` for democratzing Large Language Models (LLMs) through 4bit quantization\n","\n","<center>\n","<img src=\"https://github.com/huggingface/blog/blob/main/assets/96_hf_bitsandbytes_integration/Thumbnail_blue.png?raw=true\" alt=\"drawing\" width=\"700\" class=\"center\"/>\n","</center>\n","\n","Welcome to this notebook that goes through the recent `bitsandbytes` integration that includes the work from Wilfredo Aaron Sosa Ramos that introduces no performance degradation 4bit quantization techniques, for democratizing LLMs inference and training.\n","\n","In this notebook, we will learn together how to load a large model in 4bit (`gpt-neo-x-1.3b`) and train it using Google Colab and PEFT library from Hugging Face 🤗.\n","\n","[In the general usage notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing), you can learn how to propely load a model in 4bit with all its variants.\n","\n","If you liked the previous work for integrating [*LLM.int8*](https://arxiv.org/abs/2208.07339), you can have a look at the [introduction blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) to lean more about that quantization method.\n"],"metadata":{"id":"XIyP_0r6zuVc"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"FuXIFTFapAMI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a7926ad-386a-406a-8a9f-22229eeb76cf","executionInfo":{"status":"ok","timestamp":1719533250588,"user_tz":300,"elapsed":73526,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q -U bitsandbytes\n","!pip install -q -U git+https://github.com/huggingface/transformers.git\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q datasets"]},{"cell_type":"markdown","source":["First let's load the model we are going to use - GPT-neo-x-20B! Note that the model itself is around 40GB in half precision"],"metadata":{"id":"MJ-5idQwzvg-"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","model_id = \"EleutherAI/gpt-neo-1.3B\"\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"],"metadata":{"id":"E0Nl5mWL0k2T","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7efb5789-b511-48ea-9dd9-8c6819b695b1","executionInfo":{"status":"ok","timestamp":1719533323718,"user_tz":300,"elapsed":11382,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I2RM_erNJFHD","executionInfo":{"status":"ok","timestamp":1719533436182,"user_tz":300,"elapsed":362,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"0518e0e3-7055-443d-b8f4-f3b3883cf44f"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPTNeoForCausalLM(\n","  (transformer): GPTNeoModel(\n","    (wte): Embedding(50257, 2048)\n","    (wpe): Embedding(2048, 2048)\n","    (drop): Dropout(p=0.0, inplace=False)\n","    (h): ModuleList(\n","      (0-23): 24 x GPTNeoBlock(\n","        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPTNeoAttention(\n","          (attention): GPTNeoSelfAttention(\n","            (attn_dropout): Dropout(p=0.0, inplace=False)\n","            (resid_dropout): Dropout(p=0.0, inplace=False)\n","            (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n","          )\n","        )\n","        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPTNeoMLP(\n","          (c_fc): Linear4bit(in_features=2048, out_features=8192, bias=True)\n","          (c_proj): Linear4bit(in_features=8192, out_features=2048, bias=True)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."],"metadata":{"id":"Mp2gMi1ZzGET"}},{"cell_type":"code","source":["from peft import prepare_model_for_kbit_training\n","\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"],"metadata":{"id":"a9EUEDAl0ss3","executionInfo":{"status":"ok","timestamp":1719533440656,"user_tz":300,"elapsed":406,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"],"metadata":{"id":"gkIcwsSU01EB","executionInfo":{"status":"ok","timestamp":1719533442119,"user_tz":300,"elapsed":4,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model\n","\n","config = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"attn.attention.k_proj\", \"attn.attention.v_proj\", \"attn.attention.q_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, config)\n","print_trainable_parameters(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ybeyl20n3dYH","outputId":"ac3d98ef-ff45-4945-83a8-a0cba36ba0af","executionInfo":{"status":"ok","timestamp":1719533446289,"user_tz":300,"elapsed":420,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 2359296 || all params: 713955328 || trainable%: 0.33045428859100834\n"]}]},{"cell_type":"markdown","source":["Let's load a common dataset, english quotes, to fine tune our model on famous quotes."],"metadata":{"id":"FCc64bfnmd3j"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","data = load_dataset(\"Abirate/english_quotes\")\n","data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"],"metadata":{"id":"s6f4z8EYmcJ6","executionInfo":{"status":"ok","timestamp":1719533452141,"user_tz":300,"elapsed":4018,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem."],"metadata":{"id":"_0MOtwf3zdZp"}},{"cell_type":"code","source":["import transformers\n","\n","# needed for gpt-neo-x tokenizer\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=data[\"train\"],\n","    args=transformers.TrainingArguments(\n","        per_device_train_batch_size=1,\n","        gradient_accumulation_steps=4,\n","        warmup_steps=2,\n","        max_steps=100,\n","        learning_rate=2e-4,\n","        fp16=True,\n","        logging_steps=1,\n","        output_dir=\"outputs\",\n","        optim=\"paged_adamw_8bit\"\n","    ),\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jq0nX33BmfaC","outputId":"3968b4f9-cf10-4b2b-d98a-939ab968069b","executionInfo":{"status":"ok","timestamp":1719533602874,"user_tz":300,"elapsed":147174,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["max_steps is given, it will override any value given in num_train_epochs\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 02:20, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.893100</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.389900</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.515400</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.551100</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>3.139700</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>3.398200</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>2.924800</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.865500</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>3.257200</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.496100</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>2.026800</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.429800</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>2.838700</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>3.062700</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>2.551600</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>2.798100</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>2.298800</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>2.629700</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>2.650800</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.267800</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>2.771500</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>2.755500</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>2.382300</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>2.835600</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>2.410800</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>2.268900</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>2.706200</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>2.698000</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>2.556700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.849000</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>2.860700</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>2.436500</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>2.494900</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>2.840800</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>2.057800</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>3.436100</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>2.679700</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>2.491000</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>2.503200</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.390700</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>2.362500</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>3.041800</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>2.001600</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>2.874900</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>2.276900</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>2.359900</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>2.203600</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>2.744600</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>1.764900</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.725900</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>2.895400</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>2.773800</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>3.046200</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>2.467100</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>2.356300</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>2.865500</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>2.374000</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>1.863300</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>2.374500</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.760200</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>2.293800</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>2.732100</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>2.265200</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>2.798700</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>2.685700</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>2.355700</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>2.708000</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>2.130500</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>2.155300</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>2.430000</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>2.774300</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>2.649400</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>2.456400</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>2.437700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>2.108400</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>2.442300</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>3.069800</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>2.286400</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>2.548400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.804800</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>3.057600</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>2.660800</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>2.224100</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>2.325900</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>2.802900</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>2.932600</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>2.373600</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>2.199800</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>2.057800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.907500</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>2.662000</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>3.098500</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>3.070200</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>2.686300</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>2.005800</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>3.178400</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>2.503000</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>2.677200</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>2.484500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>2.169700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=100, training_loss=2.5675597870349884, metrics={'train_runtime': 142.8047, 'train_samples_per_second': 2.801, 'train_steps_per_second': 0.7, 'total_flos': 122420607197184.0, 'train_loss': 2.5675597870349884, 'epoch': 0.1594896331738437})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(\"outputs\")"],"metadata":{"id":"p66mZk1RAlOR","executionInfo":{"status":"ok","timestamp":1719533644255,"user_tz":300,"elapsed":334,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["lora_config = LoraConfig.from_pretrained('outputs')\n","model = get_peft_model(model, lora_config)"],"metadata":{"id":"L2Hllu-bCuN6","executionInfo":{"status":"ok","timestamp":1719533646072,"user_tz":300,"elapsed":516,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["text = \"Barack Obama \"\n","device = \"cuda:0\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","outputs = model.generate(**inputs, max_new_tokens=20)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"],"metadata":{"id":"kEESIVXyESi-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719533692373,"user_tz":300,"elapsed":3697,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"d9116045-78b2-46ae-e955-a9016f7a3c09"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Barack Obama  \n","President of the United States  \n","(born April 20, 1961)\n","\n","The\n"]}]},{"cell_type":"code","source":["text = \"Mark Zuckerberg \"\n","device = \"cuda:0\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","outputs = model.generate(**inputs, max_new_tokens=20)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jmvu6CAcMgdb","executionInfo":{"status":"ok","timestamp":1719533755145,"user_tz":300,"elapsed":3289,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"9ea8d316-602e-415d-9524-94fda9d778e1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Mark Zuckerberg  \n"," **Facebook**  \n"," **Facebook**  \n"," **Facebook**  \n","\n"]}]},{"cell_type":"code","source":["text = \"Sam Altman \"\n","device = \"cuda:0\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","outputs = model.generate(**inputs, max_new_tokens=20)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQozHYPjMumx","executionInfo":{"status":"ok","timestamp":1719533767948,"user_tz":300,"elapsed":3088,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"0635da4a-f67b-4f2c-e323-832f8e48d7ab"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Sam Altman  \n","\n","#1 New York Times bestselling author\n","\n","**_The Last Time I Saw Her\n"]}]},{"cell_type":"code","source":["text = \"Businesspeople \"\n","device = \"cuda:0\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","outputs = model.generate(**inputs, max_new_tokens=20)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3IGxbuLMxVm","executionInfo":{"status":"ok","timestamp":1719533858265,"user_tz":300,"elapsed":3905,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"fc80b75e-c6ae-4fcd-cd22-4fb2566e2c22"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Businesspeople \n","\n","Category:1940 births\n","Category:Living people\n","Category:American businesspeople\n","Category\n"]}]},{"cell_type":"code","source":["text = \"Elon Musk \"\n","device = \"cuda:0\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","outputs = model.generate(**inputs, max_new_tokens=20)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"],"metadata":{"id":"T1TiIH6vAlr_","outputId":"929c7a41-00a6-41b6-a385-40e3a72bcbd7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719533678387,"user_tz":300,"elapsed":6978,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Elon Musk \n","\n","The Tesla Model 3 is the most expensive car in the world, and the most expensive car\n"]}]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["78103416c77e4df690864225850fe5d7","534d40d694d341289a1d8088113b3990","0f1dfb1d062a4adbabeadf34265b538f","742b8a9703ec415aa244d87b595750da","9ffb4af6122245058b879852a0ac70b0","a9402ea402cc43beaae32590d7ee140d","32a0c5873e364332b81e96542be99632","377ca665254847e3b26ebe4f5b06ee47","ef7b9c48b5e34460a9ef294e0ee8dbeb","d604dc72a57c4b6bade0a24410c571a8","535b7a5d2ea441d2bfcd4dcefc18fce7","f6dfbc0ada76456c88ebffab19bf0719","b8ccaca9f0d94fdcae6b2b8fd2a2f707","bfadcba349a7436fbc45c85eb8bb59cb","c0f415df449a4d5ca9577d24bd67105f","982c3f2684da43199147106c4e1f9e45","b2e76357a15147d0b72be8d217d98ad1","c6c45600947849148e184eff811490b2","68e9f915b1c5413e95ba308c50031f23","7b4b3de6d30a47d3b2c03081ee98fbd1","ab86ec2efdec4fbea73e62a7a2a6872c","ff3a24f4356e475788d05ce35a4e6110","0d4653dee8fd4af997fe7d5a44c05a88","5ac262b05f2542c5a63636087517b08c","f9346fa62396433f9356e27080d566b9","06989eef0d734fa1b7cc83789b502a14","a8ed6677d4974fec83e42ba94c573bc5","7328bee5776d4ccc8061beb53ed5079a","627bd6820a7c42b082adfe2663f8d760","8e45fbe793254218aa6590cb184ba421","317816a31c8d442ca0e396efa14f700a","ef5b8ac85a0b48eeb1c5271bdca1a7d5"]},"id":"QB-1NXy3NM7G","executionInfo":{"status":"ok","timestamp":1719533989358,"user_tz":300,"elapsed":439,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"2148b03c-ab29-46a0-af3e-48e5891592d4"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78103416c77e4df690864225850fe5d7"}},"metadata":{}}]},{"cell_type":"code","source":["model.push_to_hub(\"WilAI/gpt-neo-x-1.3b-qlora-test\",\n","                  use_auth_token=True,\n","                  commit_message=\"basic training\",\n","                  private=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["3ef29f2ddb3e475dbb93764cac1d4045","1f696a894b96497182abbf901c6d6dcf","6ee470b8ebe64ec7bea92f51162edadf","2ebfcb1252b44aaf83c682df9056d784","26c32c2cad2f4639b6c15c29ddc6e2aa","acbc739cd6884b7a93875d8ab45c7b24","f8e69ca6b5594e13911ca99d84582b0c","325ec8373cd24798b75e3a5a87f3428e","22366bac488c43eabdf384a0f5b9c98d","a6d7bcfe2a574f9b822cf4dbdcf5e1b9","2e64a4f5a6874bfeb7202ac5254bdda6"]},"id":"Zi4QbniBNx3u","executionInfo":{"status":"ok","timestamp":1719534076286,"user_tz":300,"elapsed":6570,"user":{"displayName":"Wilfredo Aaron Sosa Ramos","userId":"09390934792605274189"}},"outputId":"e533bb13-28c6-4376-e8a9-b412aa429832"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:875: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["adapter_model.safetensors:   0%|          | 0.00/9.46M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ef29f2ddb3e475dbb93764cac1d4045"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/WilAI/gpt-neo-x-1.3b-qlora-test/commit/ad5d31bf0c5eeb481dbfa6cba61f5abe69f9d775', commit_message='basic training', commit_description='', oid='ad5d31bf0c5eeb481dbfa6cba61f5abe69f9d775', pr_url=None, pr_revision=None, pr_num=None)"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["https://huggingface.co/WilAI/gpt-neo-x-1.3b-qlora-test"],"metadata":{"id":"Pn5VD3MeOc03"}}]}