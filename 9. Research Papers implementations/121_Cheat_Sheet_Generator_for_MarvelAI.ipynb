{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Cheat Sheet Generator with Open Source LLMs and Cohere\n",
        "Made by: Wilfredo Aaron Sosa Ramos (AI Lab Manager at RealityAI Labs)"
      ],
      "metadata": {
        "id": "0hliAYZ9IgMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install the dependencies:"
      ],
      "metadata": {
        "id": "fhJTav1rJBlW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qes-V0ndIVn0",
        "outputId": "2b065fc5-5008-4e8e-a8be-9c74ad9f30f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/250.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_core langchain_community chroma langchain_chroma langchain_groq langchain_cohere"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sS3W24CL6Fh",
        "outputId": "3c9fba9e-e399-42d2-c07e-d2b67af64c11"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/298.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Set env. variables"
      ],
      "metadata": {
        "id": "BZcfQS2kJa2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "os.environ['COHERE_API_KEY'] = userdata.get('COHERE_API_KEY')"
      ],
      "metadata": {
        "id": "EwFyHymmJcUJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Create vector store with a PDF file"
      ],
      "metadata": {
        "id": "iQDerhslJj9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1. Create File Handler"
      ],
      "metadata": {
        "id": "4_mVm7y1KubW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import uuid\n",
        "import requests\n",
        "\n",
        "class FileHandler:\n",
        "    def __init__(self, file_loader, file_extension):\n",
        "        self.file_loader = file_loader\n",
        "        self.file_extension = file_extension\n",
        "\n",
        "    def load(self, url):\n",
        "        # Generate a unique filename with a UUID prefix\n",
        "        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n",
        "\n",
        "        try:\n",
        "            # Download the file from the URL and save it to a temporary file\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "\n",
        "            with tempfile.NamedTemporaryFile(delete=False, prefix=unique_filename) as temp_file:\n",
        "                temp_file.write(response.content)\n",
        "                temp_file_path = temp_file.name\n",
        "\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            raise Exception(f\"Failed to download file from URL\", url) from req_err\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to handle file download\", url) from e\n",
        "\n",
        "        # Use the file_loader to load the documents\n",
        "        try:\n",
        "            loader = self.file_loader(file_path=temp_file_path)\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"No file found\", temp_file_path) from e\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"No file content available\", temp_file_path) from e\n",
        "\n",
        "        # Remove the temporary file\n",
        "        os.remove(temp_file_path)\n",
        "\n",
        "        return documents"
      ],
      "metadata": {
        "id": "gbEnrRz5KuIk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2. Create PDF document loader util function"
      ],
      "metadata": {
        "id": "hD7edAx-K_VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 100\n",
        ")\n",
        "\n",
        "def load_pdf_documents(pdf_url: str):\n",
        "    pdf_loader = FileHandler(PyPDFLoader, \"pdf\")\n",
        "    docs = pdf_loader.load(pdf_url)\n",
        "\n",
        "    if docs:\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        return split_docs"
      ],
      "metadata": {
        "id": "7iUNN3HNLDO0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = load_pdf_documents(\"https://arxiv.org/pdf/2403.03853\")"
      ],
      "metadata": {
        "id": "AjeoXDomLpB5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS6xYNxfLxmM",
        "outputId": "a1c0ef14-e574-49a1-9540-73a6ab217bc2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 0}, page_content='ShortGPT: Layers in Large Language Models are More Redun-\\ndant Than You Expect\\nXin Men∗\\nBaichuan Inc.\\nMingyu Xu∗\\nBaichuan Inc.\\nQingyu Zhang∗\\nISCAS\\nBingning Wang †\\nBaichuan Inc.\\nHongyu Lin\\nISCAS\\nYaojie Lu\\nISCAS\\nXianpei Han\\nISCAS\\nWeipeng Chen\\nBaichuan Inc.\\nAbstract\\nAs Large Language Models (LLMs) continue to advance in performance,\\ntheir size has increased significantly, with current LLMs containing billions\\nor even trillions of parameters. In this study, we identify notable redun-\\ndancy across the layers of LLMs, where some layers contribute minimally\\nto overall network functionality. To quantify this, we introduce a metric\\ncalled Block Influence (BI) which use the similarity between layer’s input\\nand output to measure the importance of each layer. Based on the observa-\\ntion of layer redundancy, we propose a straightforward pruning method:\\nlayer removal, which eliminates redundant layers based on their BI scores.\\nOur approach, termed ShortGPT, demonstrates superior performance over'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 0}, page_content='Our approach, termed ShortGPT, demonstrates superior performance over\\nprevious state-of-the-art pruning methods. Moreover, ShortGPT is orthogo-\\nnal to quantization-like methods, enabling further reduction in parameters\\nand computation. The ability to achieve better results through simple layer\\nremoval, as opposed to more complex pruning techniques, suggests a high\\ndegree of redundancy across layers, not only in transformer models but also\\nin non-transformer models. We hope this work will contribute to future\\nresearch in LLM compression.\\n1 Introduction\\nThe field of large language models (LLMs) has witnessed rapid development recently, with\\nLLMs achieving impressive performance across various domains. Guided by the scaling\\nlaws identified in prior work (Kaplan et al., 2020; Hoffmann et al., 2022), current LLM\\nresearch tend to increase model parameters to boost performance. As a result, modern\\nLLMs, which can comprise billions to trillions of parameters, require significant hardware'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 0}, page_content='LLMs, which can comprise billions to trillions of parameters, require significant hardware\\nresources for deployment, creating substantial barriers to their practical use.\\nTo mitigate the hardware demands of large models, model compression techniques have\\nbecome a critical area of focus (Zhu et al., 2023). These techniques are generally divided\\ninto quantization (Liu et al., 2021; Gholami et al., 2022; Dettmers et al., 2022; 2024) and\\npruning(LeCun et al., 1989; Han et al., 2015; Frantar & Alistarh, 2023). Quantization reduces\\nthe precision of model parameters, but its effectiveness often requires specific hardware\\nsupport. In contrast, pruning method removes redundant parameters to decrease the\\nmodel’s size and computation, offering a more flexible and hardware-agnostic approach.\\nDespite its advantages, many existing pruning methods are complex; for example, some\\nrequire gradient information (Ma et al., 2024), which limits their practicality.'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 0}, page_content='require gradient information (Ma et al., 2024), which limits their practicality.\\nIn this paper, we focus on the issue of layer redundancy in LLMs and propose a novel\\napproach for simplifying these models. We introduce Block Influence (BI), a metric that\\nquantifies how much the hidden state changes after passing through each layer, providing a\\nmore direct measure of a layer’s importance. Leveraging this insight, we propose a simple\\n∗Equal contribution\\n†Corresponding author, daniel@baichuan-inc.com\\n1\\narXiv:2403.03853v3  [cs.CL]  11 Oct 2024'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 1}, page_content='0 5 10 15 20 25 30\\nLayer id\\n101\\n102\\n103\\n104\\nPerplexity\\nBaichuan2-7B-Base\\nLlama2-7B-Base\\nLlama2-7B-Base-Baseline\\nBaichuan2-7B-Base-Baseline\\n(a) Perplxity\\n0 5 10 15 20 25 30\\nLayer id\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nPerplexity\\n Baichuan2-7B-Base\\nLlama2-7B-Base\\nLlama2-7B-Base-Baseline\\nBaichuan2-7B-Base-Baseline (b) MMLU\\nFigure 1: Performance of removing certain layer from LLMs. We can see that certain layers\\nare redundant, and their removal results in minimal performance degradation.\\nyet effective pruning method ShortGPT, which identifies and removes layers with lower BI\\nscores, significantly reducing model size without sacrificing much performance.\\nTo evaluate our approach, we conducted evaluation across comprehensive benchmarks. Our\\nexperiments revealed that our method exhibits a smaller performance decrement compared\\nto the previous methods. For instance, removing 10 layers (25% of the total 40 layers)\\nfrom the LLaMA 2-13B model resulted in only a slight drop in performance on the MMLU'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 1}, page_content='from the LLaMA 2-13B model resulted in only a slight drop in performance on the MMLU\\nbenchmark (Hendrycks et al., 2020), from 55.0 to 52.2. Our findings highlight substantial\\nredundancy in current LLMs and suggest potential avenues for improving the efficiency of\\nmodel training by reducing inherent redundancy in the future.\\nThe main contributions of our paper are summarized as follows:\\n• We analyze the redundancy in large language models (LLMs) and find that they\\nexhibit significant redundancy at the layer level. This finding inspire us to prune\\nLLMs by simply removing redundant layers.\\n• We propose a metric called Block Influence (BI) as an indicator of layer importance.\\nBased on BI, our layer removal method maintains approximately 90% performance\\nwhile reducing approximately 25% of parameters, outperforming previous state-of-\\nthe-art methods.\\n• Furthermore, we demonstrate that our layer pruning approach is orthogonal to'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 1}, page_content='the-art methods.\\n• Furthermore, we demonstrate that our layer pruning approach is orthogonal to\\nquantization methods, meaning it can be combined with quantization techniques to\\nfurther reduce the deployment overhead of LLMs.\\n2 Motivation\\n2.1 Background\\nThe predominant LLMs are primarily based on the Transformer architecture (Vaswani et al.,\\n2017), with the pre-norm configuration being the most commonly adopted, as in models\\nlike LLaMA (Touvron et al., 2023). The pre-norm configuration, where layer normalization\\nis applied before the self-attention and feed-forward layers, offers several advantages such\\nas faster convergence, improved training stability, and better scalability for deeper networks\\n(Xiong et al., 2020; Liu et al., 2020; Wang et al., 2024). Due to these benefits, the pre-norm\\napproach has been adopted even in non-transformer models, such as Mamba (Gu & Dao,\\n2023) and RWKV (Peng et al., 2023). For the sake of simplicity in descriptions, our analysis'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 1}, page_content='2023) and RWKV (Peng et al., 2023). For the sake of simplicity in descriptions, our analysis\\nprimarily focuses on the Transformer architecture, though we extend our experiments to\\nnon-Transformer structures in Section 4.4.\\nHowever, we observe that when pre-norm is adopted, the similarity between the input and\\noutput of transformer layers tends to be higher, as illustrated in Figure 2. This high similarity\\nindicates that certain layers induce minimal changes to the hidden states, suggesting they\\ncontribute little to the model’s overall function. A detailed mathematical explanation for\\nthis phenomenon is provided in Appendix A. Which suggests that the deep layers of the\\n2'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 2}, page_content='0 5 10 15 20 25 30 35 40 45 50\\nTokens(B)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSimilarity\\npre norm 0-th layer\\npre norm 1-th layer\\npre norm 3-th layer\\npre norm 7-th layer\\npre norm 15-th layer\\npre norm 31-th layer\\npost norm 0-th layer\\npost norm 1-th layer\\npost norm 3-th layer\\npost norm 7-th layer\\npost norm 15-th layer\\npost norm 31-th layer\\nFigure 2: The cosine similarity between a layer’s input and output during the training\\nprocess. The horizontal axis (X-axis) represents the number of training tokens, while the\\nvertical axis (Y-axis) depicts the degree of similarity. Notably, the model employing post-\\nnormalization exhibits divergence after approximately ∼26B tokens of training. Training\\nsetting is provided in E.\\nmodel with pre-norm might not play a critical role in the overall function, and that the\\nlayers in large language models could be more redundant than expected, which motivates\\nthe layer-removal based pruning method we explore in the next section.\\n2.2 Layer redundancy'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 2}, page_content='the layer-removal based pruning method we explore in the next section.\\n2.2 Layer redundancy\\nTable 1: Ablation of removing FFN and\\nAttention of Llama2-7B-Base. We sample\\n100 instances from PG19 (Rae et al., 2019)\\nto calculate PPL.\\nDelete PPL\\nNone 7.60\\nThe whole last layer 13.37\\nAttention of the last layer 7.65\\nFFN of the last layer 12.35\\nAs discussed in the previous section, we spec-\\nulate that the LLMs exhibit layer redundancy.\\nTo verify this, we assess the performance degra-\\ndation caused by removing individual layers\\nof two popular models, Llama2-7B-Base (Tou-\\nvron et al., 2023), an English based LLMs, and\\nBaichuan2-7B-Base (Yang et al., 2023) which is\\nmainly focused on Chinese. Figure 1 confirms\\nour speculation, which reveals that some lay-\\ners do not play a crucial role in LLMs, causing\\nlittle degradation when omitting them individ-\\nually. Moreover, this redundancy is primarily\\nmanifested in the middle to later layers of the\\nnetwork, with the initial layers and the last layer'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 2}, page_content='manifested in the middle to later layers of the\\nnetwork, with the initial layers and the last layer\\noften being more critical. Notably, we found the last layer to be particularly important,\\naligning with findings from LLM Pruner (Ma et al., 2024). This observation contradicts\\nour mathematical explanation in Appendix A which suggests that deeper layers tend to\\nbe more redundant. We posit that this discrepancy arises because the final FFN effectively\\nfunctions as part of the token classifier and should be considered in conjunction with the\\nlanguage model head.To verify our hypothesis, we conducted further investigation, detailed\\nin Table 1. The results show that within the last layer, the FFN component is crucial, while\\nthe Attention module is less significant. This finding supports our interpretation of the final\\nlayer’s importance.\\n3'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 3}, page_content='0 5 10 15 20 25 30\\nLayer id\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\nBI score\\n0\\n5\\n10\\n15\\n20\\nPerplexity\\nBI score Perplexity\\n(a) Llama2 7B\\n0 5 10 15 20 25 30\\nLayer id\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nBI score\\n0\\n5\\n10\\n15\\n20\\n25\\nPerplexity\\nBI score Perplexity (b) Baichuan2 7B\\nFigure 3: The BI score of a layer and the PPL after removing the layer.\\n3 Methodology\\nIn this section, we present the methodological framework of our layer removal approach\\nfor LLMs, elucidating the underlying principles and techniques employed. We begin\\nby introducing Block Influence (BI), a novel metric designed to assess the hidden states\\ntransformation of each layer. Leveraging BI, we then detail our layer removal method.\\n3.1 Layer importance\\nAs outlined in the preceding section, the layers of LLMs exhibit redundancy, with varying\\ndegrees of redundancy across different layers. To capture this, we introduce a new metric,\\nBlock Influence (BI), to measure the degree of transformation performed by each layer. The'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 3}, page_content='Block Influence (BI), to measure the degree of transformation performed by each layer. The\\nBI score of ith layer can be calculated as follows:\\nBIi = 1 − EX,t\\nXT\\ni,tXi+1,t\\n||Xi,t||2||Xi+1,t||2\\n, (1)\\nwhere Xi,t means the tth row of hidden states of ith layer. Lower BI score imply that Xi and\\nXi+1 exhibit high cosine similarity, suggesting that the layer makes minimal transformations\\nto the hidden states and is therefore less important. We plot the BI scores of a single layer\\nand the PPL after removing it separately, as shown in the Figure 3. The results demonstrate\\na positive correlation between the BI score and the importance of a layer.\\n3.2 Layer Removal\\nOur goal is to obtain a pruned model that remains as close as possible to the original model.\\nSince an LLM functions as a series of transformations applied to hidden states across its\\nlayers and we can determine the importance of each layer, we propose a straightforward'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 3}, page_content='layers and we can determine the importance of each layer, we propose a straightforward\\npruning method: layer removal, which we refer to as ShortGPT. We delete certain layers\\nin LLMs based on BI score. First of all, we construct a calibration set, which is a set of\\nunlabelled text samples such as PG19 (Rae et al., 2019). Then we collect the hidden states of\\neach layer during inference on these samples. Next, we calculate the BI score based on the\\ncollected hidden states. Finally, we sort layers in ascending order according to the BI, and\\ndelete the layers with the lower BI score. The number of layers to be deleted can vary to\\ntrade off the speed and performance. The details of our layer removal setting can be found\\nin Appendix D.\\n4 Experiments\\n4.1 Experimental Setup\\nModels. To validate the effectiveness of our method, we conducted experiments on ex-\\nisting popular open-source language models, including Llama2-7B (Touvron et al., 2023),'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 3}, page_content='isting popular open-source language models, including Llama2-7B (Touvron et al., 2023),\\nLlama2-13B, Baichuan2-7B, and Baichuan2-13B. They are all large language models based\\n4'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 4}, page_content='Table 2: Comparison of pruning methods on multiple natural language benchmarks. The\\nresults of LLMPrun., SliceGPT and LaCo are reported from LaCo. The last column reports\\nthe relative performance retention.\\nLLM Method Ratio Benchmarks Ave. Per.\\nCMNLI HeSw PIQA CHID WSC CoQA BoolQ Race-H Race-M XSum C3 MMLU CMMLU\\nLlama2-7B\\nDense 0.00% 32.99 71.26 77.91 41.66 50.00 64.62 71.62 35.71 34.19 19.40 43.56 45.39 32.92 47.78 100.00\\nLLMPrun. 27.0% 34.33 56.46 71.22 25.25 36.54 42.51 55.20 22.56 22.35 11.51 25.64 23.33 25.25 34.78 72.79\\nSliceGPT 26.4% 31.70 50.27 66.21 20.79 36.54 41.36 38.32 21.07 21.66 4.89 39.78 28.92 25.37 32.84 68.73\\nLaCo 27.1% 34.43 55.69 69.80 36.14 40.38 45.70 64.07 22.61 23.61 15.64 39.67 26.45 25.24 38.41 80.39\\nShortGPT 27.1% 32.95 53.02 66.43 24.68 52.46 47.99 74.71 32.25 35.17 0.67 39.62 43.96 32.25 41.24 86.31\\nLlama2-13B\\nDense 0.00% 32.99 74.78 79.71 47.35 50.00 66.91 82.39 57.95 60.38 23.45 47.51 55.00 38.40 55.14 100.00'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 4}, page_content='LLMPrun. 24.4% 33.03 67.76 76.66 35.64 40.38 50.86 56.42 22.47 22.08 19.17 32.33 25.21 24.71 38.97 70.67\\nSliceGPT 23.6% 29.82 55.71 69.04 19.31 36.54 47.26 37.86 23.41 24.03 5.27 41.92 37.14 25.79 34.85 63.20\\nLaCo 24.6% 32.86 64.39 63.20 40.10 52.88 52.66 63.98 54.49 56.55 14.45 44.93 45.93 32.62 47.62 86.36\\nShortGPT 24.6% 33.00 66.64 73.45 36.61 50.00 58.64 62.48 58.35 60.17 17.59 46.90 54.69 38.38 50.53 91.64\\nBaichuan2-7B\\nDense 0.00% 33.37 67.56 76.17 85.56 50.00 63.14 74.10 52.63 51.04 20.82 64.55 53.87 56.95 57.67 100.00\\nLLMPrun. 24.2% 32.28 53.66 71.82 69.80 53.85 47.83 61.19 21.96 22.28 15.98 41.64 24.93 25.69 41.76 72.41\\nSliceGPT 22.2% 32.07 25.29 50.33 14.85 36.54 19.57 39.30 23.53 22.49 0.00 26.58 25.18 25.25 26.23 45.48\\nLaCo 24.2% 33.00 52.28 68.50 76.24 42.31 47.26 56.15 28.99 27.72 12.03 50.85 31.53 31.24 42.93 74.44\\nShortGPT 24.2% 33.30 56.96 67.68 65.63 50.00 46.70 67.83 53.26 46.76 0.04 56.33 45.77 47.87 49.08 85.10\\nBaichuan2-13B'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 4}, page_content='Baichuan2-13B\\nDense 0.00% 33.21 71.10 78.07 86.51 50.00 65.6 77.89 67.27 68.94 25.02 65.64 59.50 61.30 62.31 100.00\\nLLMPrun. 24.3% 33.80 53.57 71.82 72.77 37.50 38.82 56.54 21.17 21.61 13.67 39.89 23.19 25.18 39.20 62.91\\nSliceGPT 22.8% 32.07 25.85 51.03 10.40 36.54 18.02 37.83 21.56 21.52 0.00 24.99 22.95 25.26 25.23 40.49\\nLaCo 24.7% 33.03 60.71 68.88 76.73 44.23 55.45 62.35 56.92 57.80 12.32 61.10 51.35 53.65 53.43 85.75\\nShortGPT 24.7% 32.81 60.55 71.60 80.17 47.13 54.30 62.54 55.77 56.41 15.14 60.16 52.11 58.86 54.43 87.35\\non the decoder-only Transformer architecture. LLaMA 2 was trained on more than 2 trillion\\ntokens. Baichuan-series was mainly trained in Chinese and its 13-Billion model replaced the\\nRoPE (Su et al., 2024) positional embedding with ALiBi (Press et al., 2021).\\nBenchmarks. In order to comprehensively evaluate the changes in the ability of large\\nlanguage models before and after pruning, we conducted comprehensive evaluation from'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 4}, page_content='language models before and after pruning, we conducted comprehensive evaluation from\\nfive aspect: Reasoning: CMNLI (Li et al., 2024), HellaSwag (HeSw) (Zellers et al., 2019),\\nPIQA (Bisk et al., 2020). Language: CHID (Zheng et al., 2019), WSC (Levesque et al., 2012).\\nKnowledge: CommonSenseQA (CoQA) (Reddy et al., 2019), BoolQ (Clark et al., 2019).\\nExamination: MMLU (Hendrycks et al., 2020), CMMLU (Li et al., 2024). Understanding:\\nRace-High/Middle (H/M) (Lai et al., 2017), XSum (Hasan et al., 2021), C3 (Sun et al., 2020)\\nand PG19 (Rae et al., 2019). For more details, please refer to Appendix G\\nBaselines. To evaluate the effectiveness of our method, we compared several structured\\npruning methods for large language models, including:\\n1) LLMPru (Ma et al., 2024), which adopts structural pruning that selectively removes\\nnon-critical coupled structures based on gradient information, maximally preserving the'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 4}, page_content='non-critical coupled structures based on gradient information, maximally preserving the\\nmajority of the LLM’s functionality. LLMPru. applies post training to the pruned model,\\nbut for fair comparison, we do not apply post training to it.\\n2) SliceGPT (Ashkboos et al., 2024), which is a post-training sparsification scheme that\\nreplaces each weight matrix with a smaller matrix, reducing the embedding dimension\\nof the network. Specifically, they applied PCA to the hidden representation from shallow\\nto deep layers, and incorporated the dimension reduction matrix into existing network\\nparameters.\\n3) LaCo (Yang et al., 2024), which is a pruning method for large language models based\\non reducing layers. LaCo gradually merges similar layers from deep to shallow and sets a\\nthreshold to avoid continuously merging too many layers.\\nFor our evaluation, we use PG19 for layer importance and perplexity calculation. The\\nmodels, baselines and evaluate benchmarks is the same as LaCo.\\n5'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 5}, page_content='4.2 Main Results\\nTo validate the efficacy of our proposed method, we conducted comparative experiments\\nagainst baseline techniques commonly employed in large language model evaluation.\\nConsidering the current structured pruning methods generally reduce parameters by no\\nmore than 30%, we performed experiments with approximately 1/4 of the parameters\\npruned. The experimental results are presented in Table 2. Additional experiments exploring\\ndifferent parameter reduction proportions will be discussed in the subsequent section.\\nThe results demonstrate that the performance of the model pruned by our method signif-\\nicantly surpasses that of the baseline methods, maintaining most of the large language\\nmodel’s capabilities. Furthermore, we note that the approach of reducing the number of\\nlayers (ShortGPT/LaCo) outperforms the method of reducing the embedding dimensions\\n(LLMPru./SliceGPT), implying that the model exhibits more redundancy in depth than in'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 5}, page_content='(LLMPru./SliceGPT), implying that the model exhibits more redundancy in depth than in\\nwidth. Further experimental analysis will be presented in the ensuing section.\\nIn Table 2, we fully adopted the benchmark, model, and pruning ratio in the LaCo paper.\\nIn order to make a more fair comparison with LLMprun. and SliceGPT, we compared\\nthem with the same benchmark, model, and pruning ratio in their original paper. The\\nexperimental results are shown in Appendix C. Consistent with our findings in Table 2,\\nthese experiments further demonstrate the significant layer redundancy present in existing\\nlarge language models, and ShortGPT achieves superior performance compared to other\\npruning methods.\\nThe results show that coarse-grained pruning methods, such as removing entire layers,\\noften outperform fine-grained approaches like Slice GPT or LLM Pruner. We speculate that\\nthe reason is that the large language model is actually very robust, as shown in Figure 1,'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 5}, page_content='the reason is that the large language model is actually very robust, as shown in Figure 1,\\nremoving any deep layer individually actually has very little impact on the final output,\\nwhich means it is difficult to define the importance of a finer grained module and perform\\npruning.\\n4.3 Varying metric and pruning ratio\\nThe core principle of our method is to rank layers by their importance and remove the less\\nsignificant ones. The choice of importance metric significantly influences the outcome. In\\nthis section, we define and compare several different importance metrics:\\n• Sequential: The importance is directly proportional to the sequence order, with\\nshallower layers being less important. This can be implemented by assigning the\\nnegative value of each layer’s index as its importance metric.\\n• Norm/Reverse-order: This metric posits that importance is inversely proportional\\nto the sequence order. It assigns higher importance scores to the shallower layers.'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 5}, page_content='to the sequence order. It assigns higher importance scores to the shallower layers.\\nThis method gives the same order as measuring importance by hidden states norm\\nas Figure 4 shows.\\n• Relative Magnitude: Proposed in Samragh et al. (2023), this metric assumes layers\\nwith larger || f (x)\\nx+ f (x) || are of higher importance, where f is the layer transformation\\nfunction.\\n• BI: we calculate the BI score mentioned in Section 3.1 as importance metric.\\nFigure 4 demonstrates the different metrics. We observe that shallower layers in the LLM\\nnetwork are more crucial than deeper ones. Figure 5 shows the results of removing layers\\nby different metrics, demonstrating that Our proposed BI outperforms other metrics. The\\nmethod of Relative Magnitude is highly competitive, indicating that relative values can also\\nreflect the importance to some extent. It is worth noting that the hidden states norm seems\\nto be a good metric when only considering the MMLU benchmark, but the perplexity is'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 5}, page_content='to be a good metric when only considering the MMLU benchmark, but the perplexity is\\nrelatively poor.\\nAs a pruning method, we further validated the effects of different pruning ratios on model\\nperformance. Experiments were conducted on the Llama2 and Baichuan2 models, observing\\n6'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 6}, page_content='0 5 10 15 20 25 30\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nBI\\nBI\\nBaichuan-7B-Base\\nLlama2-7B-Base\\n0 5 10 15 20 25 30\\n0\\n5\\n10\\n15\\n20\\n25\\nNorm\\nNorm\\nBaichuan-7B-Base\\nLlama2-7B-Base\\n0 5 10 15 20 25 30\\nlayer_id\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRelative magnitude\\nRelative magnitude\\nBaichuan-7B-Base\\nLlama2-7B-Base\\n0 5 10 15 20 25 30\\nlayer_id\\n101\\n102\\n103\\n104\\nPerplexity\\nPerplexity\\nBaichuan-7B-Base\\nLlama2-7B-Base\\nFigure 4: Comparison of different importance metrics. Perplexity is calculated by removing\\neach single layer, other metrics is calculated by hidden states of each layer.\\n0 9 19 28 38 47 56 66 75 84 94\\n101\\n102\\n103\\n104\\n105\\n106\\nPerplexity\\nLlama2-7B-Base\\n0 8 15 22 30 38 45 52 60 68 75 82 90 98\\n101\\n102\\n103\\n104\\n105\\n106\\nLlama2-13B-Base\\n0 9 19 28 38 47 56 66 75 84 94\\nPruning Ratio(%)\\n25\\n30\\n35\\n40\\n45\\nMMLU\\n0 8 15 22 30 38 45 52 60 68 75 82 90 98\\nPruning Ratio(%)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nSequential Reverse-order Relative Magnitude BI\\nFigure 5: Performance of MMLU and perplexity when we prune by different metrics, with'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 6}, page_content='Figure 5: Performance of MMLU and perplexity when we prune by different metrics, with\\nincreasing pruning ratio. We can see that as the pruning ratio increases, the performance of\\nthe model declines.\\nthe Perplexity and MMLU. The results for Llama2, as shown in Figure 5, demonstrate that\\nthe model’s performance generally declines as the pruning ratio increases. However, we\\nobserve a notable phenomenon: the MMLU score exhibits a sharp drop at a specific layer.\\n7'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 7}, page_content='Table 3: ShortGPT pruning on RWKV and Mamba.\\nModel Pruning ratio CMNLI HeSw PIQA CHID WSC CoQA BoolQ Race-H Race-M XSum C3 MMLU CMMLU Ave. Per.\\nMamba-2.8B\\n0% 35.97 61.84 75.52 35.56 49.69 56.35 60.67 24.9 25.3 15.03 42.08 26.29 25.32 41.12 100.00\\n10.9% 32.95 59.71 73.01 32.52 49.28 52.66 51.41 24.27 25.21 14.95 41.1 26.01 25.00 39.08 95.04\\n20.3% 31.29 55.69 69.64 29.12 48.36 48.32 62.2 23.61 23.61 14.71 41.59 25.69 25.37 38.36 93.29\\n25% 29.96 52.38 68.77 26.02 48.26 44.96 62.2 23.67 23.26 14.00 40.71 24.32 24.89 37.18 90.42\\n31.3% 28.25 47.02 64.91 21.38 49.69 44.96 62.17 21.87 22.77 13.77 40.44 24.48 24.77 35.59 86.55\\nRWKV-7B\\n0% 32.07 65.98 77.09 85.36 50.00 62.65 62.72 38.56 45.47 16.5 57.97 31.85 28.54 50.37 100.00\\n9.4% 32.6 56.41 73.94 78.12 50.00 49.55 62.35 25.9 25.77 9.57 54.68 27.29 25.03 43.94 87.23\\n18.8% 32.11 49.47 71.55 65.63 50.00 40.54 61.19 22.04 23.75 8.13 49.15 26.35 25 40.38 80.17'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 7}, page_content='18.8% 32.11 49.47 71.55 65.63 50.00 40.54 61.19 22.04 23.75 8.13 49.15 26.35 25 40.38 80.17\\n25% 32.41 39.73 65.13 52.6 50.00 29.65 60.92 22.56 21.59 12.02 41.86 25.52 25.08 36.85 73.16\\n28.1% 33.11 32.22 60.01 32.47 50.1 28.34 60.85 22.27 21.31 10.43 37.81 25.64 25.15 33.82 67.14\\nTable 4: Layer removal results on Llama2-7B-Base-GPTQ.\\nModel Ratio/Layer Perplexity MMLU Throughput (speed up)\\nBaseline 0%/32 8.03 43.17 4331.23 Token/s (1.00x)\\n3.1%/31 8.37 42.88 4399.31 Token/s (1.02x)\\n9.4%/29 9.44 42.31 4602.26 Token/s (1.06x)\\nShortGPT 12.5%/28 10.24 41.62 4680.68 Token/s (1.08x)\\n15.6%/27 11.42 43.17 4756.94 Token/s (1.10x)\\n25.0%/24 22.29 41.68 5045.59 Token/s (1.16x)\\n27.1%/23 40.78 43.35 5146.99 Token/s (1.19x)\\nThis sudden decrease suggests the presence of certain critical layers within the network\\nthat play a particularly important role in maintaining performance. Similar patterns are\\nobserved in the Baichuan2 model, as illustrated in Appendix B.\\n4.4 Redundancy on non-transformer LLM'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 7}, page_content='4.4 Redundancy on non-transformer LLM\\nTo determine whether the observed depth redundancy is specific to the Transformer archi-\\ntecture, we extended our investigation to include two popular non-Transformer models,\\nRWKV-7B1 (Peng et al., 2023) and Mamba-2.8B2 (Gu & Dao, 2023). Our experiments revealed\\nthat these models also exhibit resilience to layer removal, maintaining performance despite\\nthe elimination of certain layers. This finding suggests that the redundancy phenomenon\\nmay not be unique to Transformer-based models, but rather a common characteristic across\\ncurrent large language models. Table 3 shows that our method is applicable and effective\\nfor both Mamba and RWKV models, suggesting that the redundancy is universal across\\ncurrent LLMs. However, it is worth noting that the RWKV model appears less redundant\\nthan Mamba and Transformer models, which warrants further investigation.\\n4.5 Orthogonal to Quantization'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 7}, page_content='4.5 Orthogonal to Quantization\\nIn this section, we show that our method is orthogonal to quantization methods. We apply\\nour method to Llama2-7B 3 quantized by GPTQ algorithm. Table 4 shows that our method is\\ncompatible with the quantization-like method. In addition, we compared the performance\\nof applying pruning before quantization 4. The results shown in the Table 5 further indicates\\nthat quantization and ShortGPT are orthogonal operations.\\n1We use rwkv-v5-world-7B from https://huggingface.co/RWKV/v5-Eagle-7B-HF\\n2We take the model from https://huggingface.co/state-spaces/mamba-2.8b-hf\\n3We take the model from https://huggingface.co/TheBloke/Llama-2-7B-GPTQ\\n4We use GPTQ algorithm for quantization from https://github.com/AutoGPTQ/AutoGPTQ\\n8'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 8}, page_content='Table 5: Performance comparison of different methods\\nMethod MMLU CMMLU\\nLlama2-7B-Baseline 45.4 32.9\\n4-bit quantization 44.9 32.5\\nLayer removal (27.1%) 44.0 32.3\\n4-bit quantization then layer removal 42.4 31.0\\nLayer removal then 4-bit quantization 41.2 30.5\\n4.6 Post training to restore performance\\nTo mitigate the performance loss resulting from layer removal, we explored post-training\\nstrategies inspired by Chen et al. (2024). Our approach comprised two key steps: 1)Replace-\\nment: We substituted the removed layers with lightweight Multi-Layer Perceptron (MLP)\\nmodules. 2)Retraining: We subsequently retrained the modified model. The results in Table\\n6 demonstrate the potential of post-train in recover performance loss. Appendix F list the\\ntraining details.\\nTable 6: Post-train Llama2-7B to restore performance.\\nMethod Avg. Ratio CMNLI HeSw PIQA CHID WSC CoQA BoolQ Race-H Race-M XSum C3 MMLU CMMLU\\nDense 47.78 0% 32.99 71.26 77.91 41.66 50.00 64.62 71.62 35.71 34.19 19.40 43.56 45.39 32.92'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 8}, page_content='Dense 47.78 0% 32.99 71.26 77.91 41.66 50.00 64.62 71.62 35.71 34.19 19.40 43.56 45.39 32.92\\nShortGPT 41.22 27.1% 32.95 53.02 66.43 24.68 52.46 47.99 74.41 32.25 35.17 0.67 39.62 43.96 32.25\\nShortGPT+post-train 43.16 24.0% 32.99 54.83 68.12 31.82 51.37 58.32 72.36 34.18 34.68 4.89 40.37 44.47 32.73\\n5 Limitation\\nAlthough our method demonstrates strong competitiveness compared to current pruning\\nmethods, there are some phenomena that have not been explained. Our experiments reveal\\nthat the negative effect of layer removal is more significant on generative tasks compared to\\nmultiple-choice tasks. When we remove 25% layers from Llama2-7B or Baichuan2-7B, the\\nperformance in generative tasks such as XSum and C3 deceases to nearly zero, although\\nthe performance decline was not as significant on the larger model of the 13B. We speculate\\nthat compared to multiple-choice tasks, generative tasks face the problem of accumulated'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 8}, page_content='that compared to multiple-choice tasks, generative tasks face the problem of accumulated\\nerrors and large model is more robust than small one. The reasons behind it still need to\\nbe explored. The post-training techniques discussed in Section 4.6 have the potential to\\nmitigate this issue and warrant further exploration.\\n6 Related works\\nTo reduce the inference cost of large language models and increase their practical applica-\\ntions, there have been many recent works on compressing models, which can be classified\\ninto two categories: model pruning and quantization. Besides, there are some works aim to\\nstudy the redundancy of model which is essential for compressing models.\\nModel pruning: model pruning (LeCun et al., 1989; Han et al., 2015) is a classic and effective\\nmethod of reducing model redundancy modules to compress models. The model pruning\\nmethods mainly include unstructured pruning and structured pruning. The unstructured'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 8}, page_content='methods mainly include unstructured pruning and structured pruning. The unstructured\\npruning simplifies an LLM by removing specific parameters without considering its internal\\nstructure, such as SparseGPT (Frantar & Alistarh, 2023) and LoRAPrune (Zhang et al., 2023).\\nHowever, this method disregards the overall LLM structure, resulting in an irregular sparse\\nmodel composition. Another more practical approach is structured pruning, GUM(Syed\\n9'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 9}, page_content='et al., 2023) makes an analysis of several structured pruning methods for decoder-only\\nLLMs. LLM-Pruner (Ma et al., 2024) selectively removes non-critical structures according to\\ngradient information. ShearedLLaMA (Xia et al., 2023) employs targeted structured pruning\\nand dynamic batch loading. LaCo (Yang et al., 2024) used layer merging to compress the\\nmodel. Compared to the previous method, our method is a simple and efficient structured\\npruning method.\\nQuantization: quantization (Liu et al., 2021; Gholami et al., 2022; Dettmers et al., 2022; 2024)\\nis a widely accepted technique in the field of model compression, which can significantly\\nsave the storage and computational costs of deep learning models. Traditional models are\\ngenerally stored as floating-point numbers, but quantization converts them into integers or\\nother discrete forms. LUT-GEMM (Park et al., 2022) quantifies only weights and optimizes'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 9}, page_content='other discrete forms. LUT-GEMM (Park et al., 2022) quantifies only weights and optimizes\\nmatrix multiplication in LLM using BCQ format. SPQR (Dettmers et al., 2023) identifies\\nand isolates abnormal weights, stores them with higher accuracy, and compresses all other\\nweights into 3-4 bits. Our model pruning method and quantization method are orthogonal,\\nwhich means quantification based on our pruned model can further compress the model.\\nModel redundancy: researchers have long noticed the significant redundancy in nonlinear\\nmodels (Catchpole & Morgan, 1997). In recent years, the transformer model architecture\\nhas been widely applied, and researchers have also studied its redundancy. In (Bian et al.,\\n2021), researchers analyzed redundancy in attention mechanisms, in which clear and similar\\nredundancy patterns (cluster structure) are observed among attention heads. In (Dalvi et al.,\\n2020), researchers dissect two pre-trained models, BERT (Devlin et al., 2018) and XLNet'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 9}, page_content='2020), researchers dissect two pre-trained models, BERT (Devlin et al., 2018) and XLNet\\n(Yang et al., 2019), studying how much redundancy they exhibit at a representation level\\nand a more fine-grained neuron-level. However, the redundancy in current large language\\nmodels based on decoder-only structures still needs to be explored.\\n7 Conclusion\\nIn this work, we uncovered the significant layer-wise redundancy of LLMs, Our research\\ndemonstrates that certain layers contribute minimally to overall network functionality\\nand can be removed without substantially compromising model performance. Based\\non our observation, We introduce Block influence to quantify the importance of each\\nlayer and propose a simple and straightforward pruning method: layer removal. Our\\nexperiments demonstrates that it is possible to maintain up to approximately 90% of a\\nLLM’s performance while reducing the model’s parameter amount and computational'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 9}, page_content='LLM’s performance while reducing the model’s parameter amount and computational\\nrequirements by approximately 25%. Besides, our method is orthogonal to quantization\\nmethods and can be further improved by continual training. We hope that our work\\ncan provide some insight for future model compression techniques. Moreover, our work\\nsuggests potential avenues for improving the efficiency of model training by reducing\\ninherent redundancy in the future.\\n10'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 10}, page_content='References\\nSaleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler,\\nand James Hensman. Slicegpt: Compress large language models by deleting rows and\\ncolumns. arXiv preprint arXiv:2401.15024, 2024.\\nYuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan, and Kenneth Church. On attention\\nredundancy: A comprehensive study. In Proceedings of the 2021 conference of the north\\namerican chapter of the association for computational linguistics: human language technologies,\\npp. 930–945, 2021.\\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\\nintelligence, pp. 7432–7439, 2020.\\nEdward A Catchpole and Byron JT Morgan. Detecting parameter redundancy. Biometrika,\\n84(1):187–196, 1997.\\nXiaodong Chen, Yuxuan Hu, and Jing Zhang. Compressing large language models by\\nstreamlining the unimportant layer. arXiv preprint arXiv:2403.19135, 2024.'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 10}, page_content='streamlining the unimportant layer. arXiv preprint arXiv:2403.19135, 2024.\\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\\npp. 2924–2936, 2019.\\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy\\nin pretrained transformer models, 2020.\\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\\nTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar,\\nSaleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 10}, page_content='Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A\\nsparse-quantized representation for near-lossless llm weight compression. arXiv preprint\\narXiv:2306.03078, 2023.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\\nfinetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805, 2018.\\nElias Frantar and Dan Alistarh. Massive language models can be accurately pruned in\\none-shot. arXiv preprint arXiv:2301.00774, 2023.\\nAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt\\nKeutzer. A survey of quantization methods for efficient neural network inference. In\\nLow-Power Computer Vision, pp. 291–326. Chapman and Hall/CRC, 2022.'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 10}, page_content='Low-Power Computer Vision, pp. 291–326. Chapman and Hall/CRC, 2022.\\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.\\narXiv preprint arXiv:2312.00752, 2023.\\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections\\nfor efficient neural network. Advances in neural information processing systems, 28, 2015.\\nTahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-\\nBin Kang, M Sohel Rahman, and Rifat Shahriyar. Xl-sum: Large-scale multilingual\\nabstractive summarization for 44 languages. In Findings of the Association for Computational\\nLinguistics: ACL-IJCNLP 2021, pp. 4693–4703, 2021.\\n11'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 11}, page_content='Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300, 2020.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\\nTom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\\nand Laurent Sifre. Training compute-optimal large language models, 2022.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\\nlanguage models, 2020.\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale\\nreading comprehension dataset from examinations. In Proceedings of the 2017 Conference'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 11}, page_content='reading comprehension dataset from examinations. In Proceedings of the 2017 Conference\\non Empirical Methods in Natural Language Processing, pp. 785–794, 2017.\\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural\\ninformation processing systems, 2, 1989.\\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and\\nTimothy Baldwin. Cmmlu: Measuring massive multitask language understanding in\\nchinese, 2024.\\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding\\nthe difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP), pp. 5747–5763, 2020.\\nZhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training\\nquantization for vision transformer. Advances in Neural Information Processing Systems, 34:\\n28092–28103, 2021.\\nXinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 11}, page_content='Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of\\nlarge language models. Advances in neural information processing systems, 36, 2024.\\nGunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo\\nLee. nuqmm: Quantized matmul for efficient inference of large-scale generative language\\nmodels. arXiv preprint arXiv:2206.09557, 2022.\\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao,\\nXin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , et al. Rwkv: Reinventing\\nrnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.\\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear\\nbiases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.\\nCompressive transformers for long-range sequence modelling. In International Conference'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 11}, page_content='Compressive transformers for long-range sequence modelling. In International Conference\\non Learning Representations, 2019.\\nSiva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question\\nanswering challenge. Transactions of the Association for Computational Linguistics, 7:249–266,\\n2019.\\nMohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash\\nFaghri, Devang Naik, Oncel Tuzel, and Mohammad Rastegari. Weight subcloning: direct\\ninitialization of transformers using larger pretrained ones, 2023.\\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\\nEnhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\\n12'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 12}, page_content='Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging\\nchinese machine reading comprehension. Transactions of the Association for Computational\\nLinguistics, 8:141–155, 2020.\\nAaquib Syed, Phillip Huang Guo, and Vijaykaarti Sundarapandiyan. Prune and tune:\\nImproving efficient pruning techniques for massive language models. Arxiv, 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\\ntion processing systems, 30, 2017.\\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 12}, page_content='Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.\\nDeepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 2024.\\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating\\nlanguage model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.\\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai\\nZhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the trans-\\nformer architecture. In International Conference on Machine Learning , pp. 10524–10533.\\nPMLR, 2020.\\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian\\nYu, Cong Yu, et al. Clue: A chinese language understanding evaluation benchmark. In\\nProceedings of the 28th International Conference on Computational Linguistics, pp. 4762–4772,\\n2020.\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv,'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 12}, page_content='2020.\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv,\\nDa Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models.\\narXiv preprint arXiv:2309.10305, 2023.\\nYifei Yang, Zouying Cao, and Hai Zhao. Laco: Large language model pruning via layer\\ncollapse, 2024.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. Advances\\nin neural information processing systems, 32, 2019.\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\\na machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics, pp. 4791–4800, 2019.\\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural\\nInformation Processing Systems, 32, 2019.\\nMingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al.'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 12}, page_content='Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al.\\nPruning meets low-rank parameter-efficient fine-tuning. arXiv preprint arXiv:2305.18403,\\n2023.\\nChujie Zheng, Minlie Huang, and Aixin Sun. Chid: A large-scale chinese idiom dataset\\nfor cloze test. In Proceedings of the 57th Annual Meeting of the Association for Computational\\nLinguistics, pp. 778–787, 2019.\\nXunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression\\nfor large language models, 2023.\\n13'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 13}, page_content='A Mathematical explanation for why pre-norm brings high similarity\\nWe provide a simple explanation here about how pre-norm leads to high deep similarity in\\nthis section, here we adopt RMSNorm (Zhang & Sennrich, 2019) for convenient, which is\\nalso the popular pre-norm used in many recent LLMs, such as Llama and Mamba.\\nLemma 1 (Xiong et al., 2020) At initialization, for the Pre-LN Transformer, (1 + L\\n2 )d ≤\\nE(||xL,i||2\\n2) ≤ (1 + 3L\\n2 )d for all L > 0 and i. Expectations are taken over the input and the\\nrandomness of initialization, where the hidden state of Lth layer is xL.\\nFrom Lemma 1, the hidden state of the pre-norm model will continuously increase as the\\nnumber of layers increases. And under the assumption of each component of xl has a mean\\nof 0, we can obtain ||xL|| = Θ(\\n√\\nL).\\nThen we consider xL+1 = xL + fL(xL, θL), where fL is a operation such as Attention or MLP ,\\nθL is learnable parameters. Then fL(xL, θL) =O(1) respect to L, for Attention as example,'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 13}, page_content='θL is learnable parameters. Then fL(xL, θL) =O(1) respect to L, for Attention as example,\\n||fL(xL, θL)|| = ||(so f tmax(QTK)XL/||XL|| ·(σrms ))WvWq|| = O(||σrms ||||Wv||||Wo||) =\\nO(1) respect to L.\\nThen we can get:\\ncos similarity(XL+1, XL) = xL+1xL\\n||xL+1||||xL|| = ||xL||2\\n||xL+1||||xL|| + fL(xL, θ)xL\\n||xL+1||||xL|| (2)\\n≥ ||xL||2\\n||xL+1||||xL|| − ||fL(xL, θ)||||xL||\\n||xL+1||||xL|| (3)\\n= ||xL||\\n||xL+1|| − ||fL(xL, θ)||\\nxL+1\\n= Θ(\\nr\\nL\\nL + 1 ) − O(\\nr\\n1\\nL + 1 ) (4)\\nThis means that as the number of layers L increases, the similarity between the input and\\noutput of the layer will be high. This means that the role of fL may be relatively small, and\\nremoving it from the network may have a relatively small impact to the model.\\nAlthough the above theoretical analysis is only for randomly initialized models, this phe-\\nnomenon that deep layer has similar input and output exists in both our own trained models\\nshown in Figure 2 and existing models in Figure 4.'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 13}, page_content='shown in Figure 2 and existing models in Figure 4.\\nB Layer Removal on Baichuan2-series Model\\n0 9 19 28 38 47 56 66 75 84 94\\n102\\n104\\n106\\n108\\nPerplexity\\nBaichuan2-7B-Base\\n0 8 15 22 30 38 45 52 60 68 75 82 90 98\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\nBaichuan2-13B-Base\\n0 9 19 28 38 47 56 66 75 84 94\\nPruning Ratio(%)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nMMLU\\n0 8 15 22 30 38 45 52 60 68 75 82 90 98\\nPruning Ratio(%)\\n30\\n40\\n50\\n60\\nSequential Reverse-order Relative Magnitude BI\\nFigure 6: Pruning by different metrics on Baichuan2-series model.\\n14'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 14}, page_content='C A Fair comparison with SliceGPT and LLMprun.\\nIn Table 2, we fully adopted the benchmark, model, and pruning ratio in the LaCo’s paper.\\nFor a fair comparison with LLM pruner and SliceGPT, we do the same experiments in the\\noriginal paper of LLM pruner and SliceGPT. The results is provided in Table 7 and Table 8.\\nWe take the same benchmarks, models and pruning ratio as the corresponding original\\npaper. The results demonstrate that our method is highly competitive.\\nTable 7: Comparison between ShortGPT and LLM-pruner. The Table is corresponding\\nto the Table 1 of LLM pruner(Zhang et al., 2023).\\nModel Pruning ratio Method BoolQ PIQA Hellaswag Winogrande Arc-e Arc-c OBQA Avg.\\nLlama-7B\\nRatio=0% Baseline 73.18 78.35 72.99 67.01 67.45 41.38 42.4 63.25\\nRatio=20% LLM-pruner 59.39 75.57 65.34 61.33 59.18 37.12 39.80 56.82\\nRatio=21.9 % ShortGPT 68.26 72.28 61.7 63.77 60.22 39 41.6 58.12\\nLlama-13B\\nRatio=0% Baseline 68.47 78.89 76.24 70.09 74.58 44.54 42.00 64.97'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 14}, page_content='Llama-13B\\nRatio=0% Baseline 68.47 78.89 76.24 70.09 74.58 44.54 42.00 64.97\\nRatio=20% LLM-pruner 67.68 77.15 73.41 65.11 68.35 38.4 42.4 61.79\\nRatio=20% ShortGPT 68.41 76.36 72.9 67.4 68.62 39.2 41 61.98\\nTable 8: Comparison between ShortGPT and SliceGPT. The Table is\\ncorresponding to the Table 7 of SliceGPT(Ashkboos et al., 2024).\\nModel Pruning ratio Method PIQA Hellaswag Winogrande Arc-e Arc-c Avg.\\nLlama-2-7B\\n0% Baseline 79.11 75.99 69.06 74.58 46.25 69\\n20% SliceGPT 71.87 58.1 63.04 69.87 43.09 63.45\\n25% SliceGPT 68.55 58.1 62.04 57.46 35.07 56.15\\n30% SliceGPT 66.1 52.69 56.82 35.07 56.82 56.15\\n21.9% ShortGPT 72.76 66.39 66.27 59.39 39.85 60.93\\n25% ShortGPT 70.53 62.68 64.7 58.39 39.51 59.16\\n31.6% ShortGPT 67.87 62.19 64.38 56.57 40.86 58.37\\nLlama-2-13B\\n0% Baseline 80.47 79.39 72.22 77.48 49.23 71.76\\n20% SliceGPT 71.87 69.38 63.04 69.87 43.09 63.45\\n25% SliceGPT 68.55 67.48 58.1 62.5 37.88 58.9\\n30% SliceGPT 66.1 65.11 52.69 56.82 35.07 55.16'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 14}, page_content='25% SliceGPT 68.55 67.48 58.1 62.5 37.88 58.9\\n30% SliceGPT 66.1 65.11 52.69 56.82 35.07 55.16\\n20% ShortGPT 76.95 74.67 71.14 69.56 45.63 67.59\\n25% ShortGPT 74.39 71.65 70.98 67.09 43.93 65.61\\n30% ShortGPT 72.11 71.93 67.19 61.09 40.88 62.64\\nLlama-2-70B\\n0% Baseline 82.7 83.84 77.98 80.98 57.34 76.57\\n20% SliceGPT 76.61 72.98 74.92 80.51 55.2 72.34\\n25% SliceGPT 74.92 68.74 74.92 77.9 51.71 69.75\\n30% SliceGPT 72.31 63.69 73.4 51.71 47.61 66.11\\n20% ShortGPT 76.02 78.87 71.69 76.02 52.95 71.68\\n25% ShortGPT 73.2 76.72 71.85 73.2 49.9 69.79\\n30% ShortGPT 74.44 75.31 72.33 74.44 49.22 69.4\\nD Detailed Strategies for Layer Removal\\nWe list the details of different layer removal strategies in Table 10. The concrete removed\\nlayers by ShortGPT in Table 2 are listed in Table 9\\nTable 9: Setup of Removed Layers for Benchmark Models.\\nModel Removed Layers\\nLlama-2-7B 27, 26, 25, 28, 24, 29, 23, 21, 22\\nLlama-2-13B 33, 31, 32, 30, 29, 34, 28, 35, 27, 26\\nBaichuan-2-7B 26, 27, 25, 28, 24, 29, 23, 22, 30'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 14}, page_content='Llama-2-13B 33, 31, 32, 30, 29, 34, 28, 35, 27, 26\\nBaichuan-2-7B 26, 27, 25, 28, 24, 29, 23, 22, 30\\nBaichuan-2-13B 32, 31, 33, 30, 34, 29, 28, 35, 27, 26\\n15'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 15}, page_content='Table 10: Strategies for Layer Removal in Models.\\nStrategy Description\\nSequential Layers are removed sequentially from the beginning of the\\nmodel. The process starts with layer 0 and progressively\\nincludes more layers for removal (e.g., {0}, {0, 1}, . . . ).\\nReverse-order This strategy involves starting from the model’s final layer\\nand progressively removing layers in reverse order (e.g.,\\n{-1}, {-1, -2}, . . . ).\\nRelative Magnitude Layers are removed in ascending order based on their Rel-\\native Magnitude values. The removal process accumulates\\nlayers from those with the smallest to the largest values,\\nmirroring the sequential strategy’s accumulation method.\\nBI (Block Influence) Follows a similar accumulation approach as the Sequential\\nstrategy, but layers are ordered and removed according to\\ntheir BI values, starting from the lowest and moving to the\\nhighest.\\nE Setup for training post-norm model and pre-norm model'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 15}, page_content='highest.\\nE Setup for training post-norm model and pre-norm model\\nWe have listed the specific training settings for pre norm and post norm in Table 11.\\nTable 11: Training Parameters.\\nParameter Value\\nGlobal Batch Size 2048\\nSequence length 4096\\nPrecision bf16\\nLearning Rate Scheduler cosine\\nMax Learning Rate 4e-4\\nMin Learning Rate 5e-5\\nWarm-up steps 3000\\nTraining Tokens 200B\\nWeight Decay 0.1\\nAdam Beta1 0.9\\nAdam Beta2 0.98\\nGradient Clip 1.0\\nTokenizer Llama2\\nLayers 32\\nHidden state 2048\\nAttention heads 32\\nHead dim 64\\nFFN size 5504\\nActivation function Silu\\nF post-training settings\\nWe replace the removed layer with a lightweight gated MLP layer with hidden size = 2048.\\nTable 12 show the post training settings.\\n16'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 16}, page_content='Table 12: Post-training Parameters.\\nParameter Value\\nGlobal Batch Size 2048\\nSequence length 4096\\nPrecision bf16\\nLearning Rate Scheduler cosine\\nMax Learning Rate 2e-5\\nMin Learning Rate 1e-5\\nWarm-up steps 3000\\nTraining Tokens 50B\\nWeight Decay 0.1\\nAdam Beta1 0.9\\nAdam Beta2 0.98\\nGradient Clip 1.0\\nG Evaluation Benchmarks\\nIn order to comprehensively evaluate the changes in the ability of large language models\\nbefore and after pruning, we conducted evaluations on the most commonly used Benchmark\\nMMLU Hendrycks et al. (2020), CMMLU Li et al. (2024) for evaluating large models. In\\naddition, we also followed LaCo Yang et al. (2024) to evaluate a wider dataset.\\nMMLU Hendrycks et al. (2020) is a benchmark aimed at measuring the knowledge acquired\\nduring pre-training by specifically evaluating models in zero-shot and few-shot settings.\\nThis makes benchmarks more challenging and similar to the way we evaluate humans. This'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 16}, page_content='This makes benchmarks more challenging and similar to the way we evaluate humans. This\\nbenchmark covers 57 subjects including STEM, humanities, social sciences, etc. Its difficulty\\nranges from beginner to advanced professional level, and it tests world knowledge and\\nproblem-solving ability.\\nCMMLU Li et al. (2024) is a comprehensive Chinese language assessment dataset designed\\nspecifically to evaluate LLM’s advanced knowledge and reasoning abilities in the context\\nof Chinese language and culture. CMMLU covers 67 topics, from elementary school to\\nuniversity or professional level. Including natural sciences, as well as humanities and social\\nsciences, it also includes many contents with Chinese characteristics.\\nCMNLI Xu et al. (2020) is part of the Chinese language understanding assessment bench-\\nmark. It consists of two parts: XNLI and MNLI. HellaSwag (HeSw) Zellers et al. (2019)\\nis a challenging dataset for evaluating commonsense NLI that is especially hard for state-'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 16}, page_content='is a challenging dataset for evaluating commonsense NLI that is especially hard for state-\\nof-the-art models, though its questions are trivial for humans. PIQA Bisk et al. (2020) is\\na multi-choice question and answer dataset that focuses on daily scenarios. This dataset\\nexplores the model’s grasp of the laws of the real physical world through daily scenarios.\\nCHID Zheng et al. (2019) is an idiom cloze test dataset that mainly focuses on the selection of\\ncandidate words and the representation of idioms. CoQA Reddy et al. (2019) is a large-scale\\ndataset used for conversational question-answering tasks, containing over 127000 questions\\nand their corresponding answers. BoolQ Clark et al. (2019) is a question-answer dataset\\ncontaining 15942 examples of yes/no questions. These problems occur naturally - they are\\ngenerated in an environment that is silent and unconstrained. Race Lai et al. (2017) is a\\nlarge-scale reading comprehension dataset collected from English examinations in China,'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 16}, page_content='large-scale reading comprehension dataset collected from English examinations in China,\\nwhich are designed for middle school and high school students. XSumHasan et al. (2021)\\nis used to evaluate abstract single document summarization systems. The goal is to create\\na short, one-sentence new summary of what the article is about. C3 Sun et al. (2020) is a\\nmachine reading comprehension dataset with multiple choices, consisting of multiple-choice\\nquestions, reading materials from Chinese proficiency exams, and ethnic Chinese exams.\\nPG19 Rae et al. (2019) is a long document dataset from books used to test the effectiveness\\nof language modeling.\\nH Hardware Environment\\nThe platform we use to experiment is GPU heterogeneous platform. The hardware of our\\nplatform is shown in Table 13\\n17'),\n",
              " Document(metadata={'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8', 'page': 17}, page_content='Table 13: Setup of Removed Layers for Benchmark Models.\\nName Details\\nCPU 2x Intel(R) Xeon(R) Gold 6430 CPU @ 2.1GHz\\nGPU 8x NVIDIA A100-80GB Tensor Core GPU\\n18')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3. Create vector store using Cohere and Chroma (For English lang.)"
      ],
      "metadata": {
        "id": "UtpzTMQlLGaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "embedding_model = CohereEmbeddings(model=\"embed-english-v3.0\")"
      ],
      "metadata": {
        "id": "8gCvTeXfLMBj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    collection_name=\"rag\",\n",
        "    embedding=embedding_model,\n",
        ")"
      ],
      "metadata": {
        "id": "1Oy_CtCWLlSD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4. Obtain retriever"
      ],
      "metadata": {
        "id": "FyI5OMPUMSQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={'k': 4},\n",
        "            )"
      ],
      "metadata": {
        "id": "je4KKXZpMT5T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Create Pydantic Schemas:"
      ],
      "metadata": {
        "id": "_O2LcM2SMd8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class TopicSchema(BaseModel):\n",
        "    title: str = Field(..., description=\"The title of the topic covered in the cheat sheet.\")\n",
        "    description: str = Field(..., description=\"A brief description or summary of the topic.\")\n",
        "    key_points: List[str] = Field(..., description=\"List of key takeaways or bullet points for the topic.\")\n",
        "    resources: List[str] = Field(..., description=\"List of external resources or references for the topic.\")\n",
        "\n",
        "class ExampleSchema(BaseModel):\n",
        "    example_title: str = Field(..., description=\"The title or name of the example.\")\n",
        "    code_snippet: str = Field(..., description=\"A code snippet or text representing the example.\")\n",
        "    explanation: str = Field(..., description=\"A detailed explanation of the example and its purpose.\")\n",
        "    output: str = Field(..., description=\"The expected output or result of the example when executed.\")\n",
        "    tags: List[str] = Field(..., description=\"Tags or keywords associated with the example for indexing.\")\n",
        "\n",
        "class SectionSchema(BaseModel):\n",
        "    section_title: str = Field(..., description=\"The title of the section.\")\n",
        "    topics: List[TopicSchema] = Field(..., description=\"List of topics included in this section.\")\n",
        "    examples: List[ExampleSchema] = Field(..., description=\"List of examples relevant to the section.\")\n",
        "    summary: str = Field(..., description=\"A brief summary of the section's content and purpose.\")\n",
        "    importance: int = Field(..., description=\"An importance level or priority score for the section.\")\n",
        "\n",
        "class MetadataSchema(BaseModel):\n",
        "    author: str = Field(..., description=\"The name of the cheat sheet's author.\")\n",
        "    version: str = Field(..., description=\"The version of the cheat sheet.\")\n",
        "    last_updated: str = Field(..., description=\"The last updated date of the cheat sheet in ISO 8601 format.\")\n",
        "    created_date: str = Field(..., description=\"The creation date of the cheat sheet in ISO 8601 format.\")\n",
        "    tags: List[str] = Field(..., description=\"A list of tags or keywords describing the cheat sheet.\")\n",
        "    intended_audience: str = Field(..., description=\"The primary audience or user group for the cheat sheet.\")\n",
        "    license: str = Field(..., description=\"The licensing information for using or sharing the cheat sheet.\")\n",
        "\n",
        "class CheatSheetSchema(BaseModel):\n",
        "    title: str = Field(..., description=\"The main title of the cheat sheet.\")\n",
        "    description: str = Field(..., description=\"A brief overview or introduction to the cheat sheet.\")\n",
        "    metadata: MetadataSchema = Field(..., description=\"Metadata information about the cheat sheet.\")\n",
        "    sections: List[SectionSchema] = Field(..., description=\"A list of sections containing the cheat sheet's content.\")\n",
        "    total_sections: int = Field(..., description=\"The total number of sections in the cheat sheet.\")\n",
        "    difficulty_level: str = Field(..., description=\"The difficulty level of the cheat sheet content.\")\n",
        "    usage_scenarios: List[str] = Field(..., description=\"Scenarios or contexts where this cheat sheet is useful.\")"
      ],
      "metadata": {
        "id": "gogQFnslNUfv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Create RAG chains with Llama and Mixtral"
      ],
      "metadata": {
        "id": "39LrWgPeNuce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser"
      ],
      "metadata": {
        "id": "gHUKHEaDUELh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm_llama_3_1 = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "llm_mixtral_8_7b = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)"
      ],
      "metadata": {
        "id": "nhBqWqcqUG6N"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"Create a Cheat Sheet for Large Language Models\"\n",
        "language = \"English\"\n",
        "context = retriever.invoke(topic)"
      ],
      "metadata": {
        "id": "eKCt190BTPIy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO_nPSoEUg2V",
        "outputId": "6c4c8c72-57aa-4a42-b110-712beedf4b7b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 8, 'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8'}, page_content='that compared to multiple-choice tasks, generative tasks face the problem of accumulated\\nerrors and large model is more robust than small one. The reasons behind it still need to\\nbe explored. The post-training techniques discussed in Section 4.6 have the potential to\\nmitigate this issue and warrant further exploration.\\n6 Related works\\nTo reduce the inference cost of large language models and increase their practical applica-\\ntions, there have been many recent works on compressing models, which can be classified\\ninto two categories: model pruning and quantization. Besides, there are some works aim to\\nstudy the redundancy of model which is essential for compressing models.\\nModel pruning: model pruning (LeCun et al., 1989; Han et al., 2015) is a classic and effective\\nmethod of reducing model redundancy modules to compress models. The model pruning\\nmethods mainly include unstructured pruning and structured pruning. The unstructured'),\n",
              " Document(metadata={'page': 5, 'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8'}, page_content='4.2 Main Results\\nTo validate the efficacy of our proposed method, we conducted comparative experiments\\nagainst baseline techniques commonly employed in large language model evaluation.\\nConsidering the current structured pruning methods generally reduce parameters by no\\nmore than 30%, we performed experiments with approximately 1/4 of the parameters\\npruned. The experimental results are presented in Table 2. Additional experiments exploring\\ndifferent parameter reduction proportions will be discussed in the subsequent section.\\nThe results demonstrate that the performance of the model pruned by our method signif-\\nicantly surpasses that of the baseline methods, maintaining most of the large language\\nmodel’s capabilities. Furthermore, we note that the approach of reducing the number of\\nlayers (ShortGPT/LaCo) outperforms the method of reducing the embedding dimensions\\n(LLMPru./SliceGPT), implying that the model exhibits more redundancy in depth than in'),\n",
              " Document(metadata={'page': 0, 'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8'}, page_content='ShortGPT: Layers in Large Language Models are More Redun-\\ndant Than You Expect\\nXin Men∗\\nBaichuan Inc.\\nMingyu Xu∗\\nBaichuan Inc.\\nQingyu Zhang∗\\nISCAS\\nBingning Wang †\\nBaichuan Inc.\\nHongyu Lin\\nISCAS\\nYaojie Lu\\nISCAS\\nXianpei Han\\nISCAS\\nWeipeng Chen\\nBaichuan Inc.\\nAbstract\\nAs Large Language Models (LLMs) continue to advance in performance,\\ntheir size has increased significantly, with current LLMs containing billions\\nor even trillions of parameters. In this study, we identify notable redun-\\ndancy across the layers of LLMs, where some layers contribute minimally\\nto overall network functionality. To quantify this, we introduce a metric\\ncalled Block Influence (BI) which use the similarity between layer’s input\\nand output to measure the importance of each layer. Based on the observa-\\ntion of layer redundancy, we propose a straightforward pruning method:\\nlayer removal, which eliminates redundant layers based on their BI scores.\\nOur approach, termed ShortGPT, demonstrates superior performance over'),\n",
              " Document(metadata={'page': 1, 'source': '/tmp/38da2325-309c-4807-87ec-7d862dcfc5d9.pdfi8n9sao8'}, page_content='from the LLaMA 2-13B model resulted in only a slight drop in performance on the MMLU\\nbenchmark (Hendrycks et al., 2020), from 55.0 to 52.2. Our findings highlight substantial\\nredundancy in current LLMs and suggest potential avenues for improving the efficiency of\\nmodel training by reducing inherent redundancy in the future.\\nThe main contributions of our paper are summarized as follows:\\n• We analyze the redundancy in large language models (LLMs) and find that they\\nexhibit significant redundancy at the layer level. This finding inspire us to prune\\nLLMs by simply removing redundant layers.\\n• We propose a metric called Block Influence (BI) as an indicator of layer importance.\\nBased on BI, our layer removal method maintains approximately 90% performance\\nwhile reducing approximately 25% of parameters, outperforming previous state-of-\\nthe-art methods.\\n• Furthermore, we demonstrate that our layer pruning approach is orthogonal to')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "\n",
        "def generate_cheatsheet_topics(topic, language, context):\n",
        "    # System prompt\n",
        "    system = \"\"\"\n",
        "    You are an expert assistant specializing in creating well-structured and insightful cheat sheets.\n",
        "    When given a topic, your task is to brainstorm and return a comprehensive list of related subtopics that\n",
        "    are relevant, practical, and helpful for creating a cheat sheet. You must strictly adhere to the following:\n",
        "\n",
        "    1. **Language Specification**: Always return the subtopics in the language requested by the user (e.g., Spanish, English, etc.).\n",
        "    2. **Structure**:\n",
        "        - Each subtopic must have a descriptive title.\n",
        "        - Optionally, include a one-sentence description of each subtopic if requested.\n",
        "    3. **Coverage**:\n",
        "        - Cover fundamental concepts, advanced details, practical applications, and examples.\n",
        "        - Include tools, frameworks, or methodologies relevant to the topic.\n",
        "    4. **Clarity**:\n",
        "        - Use clear, concise language suitable for the given audience's level (beginner, intermediate, advanced).\n",
        "    5. **Examples**:\n",
        "        - Provide diverse subtopics catering to different perspectives or practical uses of the main topic.\n",
        "    6. **Scalability**:\n",
        "        - Ensure the subtopics are modular for easy expansion or division into cheat sheet sections.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt template\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"\"\"\n",
        "            User's topic and language request:\n",
        "            **Topic**: {topic}\n",
        "            **Language**: {language}\n",
        "\n",
        "            You must use the following context: {context}.\n",
        "\n",
        "            Generate a list of subtopics for the cheat sheet with the above context.\n",
        "            \"\"\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Chain definition\n",
        "    rag_chain = prompt | llm_llama_3_1 | StrOutputParser()\n",
        "\n",
        "    # Debugging information\n",
        "    print(f\"Generating cheat sheet topics for: {topic} in {language}\")\n",
        "\n",
        "    # Run the chain\n",
        "    generation = rag_chain.invoke({\n",
        "        \"context\": context,\n",
        "        \"topic\": topic,\n",
        "        \"language\": language,\n",
        "    })\n",
        "\n",
        "    # Return generation results\n",
        "    return generation"
      ],
      "metadata": {
        "id": "9UX7v2NWQSaV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cheat_sheet_topics = generate_cheatsheet_topics(topic, language, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCj58WouUoAu",
        "outputId": "9b46a9dc-613e-4933-a8c5-77edf99f7555"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating cheat sheet topics for: Create a Cheat Sheet for Large Language Models in English\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cheat_sheet_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "769NL4V9UvUV",
        "outputId": "c4838901-e605-4086-f2a6-7acd7e8a74b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a comprehensive list of subtopics for the cheat sheet on \"Create a Cheat Sheet for Large Language Models\" in English:\n",
            "\n",
            "### 1. **Understanding Large Language Models**\n",
            "\n",
            "* **1.1. Definition and Overview**: Define what large language models are and their significance in natural language processing.\n",
            "* **1.2. Types of Large Language Models**: Discuss the different types of large language models, including transformer-based models and recurrent neural networks.\n",
            "* **1.3. Advantages and Challenges**: Highlight the advantages and challenges of large language models, including their ability to handle complex tasks and their high computational requirements.\n",
            "\n",
            "### 2. **Model Pruning Techniques**\n",
            "\n",
            "* **2.1. Introduction to Model Pruning**: Explain the concept of model pruning and its importance in reducing the size of large language models.\n",
            "* **2.2. Unstructured Pruning**: Describe unstructured pruning, including its methods and applications.\n",
            "* **2.3. Structured Pruning**: Explain structured pruning, including its methods and applications.\n",
            "* **2.4. Comparison of Pruning Methods**: Compare and contrast different pruning methods, including their strengths and weaknesses.\n",
            "\n",
            "### 3. **Layer Removal and Block Influence**\n",
            "\n",
            "* **3.1. Introduction to Layer Removal**: Explain the concept of layer removal and its significance in reducing the size of large language models.\n",
            "* **3.2. Block Influence (BI) Metric**: Describe the Block Influence (BI) metric and its application in layer removal.\n",
            "* **3.3. Layer Removal Method**: Explain the layer removal method and its advantages and challenges.\n",
            "\n",
            "### 4. **Quantization Techniques**\n",
            "\n",
            "* **4.1. Introduction to Quantization**: Explain the concept of quantization and its importance in reducing the size of large language models.\n",
            "* **4.2. Types of Quantization**: Discuss the different types of quantization, including integer quantization and ternary quantization.\n",
            "* **4.3. Quantization Methods**: Explain the different quantization methods, including linear quantization and non-linear quantization.\n",
            "\n",
            "### 5. **Model Compression**\n",
            "\n",
            "* **5.1. Introduction to Model Compression**: Explain the concept of model compression and its significance in reducing the size of large language models.\n",
            "* **5.2. Model Compression Techniques**: Discuss the different model compression techniques, including pruning, quantization, and knowledge distillation.\n",
            "* **5.3. Comparison of Compression Techniques**: Compare and contrast different compression techniques, including their strengths and weaknesses.\n",
            "\n",
            "### 6. **Tools and Frameworks**\n",
            "\n",
            "* **6.1. Popular Frameworks**: Discuss popular frameworks for large language models, including TensorFlow, PyTorch, and Hugging Face Transformers.\n",
            "* **6.2. Model Pruning Tools**: Explain model pruning tools, including TensorFlow Model Optimization Toolkit and PyTorch Model Pruning.\n",
            "* **6.3. Quantization Tools**: Discuss quantization tools, including TensorFlow Quantization and PyTorch Quantization.\n",
            "\n",
            "### 7. **Best Practices and Future Directions**\n",
            "\n",
            "* **7.1. Best Practices for Model Pruning**: Provide best practices for model pruning, including selecting the right pruning method and tuning hyperparameters.\n",
            "* **7.2. Best Practices for Quantization**: Explain best practices for quantization, including selecting the right quantization method and tuning hyperparameters.\n",
            "* **7.3. Future Directions**: Discuss future directions for large language models, including the development of more efficient pruning and quantization methods.\n",
            "\n",
            "This list of subtopics provides a comprehensive overview of the key concepts and techniques related to large language models, including model pruning, quantization, and compression. It also covers tools and frameworks, best practices, and future directions, making it a valuable resource for anyone looking to create a cheat sheet on this topic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cheatsheet(list_topics):\n",
        "    # Parser\n",
        "    parser = JsonOutputParser(pydantic_object=CheatSheetSchema)\n",
        "\n",
        "    # Prompt\n",
        "    prompt_message = \"\"\"\n",
        "    You are an advanced assistant specializing in generating structured and insightful cheat sheets.\n",
        "    When given a list of subtopics and their descriptions, your task is to create a fully detailed\n",
        "    cheat sheet strictly following the structure defined below:\n",
        "\n",
        "    1. Use the following structure:\n",
        "       - `title`: The overall title of the cheat sheet.\n",
        "       - `description`: A brief introduction to the cheat sheet, explaining its purpose and scope.\n",
        "       - `metadata`: Include `author`, `version`, `last_updated`, `created_date`, `tags`, `intended_audience`, and `license`.\n",
        "       - `sections`: Divide content into logical sections based on the subtopics.\n",
        "\n",
        "    2. Each section must include:\n",
        "       - `section_title`: The title of the subtopic.\n",
        "       - `topics`: A list of key concepts, each with:\n",
        "         - `title`: Key concept title.\n",
        "         - `description`: Summary of the concept.\n",
        "         - `key_points`: List of essential points to understand.\n",
        "         - `resources`: Links or references for further exploration.\n",
        "       - `examples`: A list of examples, each with:\n",
        "         - `example_title`: Name of the example.\n",
        "         - `code_snippet`: Relevant code or practical example text.\n",
        "         - `explanation`: Explanation of the example's purpose.\n",
        "         - `output`: Expected output or results.\n",
        "         - `tags`: Keywords related to the example.\n",
        "       - `summary`: Summarize the section content in 1–2 sentences.\n",
        "       - `importance`: A numerical score (1–10) for the section's importance.\n",
        "\n",
        "    3. Metadata Rules:\n",
        "       - Always include metadata like the author's name, the cheat sheet's version, and a list of tags relevant to the topic.\n",
        "       - Specify the intended audience (e.g., beginners, intermediate users, experts).\n",
        "\n",
        "    4. Additional Information:\n",
        "       - Include at least one practical example in each section.\n",
        "       - Ensure the content is written in the language requested by the user.\n",
        "\n",
        "    Use this list of topics:\n",
        "    <list_topics>\n",
        "    {list_topics}\n",
        "    </list_topics>\n",
        "\n",
        "    You must respond as a JSON following this format:\n",
        "\n",
        "    <format_instruction>\n",
        "    {format_instructions}\n",
        "    </format_instruction>\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_message,\n",
        "        input_variables=[\"list_topics\"],\n",
        "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "    )\n",
        "\n",
        "    # Chain\n",
        "    cheat_sheet_chain = prompt | llm_mixtral_8_7b | parser\n",
        "\n",
        "    # Run\n",
        "    cheatsheet_response = cheat_sheet_chain.invoke(list_topics)\n",
        "\n",
        "    print(f\"Generated cheat sheet based on the topics: {cheatsheet_response}\")\n",
        "\n",
        "    return cheatsheet_response"
      ],
      "metadata": {
        "id": "fEXxqAWaPI46"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cheat_sheet = create_cheatsheet(cheat_sheet_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUu0pbutWryh",
        "outputId": "61205b8d-6ad5-4328-84f3-5ac94629cd1e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated cheat sheet based on the topics: {'title': 'Create a Cheat Sheet for Large Language Models', 'description': 'This cheat sheet provides a comprehensive overview of large language models, including model pruning, quantization, and compression techniques. It also covers tools and frameworks, best practices, and future directions for large language models.', 'metadata': {'author': 'Advanced Assistant', 'version': '1.0', 'last_updated': '2023-03-15', 'created_date': '2023-03-01', 'tags': ['large language models', 'model pruning', 'quantization', 'compression'], 'intended_audience': 'Intermediate users, Experts', 'license': 'Creative Commons Attribution-NonCommercial 4.0 International'}, 'sections': [{'section_title': '1.1. Definition and Overview', 'topics': [{'title': 'Definition', 'description': 'Large language models are artificial neural networks designed for natural language processing tasks.', 'key_points': ['Handle complex language tasks', 'Deep learning-based', 'Transformer-based or recurrent neural networks'], 'resources': ['Jurafsky, D., & Martin, J. H. (2020). Speech and Language Processing (3rd ed.). Pearson Education.']}], 'examples': [], 'summary': 'Defines large language models and their significance in natural language processing.', 'importance': 10}, {'section_title': '1.2. Types of Large Language Models', 'topics': [{'title': 'Transformer-based Models', 'description': 'Transformer-based models use self-attention mechanisms to process input sequences.', 'key_points': ['BERT, RoBERTa, and DistilBERT', 'Efficient parallel processing', 'Handling long-range dependencies'], 'resources': ['Vaswani, A., et al. (2017). Attention is All You Need. Advances in Neural Information Processing Systems.']}, {'title': 'Recurrent Neural Networks', 'description': 'Recurrent neural networks use recurrent connections to process input sequences.', 'key_points': ['LSTM and GRU', 'Sequential data processing', 'Limited parallelization'], 'resources': ['Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.']}], 'examples': [], 'summary': 'Discusses the different types of large language models, including transformer-based models and recurrent neural networks.', 'importance': 9}, {'section_title': '1.3. Advantages and Challenges', 'topics': [{'title': 'Advantages', 'description': 'Large language models offer several advantages in natural language processing tasks.', 'key_points': ['Handling complex tasks', 'Improved accuracy', 'Generalization capabilities'], 'resources': []}, {'title': 'Challenges', 'description': 'Large language models also come with several challenges.', 'key_points': ['High computational requirements', 'Data bias and fairness issues', 'Interpretability and explainability'], 'resources': []}], 'examples': [], 'summary': 'Highlights the advantages and challenges of large language models.', 'importance': 8}, {'section_title': '2.1. Introduction to Model Pruning', 'topics': [{'title': 'Model Pruning', 'description': 'Model pruning is a technique for reducing the size of large language models.', 'key_points': ['Remove less important weights', 'Improve inference speed', 'Reduce memory footprint'], 'resources': []}], 'examples': [], 'summary': 'Explains the concept of model pruning and its importance in reducing the size of large language models.', 'importance': 8}, {'section_title': '2.2. Unstructured Pruning', 'topics': [{'title': 'Unstructured Pruning', 'description': 'Unstructured pruning removes individual weights from the model.', 'key_points': ['Iterative magnitude pruning', 'Lottery ticket hypothesis', 'Irregular sparsity patterns'], 'resources': []}], 'examples': [{'example_title': 'Iterative Magnitude Pruning', 'code_snippet': \"model.prune(pruning_method='iterative_magnitude', amount=0.1)\", 'explanation': 'Iteratively prunes the smallest weights until the desired sparsity is reached.', 'output': 'Pruned model with 10% sparsity', 'tags': ['unstructured pruning', 'iterative magnitude pruning']}], 'summary': 'Describes unstructured pruning, including its methods and applications.', 'importance': 7}, {'section_title': '2.3. Structured Pruning', 'topics': [{'title': 'Structured Pruning', 'description': 'Structured pruning removes entire structures, such as filters or channels.', 'key_points': ['Channel pruning', 'Filter pruning', 'Regular sparsity patterns'], 'resources': []}], 'examples': [{'example_title': 'Channel Pruning', 'code_snippet': \"model.prune(pruning_method='channel_pruning', amount=0.1)\", 'explanation': 'Prunes 10% of the channels in the model.', 'output': 'Pruned model with 10% fewer channels', 'tags': ['structured pruning', 'channel pruning']}], 'summary': 'Explains structured pruning, including its methods and applications.', 'importance': 7}, {'section_title': '2.4. Comparison of Pruning Methods', 'topics': [{'title': 'Comparison', 'description': 'Unstructured and structured pruning methods have different strengths and weaknesses.', 'key_points': ['Trade-off between sparsity and accuracy', 'Hardware compatibility', 'Implementation complexity'], 'resources': []}], 'examples': [], 'summary': 'Compares and contrasts different pruning methods, including their strengths and weaknesses.', 'importance': 6}, {'section_title': '3.1. Introduction to Layer Removal', 'topics': [{'title': 'Layer Removal', 'description': 'Layer removal is a technique for reducing the size of large language models.', 'key_points': ['Remove entire layers', 'Improve inference speed', 'Reduce memory footprint'], 'resources': []}], 'examples': [], 'summary': 'Explains the concept of layer removal and its significance in reducing the size of large language models.', 'importance': 7}, {'section_title': '3.2. Block Influence (BI) Metric', 'topics': [{'title': 'Block Influence (BI) Metric', 'description': 'The Block Influence (BI) metric measures the importance of a layer in a model.', 'key_points': ['Layer-wise importance score', 'Computationally efficient', 'Applied to layer removal'], 'resources': []}], 'examples': [], 'summary': 'Describes the Block Influence (BI) metric and its application in layer removal.', 'importance': 6}, {'section_title': '3.3. Layer Removal Method', 'topics': [{'title': 'Layer Removal Method', 'description': 'The layer removal method removes layers based on the Block Influence (BI) metric.', 'key_points': ['Greedy layer selection', 'Performance-preserving pruning', 'Applicable to various architectures'], 'resources': []}], 'examples': [], 'summary': 'Explains the layer removal method and its advantages and challenges.', 'importance': 6}, {'section_title': '4.1. Introduction to Quantization', 'topics': [{'title': 'Quantization', 'description': 'Quantization is a technique for reducing the size of large language models.', 'key_points': ['Reduce precision of weights', 'Improve inference speed', 'Reduce memory footprint'], 'resources': []}], 'examples': [], 'summary': 'Explains the concept of quantization and its importance in reducing the size of large language models.', 'importance': 7}, {'section_title': '4.2. Types of Quantization', 'topics': [{'title': 'Types of Quantization', 'description': 'There are different types of quantization based on the precision of weights.', 'key_points': ['Integer quantization', 'Ternary quantization', 'Mixed-precision quantization'], 'resources': []}], 'examples': [], 'summary': 'Discusses the different types of quantization.', 'importance': 6}, {'section_title': '4.3. Quantization Methods', 'topics': [{'title': 'Quantization Methods', 'description': 'There are different methods for quantizing large language models.', 'key_points': ['Linear quantization', 'Non-linear quantization', 'Post-training quantization'], 'resources': []}], 'examples': [], 'summary': 'Explains the different quantization methods.', 'importance': 6}, {'section_title': '5.1. Introduction to Model Compression', 'topics': [{'title': 'Model Compression', 'description': 'Model compression is a technique for reducing the size of large language models.', 'key_points': ['Combine multiple techniques', 'Improve inference speed', 'Reduce memory footprint'], 'resources': []}], 'examples': [], 'summary': 'Explains the concept of model compression and its significance in reducing the size of large language models.', 'importance': 8}, {'section_title': '5.2. Model Compression Techniques', 'topics': [{'title': 'Model Compression Techniques', 'description': 'Model compression combines multiple techniques for reducing the size of large language models.', 'key_points': ['Model pruning', 'Quantization', 'Knowledge distillation'], 'resources': []}], 'examples': [], 'summary': 'Discusses the different model compression techniques.', 'importance': 7}, {'section_title': '5.3. Comparison of Compression Techniques', 'topics': [{'title': 'Comparison', 'description': 'Model compression techniques have different strengths and weaknesses.', 'key_points': ['Trade-off between compression and accuracy', 'Hardware compatibility', 'Implementation complexity'], 'resources': []}], 'examples': [], 'summary': 'Compares and contrasts different compression techniques, including their strengths and weaknesses.', 'importance': 6}, {'section_title': '6.1. Popular Frameworks', 'topics': [{'title': 'Popular Frameworks', 'description': 'There are several popular frameworks for working with large language models.', 'key_points': ['TensorFlow', 'PyTorch', 'Hugging Face Transformers'], 'resources': []}], 'examples': [], 'summary': 'Discusses popular frameworks for large language models.', 'importance': 6}, {'section_title': '6.2. Model Pruning Tools', 'topics': [{'title': 'Model Pruning Tools', 'description': 'There are several tools for model pruning in large language models.', 'key_points': ['TensorFlow Model Optimization Toolkit', 'PyTorch Model Pruning'], 'resources': []}], 'examples': [], 'summary': 'Explains model pruning tools.', 'importance': 5}, {'section_title': '6.3. Quantization Tools', 'topics': [{'title': 'Quantization Tools', 'description': 'There are several tools for quantization in large language models.', 'key_points': ['TensorFlow Quantization', 'PyTorch Quantization'], 'resources': []}], 'examples': [], 'summary': 'Discusses quantization tools.', 'importance': 5}, {'section_title': '7.1. Best Practices for Model Pruning', 'topics': [{'title': 'Best Practices', 'description': 'There are several best practices for model pruning in large language models.', 'key_points': ['Select the right pruning method', 'Tune hyperparameters', 'Monitor performance'], 'resources': []}], 'examples': [], 'summary': 'Provides best practices for model pruning.', 'importance': 6}, {'section_title': '7.2. Best Practices for Quantization', 'topics': [{'title': 'Best Practices', 'description': 'There are several best practices for quantization in large language models.', 'key_points': ['Select the right quantization method', 'Tune hyperparameters', 'Monitor performance'], 'resources': []}], 'examples': [], 'summary': 'Explains best practices for quantization.', 'importance': 6}, {'section_title': '7.3. Future Directions', 'topics': [{'title': 'Future Directions', 'description': 'There are several future directions for large language models.', 'key_points': ['Develop more efficient pruning and quantization methods', 'Investigate hardware-aware compression techniques', 'Explore adaptive compression methods'], 'resources': []}], 'examples': [], 'summary': 'Discusses future directions for large language models.', 'importance': 6}], 'total_sections': 29, 'difficulty_level': 'Intermediate', 'usage_scenarios': ['Natural language processing tasks', 'Large language model development', 'Model optimization and compression']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cheat_sheet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKWbl4d2W1TP",
        "outputId": "325605b1-859c-465c-a4e4-dde45209dfe8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Create a Cheat Sheet for Large Language Models',\n",
              " 'description': 'This cheat sheet provides a comprehensive overview of large language models, including model pruning, quantization, and compression techniques. It also covers tools and frameworks, best practices, and future directions for large language models.',\n",
              " 'metadata': {'author': 'Advanced Assistant',\n",
              "  'version': '1.0',\n",
              "  'last_updated': '2023-03-15',\n",
              "  'created_date': '2023-03-01',\n",
              "  'tags': ['large language models',\n",
              "   'model pruning',\n",
              "   'quantization',\n",
              "   'compression'],\n",
              "  'intended_audience': 'Intermediate users, Experts',\n",
              "  'license': 'Creative Commons Attribution-NonCommercial 4.0 International'},\n",
              " 'sections': [{'section_title': '1.1. Definition and Overview',\n",
              "   'topics': [{'title': 'Definition',\n",
              "     'description': 'Large language models are artificial neural networks designed for natural language processing tasks.',\n",
              "     'key_points': ['Handle complex language tasks',\n",
              "      'Deep learning-based',\n",
              "      'Transformer-based or recurrent neural networks'],\n",
              "     'resources': ['Jurafsky, D., & Martin, J. H. (2020). Speech and Language Processing (3rd ed.). Pearson Education.']}],\n",
              "   'examples': [],\n",
              "   'summary': 'Defines large language models and their significance in natural language processing.',\n",
              "   'importance': 10},\n",
              "  {'section_title': '1.2. Types of Large Language Models',\n",
              "   'topics': [{'title': 'Transformer-based Models',\n",
              "     'description': 'Transformer-based models use self-attention mechanisms to process input sequences.',\n",
              "     'key_points': ['BERT, RoBERTa, and DistilBERT',\n",
              "      'Efficient parallel processing',\n",
              "      'Handling long-range dependencies'],\n",
              "     'resources': ['Vaswani, A., et al. (2017). Attention is All You Need. Advances in Neural Information Processing Systems.']},\n",
              "    {'title': 'Recurrent Neural Networks',\n",
              "     'description': 'Recurrent neural networks use recurrent connections to process input sequences.',\n",
              "     'key_points': ['LSTM and GRU',\n",
              "      'Sequential data processing',\n",
              "      'Limited parallelization'],\n",
              "     'resources': ['Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.']}],\n",
              "   'examples': [],\n",
              "   'summary': 'Discusses the different types of large language models, including transformer-based models and recurrent neural networks.',\n",
              "   'importance': 9},\n",
              "  {'section_title': '1.3. Advantages and Challenges',\n",
              "   'topics': [{'title': 'Advantages',\n",
              "     'description': 'Large language models offer several advantages in natural language processing tasks.',\n",
              "     'key_points': ['Handling complex tasks',\n",
              "      'Improved accuracy',\n",
              "      'Generalization capabilities'],\n",
              "     'resources': []},\n",
              "    {'title': 'Challenges',\n",
              "     'description': 'Large language models also come with several challenges.',\n",
              "     'key_points': ['High computational requirements',\n",
              "      'Data bias and fairness issues',\n",
              "      'Interpretability and explainability'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Highlights the advantages and challenges of large language models.',\n",
              "   'importance': 8},\n",
              "  {'section_title': '2.1. Introduction to Model Pruning',\n",
              "   'topics': [{'title': 'Model Pruning',\n",
              "     'description': 'Model pruning is a technique for reducing the size of large language models.',\n",
              "     'key_points': ['Remove less important weights',\n",
              "      'Improve inference speed',\n",
              "      'Reduce memory footprint'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Explains the concept of model pruning and its importance in reducing the size of large language models.',\n",
              "   'importance': 8},\n",
              "  {'section_title': '2.2. Unstructured Pruning',\n",
              "   'topics': [{'title': 'Unstructured Pruning',\n",
              "     'description': 'Unstructured pruning removes individual weights from the model.',\n",
              "     'key_points': ['Iterative magnitude pruning',\n",
              "      'Lottery ticket hypothesis',\n",
              "      'Irregular sparsity patterns'],\n",
              "     'resources': []}],\n",
              "   'examples': [{'example_title': 'Iterative Magnitude Pruning',\n",
              "     'code_snippet': \"model.prune(pruning_method='iterative_magnitude', amount=0.1)\",\n",
              "     'explanation': 'Iteratively prunes the smallest weights until the desired sparsity is reached.',\n",
              "     'output': 'Pruned model with 10% sparsity',\n",
              "     'tags': ['unstructured pruning', 'iterative magnitude pruning']}],\n",
              "   'summary': 'Describes unstructured pruning, including its methods and applications.',\n",
              "   'importance': 7},\n",
              "  {'section_title': '2.3. Structured Pruning',\n",
              "   'topics': [{'title': 'Structured Pruning',\n",
              "     'description': 'Structured pruning removes entire structures, such as filters or channels.',\n",
              "     'key_points': ['Channel pruning',\n",
              "      'Filter pruning',\n",
              "      'Regular sparsity patterns'],\n",
              "     'resources': []}],\n",
              "   'examples': [{'example_title': 'Channel Pruning',\n",
              "     'code_snippet': \"model.prune(pruning_method='channel_pruning', amount=0.1)\",\n",
              "     'explanation': 'Prunes 10% of the channels in the model.',\n",
              "     'output': 'Pruned model with 10% fewer channels',\n",
              "     'tags': ['structured pruning', 'channel pruning']}],\n",
              "   'summary': 'Explains structured pruning, including its methods and applications.',\n",
              "   'importance': 7},\n",
              "  {'section_title': '2.4. Comparison of Pruning Methods',\n",
              "   'topics': [{'title': 'Comparison',\n",
              "     'description': 'Unstructured and structured pruning methods have different strengths and weaknesses.',\n",
              "     'key_points': ['Trade-off between sparsity and accuracy',\n",
              "      'Hardware compatibility',\n",
              "      'Implementation complexity'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Compares and contrasts different pruning methods, including their strengths and weaknesses.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '3.1. Introduction to Layer Removal',\n",
              "   'topics': [{'title': 'Layer Removal',\n",
              "     'description': 'Layer removal is a technique for reducing the size of large language models.',\n",
              "     'key_points': ['Remove entire layers',\n",
              "      'Improve inference speed',\n",
              "      'Reduce memory footprint'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Explains the concept of layer removal and its significance in reducing the size of large language models.',\n",
              "   'importance': 7},\n",
              "  {'section_title': '3.2. Block Influence (BI) Metric',\n",
              "   'topics': [{'title': 'Block Influence (BI) Metric',\n",
              "     'description': 'The Block Influence (BI) metric measures the importance of a layer in a model.',\n",
              "     'key_points': ['Layer-wise importance score',\n",
              "      'Computationally efficient',\n",
              "      'Applied to layer removal'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Describes the Block Influence (BI) metric and its application in layer removal.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '3.3. Layer Removal Method',\n",
              "   'topics': [{'title': 'Layer Removal Method',\n",
              "     'description': 'The layer removal method removes layers based on the Block Influence (BI) metric.',\n",
              "     'key_points': ['Greedy layer selection',\n",
              "      'Performance-preserving pruning',\n",
              "      'Applicable to various architectures'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Explains the layer removal method and its advantages and challenges.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '4.1. Introduction to Quantization',\n",
              "   'topics': [{'title': 'Quantization',\n",
              "     'description': 'Quantization is a technique for reducing the size of large language models.',\n",
              "     'key_points': ['Reduce precision of weights',\n",
              "      'Improve inference speed',\n",
              "      'Reduce memory footprint'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Explains the concept of quantization and its importance in reducing the size of large language models.',\n",
              "   'importance': 7},\n",
              "  {'section_title': '4.2. Types of Quantization',\n",
              "   'topics': [{'title': 'Types of Quantization',\n",
              "     'description': 'There are different types of quantization based on the precision of weights.',\n",
              "     'key_points': ['Integer quantization',\n",
              "      'Ternary quantization',\n",
              "      'Mixed-precision quantization'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Discusses the different types of quantization.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '4.3. Quantization Methods',\n",
              "   'topics': [{'title': 'Quantization Methods',\n",
              "     'description': 'There are different methods for quantizing large language models.',\n",
              "     'key_points': ['Linear quantization',\n",
              "      'Non-linear quantization',\n",
              "      'Post-training quantization'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Explains the different quantization methods.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '5.1. Introduction to Model Compression',\n",
              "   'topics': [{'title': 'Model Compression',\n",
              "     'description': 'Model compression is a technique for reducing the size of large language models.',\n",
              "     'key_points': ['Combine multiple techniques',\n",
              "      'Improve inference speed',\n",
              "      'Reduce memory footprint'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Explains the concept of model compression and its significance in reducing the size of large language models.',\n",
              "   'importance': 8},\n",
              "  {'section_title': '5.2. Model Compression Techniques',\n",
              "   'topics': [{'title': 'Model Compression Techniques',\n",
              "     'description': 'Model compression combines multiple techniques for reducing the size of large language models.',\n",
              "     'key_points': ['Model pruning', 'Quantization', 'Knowledge distillation'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Discusses the different model compression techniques.',\n",
              "   'importance': 7},\n",
              "  {'section_title': '5.3. Comparison of Compression Techniques',\n",
              "   'topics': [{'title': 'Comparison',\n",
              "     'description': 'Model compression techniques have different strengths and weaknesses.',\n",
              "     'key_points': ['Trade-off between compression and accuracy',\n",
              "      'Hardware compatibility',\n",
              "      'Implementation complexity'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Compares and contrasts different compression techniques, including their strengths and weaknesses.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '6.1. Popular Frameworks',\n",
              "   'topics': [{'title': 'Popular Frameworks',\n",
              "     'description': 'There are several popular frameworks for working with large language models.',\n",
              "     'key_points': ['TensorFlow', 'PyTorch', 'Hugging Face Transformers'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Discusses popular frameworks for large language models.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '6.2. Model Pruning Tools',\n",
              "   'topics': [{'title': 'Model Pruning Tools',\n",
              "     'description': 'There are several tools for model pruning in large language models.',\n",
              "     'key_points': ['TensorFlow Model Optimization Toolkit',\n",
              "      'PyTorch Model Pruning'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Explains model pruning tools.',\n",
              "   'importance': 5},\n",
              "  {'section_title': '6.3. Quantization Tools',\n",
              "   'topics': [{'title': 'Quantization Tools',\n",
              "     'description': 'There are several tools for quantization in large language models.',\n",
              "     'key_points': ['TensorFlow Quantization', 'PyTorch Quantization'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Discusses quantization tools.',\n",
              "   'importance': 5},\n",
              "  {'section_title': '7.1. Best Practices for Model Pruning',\n",
              "   'topics': [{'title': 'Best Practices',\n",
              "     'description': 'There are several best practices for model pruning in large language models.',\n",
              "     'key_points': ['Select the right pruning method',\n",
              "      'Tune hyperparameters',\n",
              "      'Monitor performance'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Provides best practices for model pruning.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '7.2. Best Practices for Quantization',\n",
              "   'topics': [{'title': 'Best Practices',\n",
              "     'description': 'There are several best practices for quantization in large language models.',\n",
              "     'key_points': ['Select the right quantization method',\n",
              "      'Tune hyperparameters',\n",
              "      'Monitor performance'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Explains best practices for quantization.',\n",
              "   'importance': 6},\n",
              "  {'section_title': '7.3. Future Directions',\n",
              "   'topics': [{'title': 'Future Directions',\n",
              "     'description': 'There are several future directions for large language models.',\n",
              "     'key_points': ['Develop more efficient pruning and quantization methods',\n",
              "      'Investigate hardware-aware compression techniques',\n",
              "      'Explore adaptive compression methods'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'Discusses future directions for large language models.',\n",
              "   'importance': 6}],\n",
              " 'total_sections': 29,\n",
              " 'difficulty_level': 'Intermediate',\n",
              " 'usage_scenarios': ['Natural language processing tasks',\n",
              "  'Large language model development',\n",
              "  'Model optimization and compression']}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}