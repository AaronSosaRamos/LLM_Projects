{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4637b6f3c313482ea18ea542fed9be91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a553f7b2fe342fcb6d9c831897bd274",
              "IPY_MODEL_5560c4da332c46bb9cce1350a9874aca",
              "IPY_MODEL_0eacf69a27fd46e2aae7ebc92d149f29"
            ],
            "layout": "IPY_MODEL_92a64a48cf864c9fbee8834596ed1de1"
          }
        },
        "3a553f7b2fe342fcb6d9c831897bd274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2f2649d11149d495976401564dd228",
            "placeholder": "​",
            "style": "IPY_MODEL_9f6347d2e0494a3eaf3a0f788f654b72",
            "value": "tokenizer.json: 100%"
          }
        },
        "5560c4da332c46bb9cce1350a9874aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45324c5ce2da4a76ae48638533a28626",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c135af5e232d411094ac2858f90bdc6f",
            "value": 1355256
          }
        },
        "0eacf69a27fd46e2aae7ebc92d149f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4252921502814ed997c343724b4051f6",
            "placeholder": "​",
            "style": "IPY_MODEL_3922d29975c647b484a4b3e9a5b02d9c",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.77MB/s]"
          }
        },
        "92a64a48cf864c9fbee8834596ed1de1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d2f2649d11149d495976401564dd228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f6347d2e0494a3eaf3a0f788f654b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45324c5ce2da4a76ae48638533a28626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c135af5e232d411094ac2858f90bdc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4252921502814ed997c343724b4051f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3922d29975c647b484a4b3e9a5b02d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Chonkie Fundamentals\n",
        "Made by: Wilfredo Aaron Sosa Ramos"
      ],
      "metadata": {
        "id": "-L5k-YmTI3PJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeRCv2u_IjMQ",
        "outputId": "462a0d22-4c0a-4fe7-9b61-f3e636067798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q chonkie[all]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. TokenChunker"
      ],
      "metadata": {
        "id": "cHBHgLLFJHoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First import the chunker you want from Chonkie\n",
        "from chonkie import TokenChunker\n",
        "\n",
        "# Import your favorite tokenizer library\n",
        "# Also supports AutoTokenizers, TikToken and AutoTikTokenizer\n",
        "from tokenizers import Tokenizer\n",
        "tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Initialize the chunker\n",
        "chunker = TokenChunker(tokenizer)\n",
        "\n",
        "# Chunk some text\n",
        "chunks = chunker(\"Woah! Chonkie, the chunking library is so cool! I love the tiny hippo hehe.\")\n",
        "\n",
        "# Access chunks\n",
        "for chunk in chunks:\n",
        "    print(f\"Chunk: {chunk.text}\")\n",
        "    print(f\"Tokens: {chunk.token_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "4637b6f3c313482ea18ea542fed9be91",
            "3a553f7b2fe342fcb6d9c831897bd274",
            "5560c4da332c46bb9cce1350a9874aca",
            "0eacf69a27fd46e2aae7ebc92d149f29",
            "92a64a48cf864c9fbee8834596ed1de1",
            "4d2f2649d11149d495976401564dd228",
            "9f6347d2e0494a3eaf3a0f788f654b72",
            "45324c5ce2da4a76ae48638533a28626",
            "c135af5e232d411094ac2858f90bdc6f",
            "4252921502814ed997c343724b4051f6",
            "3922d29975c647b484a4b3e9a5b02d9c"
          ]
        },
        "id": "I3QcINnnI7qO",
        "outputId": "824e509e-ff90-454b-c30c-d335aaaeb36b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4637b6f3c313482ea18ea542fed9be91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk: Woah! Chonkie, the chunking library is so cool! I love the tiny hippo hehe.\n",
            "Tokens: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chonkie import TokenChunker\n",
        "\n",
        "# Basic initialization with default parameters\n",
        "chunker = TokenChunker(\n",
        "    tokenizer=\"gpt2\",  # Supports string identifiers\n",
        "    chunk_size=512,    # Maximum tokens per chunk\n",
        "    chunk_overlap=128  # Overlap between chunks\n",
        ")\n",
        "\n",
        "# Using a custom tokenizer\n",
        "from tokenizers import Tokenizer\n",
        "custom_tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
        "chunker = TokenChunker(\n",
        "    tokenizer=custom_tokenizer,\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=128\n",
        ")"
      ],
      "metadata": {
        "id": "3fL5qg7ZJYWa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Text Chunking"
      ],
      "metadata": {
        "id": "Ynf1MiKVLDJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "1. The Evolution of LLM Engineering\n",
        "LLM engineering is a cornerstone of modern artificial intelligence, enabling the development and optimization of large language models for diverse applications. These models, like GPT-4 or Gemini, are trained on massive datasets and possess extraordinary capabilities in natural language understanding and generation. However, their true utility lies in how they are engineered to interact with specific tasks and contexts. LLM engineering focuses on creating robust pipelines, integrating external tools, and refining user interactions to make models more effective, reliable, and adaptable. It also addresses challenges like hallucinations, bias, and efficiency, ensuring that these models deliver meaningful and accurate results across industries such as education, healthcare, and business.\n",
        "\n",
        "2. The Science of Prompt Design\n",
        "Central to LLM engineering is the art and science of prompt design—crafting the textual or programmatic inputs that guide the model’s behavior. Prompting techniques range from basic approaches, like zero-shot and few-shot prompting, to more advanced strategies, such as instruction tuning and dynamic prompt engineering. Zero-shot prompting involves giving the model a direct task with minimal context, while few-shot prompting provides examples to improve performance. Engineers often employ techniques like chaining prompts or incorporating contextual constraints to guide the model’s reasoning process. By mastering prompt design, LLM engineers can unlock the model's latent potential, ensuring it understands tasks deeply and produces high-quality outputs.\n",
        "\n",
        "3. Advanced Prompting Techniques and Applications\n",
        "Beyond basic prompting, advanced techniques enable LLMs to tackle complex, multi-step tasks effectively. For example, chain-of-thought prompting encourages models to explain their reasoning step by step, improving accuracy in tasks requiring logical deduction. Similarly, self-reflection prompts ask the model to evaluate its own responses, fostering iterative improvements. These techniques are particularly valuable in applications like legal research, scientific analysis, and technical writing, where precision and clarity are paramount. By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments.\n",
        "\n",
        "4. Integration with External Systems\n",
        "Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality. Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems. For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries. This seamless integration transforms LLMs into versatile agents capable of executing end-to-end workflows, enhancing their value in industries like finance, e-commerce, and AI-powered customer support.\n",
        "\n",
        "5. Ethical Considerations in LLM Engineering\n",
        "As LLMs become increasingly integrated into society, ethical considerations are a vital component of LLM engineering. Engineers must address issues such as bias, fairness, and privacy to ensure responsible AI deployment. Prompt design plays a significant role in mitigating biases by carefully framing tasks and curating training datasets. Moreover, engineers are tasked with implementing safeguards against misuse, such as restricting harmful outputs or ensuring compliance with data privacy regulations. Ethical LLM engineering not only protects users but also builds trust in AI systems, paving the way for their widespread acceptance and integration.\n",
        "\n",
        "6. The Future of LLM Engineering and Prompting\n",
        "The future of LLM engineering lies in the continued refinement of prompting techniques and the development of specialized models tailored to specific industries. Emerging trends include adaptive prompt generation, where AI dynamically modifies prompts based on user behavior, and multi-modal integration, allowing models to process and generate text, images, and other data types seamlessly. Additionally, advancements in reinforcement learning and fine-tuning will enable LLMs to learn from user feedback, improving their performance over time. As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society, driving innovation while upholding ethical and technical standards.\n",
        "\"\"\"\n",
        "chunks = chunker.chunk(text)\n",
        "\n",
        "for chunk in chunks:\n",
        "    print(f\"Chunk text: {chunk.text}\")\n",
        "    print(f\"Token count: {chunk.token_count}\")\n",
        "    print(f\"Start index: {chunk.start_index}\")\n",
        "    print(f\"End index: {chunk.end_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSyjv2NMJs5_",
        "outputId": "0d6856c1-2cd9-43de-e0af-6f8ad128d096"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk text: \n",
            "1. The Evolution of LLM Engineering\n",
            "LLM engineering is a cornerstone of modern artificial intelligence, enabling the development and optimization of large language models for diverse applications. These models, like GPT-4 or Gemini, are trained on massive datasets and possess extraordinary capabilities in natural language understanding and generation. However, their true utility lies in how they are engineered to interact with specific tasks and contexts. LLM engineering focuses on creating robust pipelines, integrating external tools, and refining user interactions to make models more effective, reliable, and adaptable. It also addresses challenges like hallucinations, bias, and efficiency, ensuring that these models deliver meaningful and accurate results across industries such as education, healthcare, and business.\n",
            "\n",
            "2. The Science of Prompt Design\n",
            "Central to LLM engineering is the art and science of prompt design—crafting the textual or programmatic inputs that guide the model’s behavior. Prompting techniques range from basic approaches, like zero-shot and few-shot prompting, to more advanced strategies, such as instruction tuning and dynamic prompt engineering. Zero-shot prompting involves giving the model a direct task with minimal context, while few-shot prompting provides examples to improve performance. Engineers often employ techniques like chaining prompts or incorporating contextual constraints to guide the model’s reasoning process. By mastering prompt design, LLM engineers can unlock the model's latent potential, ensuring it understands tasks deeply and produces high-quality outputs.\n",
            "\n",
            "3. Advanced Prompting Techniques and Applications\n",
            "Beyond basic prompting, advanced techniques enable LLMs to tackle complex, multi-step tasks effectively. For example, chain-of-thought prompting encourages models to explain their reasoning step by step, improving accuracy in tasks requiring logical deduction. Similarly, self-reflection prompts ask the model to evaluate its own responses, fostering iterative improvements. These techniques are particularly valuable in applications like legal research, scientific analysis, and technical writing, where precision and clarity are paramount. By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments.\n",
            "\n",
            "4. Integration with External Systems\n",
            "Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality. Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems. For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries. This seamless integration transforms LLMs into\n",
            "Token count: 512\n",
            "Start index: 0\n",
            "End index: 2904\n",
            "Chunk text:  writing, where precision and clarity are paramount. By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments.\n",
            "\n",
            "4. Integration with External Systems\n",
            "Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality. Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems. For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries. This seamless integration transforms LLMs into versatile agents capable of executing end-to-end workflows, enhancing their value in industries like finance, e-commerce, and AI-powered customer support.\n",
            "\n",
            "5. Ethical Considerations in LLM Engineering\n",
            "As LLMs become increasingly integrated into society, ethical considerations are a vital component of LLM engineering. Engineers must address issues such as bias, fairness, and privacy to ensure responsible AI deployment. Prompt design plays a significant role in mitigating biases by carefully framing tasks and curating training datasets. Moreover, engineers are tasked with implementing safeguards against misuse, such as restricting harmful outputs or ensuring compliance with data privacy regulations. Ethical LLM engineering not only protects users but also builds trust in AI systems, paving the way for their widespread acceptance and integration.\n",
            "\n",
            "6. The Future of LLM Engineering and Prompting\n",
            "The future of LLM engineering lies in the continued refinement of prompting techniques and the development of specialized models tailored to specific industries. Emerging trends include adaptive prompt generation, where AI dynamically modifies prompts based on user behavior, and multi-modal integration, allowing models to process and generate text, images, and other data types seamlessly. Additionally, advancements in reinforcement learning and fine-tuning will enable LLMs to learn from user feedback, improving their performance over time. As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society, driving innovation while upholding ethical and technical standards.\n",
            "\n",
            "Token count: 421\n",
            "Start index: -1\n",
            "End index: 2368\n",
            "Chunk text:  performance over time. As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society, driving innovation while upholding ethical and technical standards.\n",
            "\n",
            "Token count: 37\n",
            "Start index: 4331\n",
            "End index: 4535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Chunking"
      ],
      "metadata": {
        "id": "eUiGxPsCLHw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The Evolution of LLM Engineering\n",
        "evolution_of_llm_engineering = [\n",
        "    \"LLM engineering is a cornerstone of modern artificial intelligence, enabling the development and optimization of large language models for diverse applications.\",\n",
        "    \"These models, like GPT-4 or Gemini, are trained on massive datasets and possess extraordinary capabilities in natural language understanding and generation.\",\n",
        "    \"However, their true utility lies in how they are engineered to interact with specific tasks and contexts.\",\n",
        "    \"LLM engineering focuses on creating robust pipelines, integrating external tools, and refining user interactions to make models more effective, reliable, and adaptable.\",\n",
        "    \"It also addresses challenges like hallucinations, bias, and efficiency, ensuring that these models deliver meaningful and accurate results across industries such as education, healthcare, and business.\"\n",
        "]\n",
        "\n",
        "# 2. The Science of Prompt Design\n",
        "science_of_prompt_design = [\n",
        "    \"Central to LLM engineering is the art and science of prompt design—crafting the textual or programmatic inputs that guide the model’s behavior.\",\n",
        "    \"Prompting techniques range from basic approaches, like zero-shot and few-shot prompting, to more advanced strategies, such as instruction tuning and dynamic prompt engineering.\",\n",
        "    \"Zero-shot prompting involves giving the model a direct task with minimal context, while few-shot prompting provides examples to improve performance.\",\n",
        "    \"Engineers often employ techniques like chaining prompts or incorporating contextual constraints to guide the model’s reasoning process.\",\n",
        "    \"By mastering prompt design, LLM engineers can unlock the model's latent potential, ensuring it understands tasks deeply and produces high-quality outputs.\"\n",
        "]\n",
        "\n",
        "# 3. Advanced Prompting Techniques and Applications\n",
        "advanced_prompting_techniques = [\n",
        "    \"Beyond basic prompting, advanced techniques enable LLMs to tackle complex, multi-step tasks effectively.\",\n",
        "    \"For example, chain-of-thought prompting encourages models to explain their reasoning step by step, improving accuracy in tasks requiring logical deduction.\",\n",
        "    \"Similarly, self-reflection prompts ask the model to evaluate its own responses, fostering iterative improvements.\",\n",
        "    \"These techniques are particularly valuable in applications like legal research, scientific analysis, and technical writing, where precision and clarity are paramount.\",\n",
        "    \"By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments.\"\n",
        "]\n",
        "\n",
        "# 4. Integration with External Systems\n",
        "integration_with_external_systems = [\n",
        "    \"Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality.\",\n",
        "    \"Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems.\",\n",
        "    \"For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries.\",\n",
        "    \"This seamless integration transforms LLMs into versatile agents capable of executing end-to-end workflows.\",\n",
        "    \"It enhances their value in industries like finance, e-commerce, and AI-powered customer support.\"\n",
        "]\n",
        "\n",
        "# 5. Ethical Considerations in LLM Engineering\n",
        "ethical_considerations = [\n",
        "    \"As LLMs become increasingly integrated into society, ethical considerations are a vital component of LLM engineering.\",\n",
        "    \"Engineers must address issues such as bias, fairness, and privacy to ensure responsible AI deployment.\",\n",
        "    \"Prompt design plays a significant role in mitigating biases by carefully framing tasks and curating training datasets.\",\n",
        "    \"Moreover, engineers are tasked with implementing safeguards against misuse, such as restricting harmful outputs or ensuring compliance with data privacy regulations.\",\n",
        "    \"Ethical LLM engineering not only protects users but also builds trust in AI systems, paving the way for their widespread acceptance and integration.\"\n",
        "]\n",
        "\n",
        "# 6. The Future of LLM Engineering and Prompting\n",
        "future_of_llm_engineering = [\n",
        "    \"The future of LLM engineering lies in the continued refinement of prompting techniques and the development of specialized models tailored to specific industries.\",\n",
        "    \"Emerging trends include adaptive prompt generation, where AI dynamically modifies prompts based on user behavior, and multi-modal integration, allowing models to process and generate text, images, and other data types seamlessly.\",\n",
        "    \"Additionally, advancements in reinforcement learning and fine-tuning will enable LLMs to learn from user feedback, improving their performance over time.\",\n",
        "    \"As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society.\",\n",
        "    \"They will drive innovation while upholding ethical and technical standards.\"\n",
        "]\n",
        "texts = evolution_of_llm_engineering + science_of_prompt_design + advanced_prompting_techniques + integration_with_external_systems + ethical_considerations + future_of_llm_engineering"
      ],
      "metadata": {
        "id": "fOUxLvuyK1CE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_chunks = chunker.chunk_batch(texts)\n",
        "\n",
        "for doc_chunks in batch_chunks:\n",
        "    for chunk in doc_chunks:\n",
        "        print(f\"Chunk: {chunk.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuMbBgSuKzPR",
        "outputId": "3d30b49b-7adf-423d-985a-1278debfde1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk: LLM engineering is a cornerstone of modern artificial intelligence, enabling the development and optimization of large language models for diverse applications.\n",
            "Chunk: These models, like GPT-4 or Gemini, are trained on massive datasets and possess extraordinary capabilities in natural language understanding and generation.\n",
            "Chunk: However, their true utility lies in how they are engineered to interact with specific tasks and contexts.\n",
            "Chunk: LLM engineering focuses on creating robust pipelines, integrating external tools, and refining user interactions to make models more effective, reliable, and adaptable.\n",
            "Chunk: It also addresses challenges like hallucinations, bias, and efficiency, ensuring that these models deliver meaningful and accurate results across industries such as education, healthcare, and business.\n",
            "Chunk: Central to LLM engineering is the art and science of prompt design—crafting the textual or programmatic inputs that guide the model’s behavior.\n",
            "Chunk: Prompting techniques range from basic approaches, like zero-shot and few-shot prompting, to more advanced strategies, such as instruction tuning and dynamic prompt engineering.\n",
            "Chunk: Zero-shot prompting involves giving the model a direct task with minimal context, while few-shot prompting provides examples to improve performance.\n",
            "Chunk: Engineers often employ techniques like chaining prompts or incorporating contextual constraints to guide the model’s reasoning process.\n",
            "Chunk: By mastering prompt design, LLM engineers can unlock the model's latent potential, ensuring it understands tasks deeply and produces high-quality outputs.\n",
            "Chunk: Beyond basic prompting, advanced techniques enable LLMs to tackle complex, multi-step tasks effectively.\n",
            "Chunk: For example, chain-of-thought prompting encourages models to explain their reasoning step by step, improving accuracy in tasks requiring logical deduction.\n",
            "Chunk: Similarly, self-reflection prompts ask the model to evaluate its own responses, fostering iterative improvements.\n",
            "Chunk: These techniques are particularly valuable in applications like legal research, scientific analysis, and technical writing, where precision and clarity are paramount.\n",
            "Chunk: By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments.\n",
            "Chunk: Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality.\n",
            "Chunk: Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems.\n",
            "Chunk: For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries.\n",
            "Chunk: This seamless integration transforms LLMs into versatile agents capable of executing end-to-end workflows.\n",
            "Chunk: It enhances their value in industries like finance, e-commerce, and AI-powered customer support.\n",
            "Chunk: As LLMs become increasingly integrated into society, ethical considerations are a vital component of LLM engineering.\n",
            "Chunk: Engineers must address issues such as bias, fairness, and privacy to ensure responsible AI deployment.\n",
            "Chunk: Prompt design plays a significant role in mitigating biases by carefully framing tasks and curating training datasets.\n",
            "Chunk: Moreover, engineers are tasked with implementing safeguards against misuse, such as restricting harmful outputs or ensuring compliance with data privacy regulations.\n",
            "Chunk: Ethical LLM engineering not only protects users but also builds trust in AI systems, paving the way for their widespread acceptance and integration.\n",
            "Chunk: The future of LLM engineering lies in the continued refinement of prompting techniques and the development of specialized models tailored to specific industries.\n",
            "Chunk: Emerging trends include adaptive prompt generation, where AI dynamically modifies prompts based on user behavior, and multi-modal integration, allowing models to process and generate text, images, and other data types seamlessly.\n",
            "Chunk: Additionally, advancements in reinforcement learning and fine-tuning will enable LLMs to learn from user feedback, improving their performance over time.\n",
            "Chunk: As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society.\n",
            "Chunk: They will drive innovation while upholding ethical and technical standards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using as a Callable"
      ],
      "metadata": {
        "id": "hVGr4iSlLLX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single text\n",
        "chunks = chunker(\"\"\"\n",
        "1. The Evolution of LLM Engineering\n",
        "LLM engineering is a cornerstone of modern artificial intelligence, enabling the development and optimization of large language models for diverse applications. These models, like GPT-4 or Gemini, are trained on massive datasets and possess extraordinary capabilities in natural language understanding and generation. However, their true utility lies in how they are engineered to interact with specific tasks and contexts. LLM engineering focuses on creating robust pipelines, integrating external tools, and refining user interactions to make models more effective, reliable, and adaptable. It also addresses challenges like hallucinations, bias, and efficiency, ensuring that these models deliver meaningful and accurate results across industries such as education, healthcare, and business.\n",
        "\n",
        "2. The Science of Prompt Design\n",
        "Central to LLM engineering is the art and science of prompt design—crafting the textual or programmatic inputs that guide the model’s behavior. Prompting techniques range from basic approaches, like zero-shot and few-shot prompting, to more advanced strategies, such as instruction tuning and dynamic prompt engineering. Zero-shot prompting involves giving the model a direct task with minimal context, while few-shot prompting provides examples to improve performance. Engineers often employ techniques like chaining prompts or incorporating contextual constraints to guide the model’s reasoning process. By mastering prompt design, LLM engineers can unlock the model's latent potential, ensuring it understands tasks deeply and produces high-quality outputs.\n",
        "\n",
        "3. Advanced Prompting Techniques and Applications\n",
        "Beyond basic prompting, advanced techniques enable LLMs to tackle complex, multi-step tasks effectively. For example, chain-of-thought prompting encourages models to explain their reasoning step by step, improving accuracy in tasks requiring logical deduction. Similarly, self-reflection prompts ask the model to evaluate its own responses, fostering iterative improvements. These techniques are particularly valuable in applications like legal research, scientific analysis, and technical writing, where precision and clarity are paramount. By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments.\n",
        "\n",
        "4. Integration with External Systems\n",
        "Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality. Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems. For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries. This seamless integration transforms LLMs into versatile agents capable of executing end-to-end workflows, enhancing their value in industries like finance, e-commerce, and AI-powered customer support.\n",
        "\n",
        "5. Ethical Considerations in LLM Engineering\n",
        "As LLMs become increasingly integrated into society, ethical considerations are a vital component of LLM engineering. Engineers must address issues such as bias, fairness, and privacy to ensure responsible AI deployment. Prompt design plays a significant role in mitigating biases by carefully framing tasks and curating training datasets. Moreover, engineers are tasked with implementing safeguards against misuse, such as restricting harmful outputs or ensuring compliance with data privacy regulations. Ethical LLM engineering not only protects users but also builds trust in AI systems, paving the way for their widespread acceptance and integration.\n",
        "\n",
        "6. The Future of LLM Engineering and Prompting\n",
        "The future of LLM engineering lies in the continued refinement of prompting techniques and the development of specialized models tailored to specific industries. Emerging trends include adaptive prompt generation, where AI dynamically modifies prompts based on user behavior, and multi-modal integration, allowing models to process and generate text, images, and other data types seamlessly. Additionally, advancements in reinforcement learning and fine-tuning will enable LLMs to learn from user feedback, improving their performance over time. As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society, driving innovation while upholding ethical and technical standards.\n",
        "\"\"\")\n",
        "\n",
        "# Multiple texts\n",
        "batch_chunks = chunker(texts)"
      ],
      "metadata": {
        "id": "5uRg_GA3LL6K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgi2_cgiLVSz",
        "outputId": "603adce8-f0a7-4c30-d610-338fc6093db9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Chunk(text= \n",
              " 1. The Evolution of LLM Engineering\n",
              " LLM engineering is a cornerstone of modern artificial intelligence, enabling the development and optimization of large language models for diverse applications. These models, like GPT-4 or Gemini, are trained on massive datasets and possess extraordinary capabilities in natural language understanding and generation. However, their true utility lies in how they are engineered to interact with specific tasks and contexts. LLM engineering focuses on creating robust pipelines, integrating external tools, and refining user interactions to make models more effective, reliable, and adaptable. It also addresses challenges like hallucinations, bias, and efficiency, ensuring that these models deliver meaningful and accurate results across industries such as education, healthcare, and business.\n",
              " \n",
              " 2. The Science of Prompt Design\n",
              " Central to LLM engineering is the art and science of prompt design—crafting the textual or programmatic inputs that guide the model’s behavior. Prompting techniques range from basic approaches, like zero-shot and few-shot prompting, to more advanced strategies, such as instruction tuning and dynamic prompt engineering. Zero-shot prompting involves giving the model a direct task with minimal context, while few-shot prompting provides examples to improve performance. Engineers often employ techniques like chaining prompts or incorporating contextual constraints to guide the model’s reasoning process. By mastering prompt design, LLM engineers can unlock the model's latent potential, ensuring it understands tasks deeply and produces high-quality outputs.\n",
              " \n",
              " 3. Advanced Prompting Techniques and Applications\n",
              " Beyond basic prompting, advanced techniques enable LLMs to tackle complex, multi-step tasks effectively. For example, chain-of-thought prompting encourages models to explain their reasoning step by step, improving accuracy in tasks requiring logical deduction. Similarly, self-reflection prompts ask the model to evaluate its own responses, fostering iterative improvements. These techniques are particularly valuable in applications like legal research, scientific analysis, and technical writing, where precision and clarity are paramount. By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments.\n",
              " \n",
              " 4. Integration with External Systems\n",
              " Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality. Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems. For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries. This seamless integration transforms LLMs, start_index=0, end_index=2900, token_count=512, context=None),\n",
              " Chunk(text= technical writing, where precision and clarity are paramount. By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments.\n",
              " \n",
              " 4. Integration with External Systems\n",
              " Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality. Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems. For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries. This seamless integration transforms LLMs into versatile agents capable of executing end-to-end workflows, enhancing their value in industries like finance, e-commerce, and AI-powered customer support.\n",
              " \n",
              " 5. Ethical Considerations in LLM Engineering\n",
              " As LLMs become increasingly integrated into society, ethical considerations are a vital component of LLM engineering. Engineers must address issues such as bias, fairness, and privacy to ensure responsible AI deployment. Prompt design plays a significant role in mitigating biases by carefully framing tasks and curating training datasets. Moreover, engineers are tasked with implementing safeguards against misuse, such as restricting harmful outputs or ensuring compliance with data privacy regulations. Ethical LLM engineering not only protects users but also builds trust in AI systems, paving the way for their widespread acceptance and integration.\n",
              " \n",
              " 6. The Future of LLM Engineering and Prompting\n",
              " The future of LLM engineering lies in the continued refinement of prompting techniques and the development of specialized models tailored to specific industries. Emerging trends include adaptive prompt generation, where AI dynamically modifies prompts based on user behavior, and multi-modal integration, allowing models to process and generate text, images, and other data types seamlessly. Additionally, advancements in reinforcement learning and fine-tuning will enable LLMs to learn from user feedback, improving their performance over time. As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society, driving innovation while upholding ethical and technical standards.\n",
              " , start_index=-1, end_index=2378, token_count=422, context=None),\n",
              " Chunk(text= their performance over time. As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society, driving innovation while upholding ethical and technical standards.\n",
              " , start_index=4326, end_index=4536, token_count=38, context=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhBE7kItLWsG",
        "outputId": "83bddaca-2d8c-4e17-9fe8-4149093c71a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Chunk(text=LLM engineering is a cornerstone of modern artificial intelligence, enabling the development and optimization of large language models for diverse applications., start_index=0, end_index=160, token_count=24, context=None)],\n",
              " [Chunk(text=These models, like GPT-4 or Gemini, are trained on massive datasets and possess extraordinary capabilities in natural language understanding and generation., start_index=0, end_index=156, token_count=27, context=None)],\n",
              " [Chunk(text=However, their true utility lies in how they are engineered to interact with specific tasks and contexts., start_index=0, end_index=105, token_count=19, context=None)],\n",
              " [Chunk(text=LLM engineering focuses on creating robust pipelines, integrating external tools, and refining user interactions to make models more effective, reliable, and adaptable., start_index=0, end_index=168, token_count=29, context=None)],\n",
              " [Chunk(text=It also addresses challenges like hallucinations, bias, and efficiency, ensuring that these models deliver meaningful and accurate results across industries such as education, healthcare, and business., start_index=0, end_index=201, token_count=32, context=None)],\n",
              " [Chunk(text=Central to LLM engineering is the art and science of prompt design—crafting the textual or programmatic inputs that guide the model’s behavior., start_index=0, end_index=143, token_count=31, context=None)],\n",
              " [Chunk(text=Prompting techniques range from basic approaches, like zero-shot and few-shot prompting, to more advanced strategies, such as instruction tuning and dynamic prompt engineering., start_index=0, end_index=176, token_count=33, context=None)],\n",
              " [Chunk(text=Zero-shot prompting involves giving the model a direct task with minimal context, while few-shot prompting provides examples to improve performance., start_index=0, end_index=148, token_count=26, context=None)],\n",
              " [Chunk(text=Engineers often employ techniques like chaining prompts or incorporating contextual constraints to guide the model’s reasoning process., start_index=0, end_index=135, token_count=22, context=None)],\n",
              " [Chunk(text=By mastering prompt design, LLM engineers can unlock the model's latent potential, ensuring it understands tasks deeply and produces high-quality outputs., start_index=0, end_index=154, token_count=28, context=None)],\n",
              " [Chunk(text=Beyond basic prompting, advanced techniques enable LLMs to tackle complex, multi-step tasks effectively., start_index=0, end_index=104, token_count=19, context=None)],\n",
              " [Chunk(text=For example, chain-of-thought prompting encourages models to explain their reasoning step by step, improving accuracy in tasks requiring logical deduction., start_index=0, end_index=155, token_count=27, context=None)],\n",
              " [Chunk(text=Similarly, self-reflection prompts ask the model to evaluate its own responses, fostering iterative improvements., start_index=0, end_index=113, token_count=21, context=None)],\n",
              " [Chunk(text=These techniques are particularly valuable in applications like legal research, scientific analysis, and technical writing, where precision and clarity are paramount., start_index=0, end_index=166, token_count=25, context=None)],\n",
              " [Chunk(text=By leveraging such methods, LLM engineers can adapt models to solve intricate problems, making them indispensable tools in high-stakes environments., start_index=0, end_index=148, token_count=26, context=None)],\n",
              " [Chunk(text=Another critical aspect of LLM engineering is integrating models with external tools and APIs to expand their functionality., start_index=0, end_index=124, token_count=20, context=None)],\n",
              " [Chunk(text=Through techniques like tool usage prompting and plugin development, LLMs can perform tasks beyond text generation, such as retrieving real-time data, running calculations, or interacting with software systems., start_index=0, end_index=210, token_count=36, context=None)],\n",
              " [Chunk(text=For example, an LLM integrated with a database API can provide detailed analytics based on structured data queries., start_index=0, end_index=115, token_count=21, context=None)],\n",
              " [Chunk(text=This seamless integration transforms LLMs into versatile agents capable of executing end-to-end workflows., start_index=0, end_index=106, token_count=20, context=None)],\n",
              " [Chunk(text=It enhances their value in industries like finance, e-commerce, and AI-powered customer support., start_index=0, end_index=96, token_count=20, context=None)],\n",
              " [Chunk(text=As LLMs become increasingly integrated into society, ethical considerations are a vital component of LLM engineering., start_index=0, end_index=117, token_count=20, context=None)],\n",
              " [Chunk(text=Engineers must address issues such as bias, fairness, and privacy to ensure responsible AI deployment., start_index=0, end_index=102, token_count=18, context=None)],\n",
              " [Chunk(text=Prompt design plays a significant role in mitigating biases by carefully framing tasks and curating training datasets., start_index=0, end_index=118, token_count=20, context=None)],\n",
              " [Chunk(text=Moreover, engineers are tasked with implementing safeguards against misuse, such as restricting harmful outputs or ensuring compliance with data privacy regulations., start_index=0, end_index=165, token_count=24, context=None)],\n",
              " [Chunk(text=Ethical LLM engineering not only protects users but also builds trust in AI systems, paving the way for their widespread acceptance and integration., start_index=0, end_index=148, token_count=27, context=None)],\n",
              " [Chunk(text=The future of LLM engineering lies in the continued refinement of prompting techniques and the development of specialized models tailored to specific industries., start_index=0, end_index=161, token_count=25, context=None)],\n",
              " [Chunk(text=Emerging trends include adaptive prompt generation, where AI dynamically modifies prompts based on user behavior, and multi-modal integration, allowing models to process and generate text, images, and other data types seamlessly., start_index=0, end_index=229, token_count=42, context=None)],\n",
              " [Chunk(text=Additionally, advancements in reinforcement learning and fine-tuning will enable LLMs to learn from user feedback, improving their performance over time., start_index=0, end_index=153, token_count=27, context=None)],\n",
              " [Chunk(text=As the field evolves, LLM engineers and researchers will play a crucial role in shaping AI's impact on society., start_index=0, end_index=111, token_count=23, context=None)],\n",
              " [Chunk(text=They will drive innovation while upholding ethical and technical standards., start_index=0, end_index=75, token_count=11, context=None)]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Recommended: Tiktoken"
      ],
      "metadata": {
        "id": "F0KUvjosLvNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "NIzjNCfJLrDe"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}