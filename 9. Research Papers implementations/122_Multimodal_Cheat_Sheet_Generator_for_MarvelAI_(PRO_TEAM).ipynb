{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Multimodal Cheat Sheet Generator with Open Source LLMs and Cohere (PRO TEAM)\n",
        "Made by: Wilfredo Aaron Sosa Ramos (AI Lab Manager at RealityAI Labs)"
      ],
      "metadata": {
        "id": "0hliAYZ9IgMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install the dependencies:"
      ],
      "metadata": {
        "id": "fhJTav1rJBlW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qes-V0ndIVn0",
        "outputId": "b4554821-20ee-46a2-9859-c3786cbf2e4d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_core langchain_community chroma langchain_chroma langchain_groq langchain_cohere langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf assemblyai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sS3W24CL6Fh",
        "outputId": "b9409624-d5f1-46cc-d591-519582fc9c81",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m256.0/298.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Set env. variables"
      ],
      "metadata": {
        "id": "BZcfQS2kJa2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['ASSEMBLYAI_API_KEY'] = userdata.get('ASSEMBLYAI_API_KEY')\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "os.environ['COHERE_API_KEY'] = userdata.get('COHERE_API_KEY')"
      ],
      "metadata": {
        "id": "EwFyHymmJcUJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Create vector store with PDF files, Images and Audio (MULTIMODAL SUPPORT)"
      ],
      "metadata": {
        "id": "iQDerhslJj9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1. Create File Handler"
      ],
      "metadata": {
        "id": "4_mVm7y1KubW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import uuid\n",
        "import requests\n",
        "\n",
        "class FileHandler:\n",
        "    def __init__(self, file_loader, file_extension):\n",
        "        self.file_loader = file_loader\n",
        "        self.file_extension = file_extension\n",
        "\n",
        "    def load(self, url):\n",
        "        # Generate a unique filename with a UUID prefix\n",
        "        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n",
        "\n",
        "        try:\n",
        "            # Download the file from the URL and save it to a temporary file\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "\n",
        "            with tempfile.NamedTemporaryFile(delete=False, prefix=unique_filename) as temp_file:\n",
        "                temp_file.write(response.content)\n",
        "                temp_file_path = temp_file.name\n",
        "\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            raise Exception(f\"Failed to download file from URL\", url) from req_err\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to handle file download\", url) from e\n",
        "\n",
        "        # Use the file_loader to load the documents\n",
        "        try:\n",
        "            loader = self.file_loader(file_path=temp_file_path)\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"No file found\", temp_file_path) from e\n",
        "\n",
        "        try:\n",
        "            documents = loader.load()\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"No file content available\", temp_file_path) from e\n",
        "\n",
        "        # Remove the temporary file\n",
        "        os.remove(temp_file_path)\n",
        "\n",
        "        return documents"
      ],
      "metadata": {
        "id": "gbEnrRz5KuIk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2. Create PDF document loader util function"
      ],
      "metadata": {
        "id": "hD7edAx-K_VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 100\n",
        ")\n",
        "\n",
        "def load_pdf_documents(pdf_url: str):\n",
        "    pdf_loader = FileHandler(PyPDFLoader, \"pdf\")\n",
        "    docs = pdf_loader.load(pdf_url)\n",
        "\n",
        "    if docs:\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        return split_docs"
      ],
      "metadata": {
        "id": "7iUNN3HNLDO0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_docs = load_pdf_documents(\"https://arxiv.org/pdf/2403.03853\")"
      ],
      "metadata": {
        "id": "AjeoXDomLpB5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS6xYNxfLxmM",
        "outputId": "8c8bca66-45ef-46b9-de57-77718eb4bc2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 0}, page_content='ShortGPT: Layers in Large Language Models are More Redun-\\ndant Than You Expect\\nXin Men∗\\nBaichuan Inc.\\nMingyu Xu∗\\nBaichuan Inc.\\nQingyu Zhang∗\\nISCAS\\nBingning Wang †\\nBaichuan Inc.\\nHongyu Lin\\nISCAS\\nYaojie Lu\\nISCAS\\nXianpei Han\\nISCAS\\nWeipeng Chen\\nBaichuan Inc.\\nAbstract\\nAs Large Language Models (LLMs) continue to advance in performance,\\ntheir size has increased significantly, with current LLMs containing billions\\nor even trillions of parameters. In this study, we identify notable redun-\\ndancy across the layers of LLMs, where some layers contribute minimally\\nto overall network functionality. To quantify this, we introduce a metric\\ncalled Block Influence (BI) which use the similarity between layer’s input\\nand output to measure the importance of each layer. Based on the observa-\\ntion of layer redundancy, we propose a straightforward pruning method:\\nlayer removal, which eliminates redundant layers based on their BI scores.\\nOur approach, termed ShortGPT, demonstrates superior performance over'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 0}, page_content='Our approach, termed ShortGPT, demonstrates superior performance over\\nprevious state-of-the-art pruning methods. Moreover, ShortGPT is orthogo-\\nnal to quantization-like methods, enabling further reduction in parameters\\nand computation. The ability to achieve better results through simple layer\\nremoval, as opposed to more complex pruning techniques, suggests a high\\ndegree of redundancy across layers, not only in transformer models but also\\nin non-transformer models. We hope this work will contribute to future\\nresearch in LLM compression.\\n1 Introduction\\nThe field of large language models (LLMs) has witnessed rapid development recently, with\\nLLMs achieving impressive performance across various domains. Guided by the scaling\\nlaws identified in prior work (Kaplan et al., 2020; Hoffmann et al., 2022), current LLM\\nresearch tend to increase model parameters to boost performance. As a result, modern\\nLLMs, which can comprise billions to trillions of parameters, require significant hardware'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 0}, page_content='LLMs, which can comprise billions to trillions of parameters, require significant hardware\\nresources for deployment, creating substantial barriers to their practical use.\\nTo mitigate the hardware demands of large models, model compression techniques have\\nbecome a critical area of focus (Zhu et al., 2023). These techniques are generally divided\\ninto quantization (Liu et al., 2021; Gholami et al., 2022; Dettmers et al., 2022; 2024) and\\npruning(LeCun et al., 1989; Han et al., 2015; Frantar & Alistarh, 2023). Quantization reduces\\nthe precision of model parameters, but its effectiveness often requires specific hardware\\nsupport. In contrast, pruning method removes redundant parameters to decrease the\\nmodel’s size and computation, offering a more flexible and hardware-agnostic approach.\\nDespite its advantages, many existing pruning methods are complex; for example, some\\nrequire gradient information (Ma et al., 2024), which limits their practicality.'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 0}, page_content='require gradient information (Ma et al., 2024), which limits their practicality.\\nIn this paper, we focus on the issue of layer redundancy in LLMs and propose a novel\\napproach for simplifying these models. We introduce Block Influence (BI), a metric that\\nquantifies how much the hidden state changes after passing through each layer, providing a\\nmore direct measure of a layer’s importance. Leveraging this insight, we propose a simple\\n∗Equal contribution\\n†Corresponding author, daniel@baichuan-inc.com\\n1\\narXiv:2403.03853v3  [cs.CL]  11 Oct 2024'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 1}, page_content='0 5 10 15 20 25 30\\nLayer id\\n101\\n102\\n103\\n104\\nPerplexity\\nBaichuan2-7B-Base\\nLlama2-7B-Base\\nLlama2-7B-Base-Baseline\\nBaichuan2-7B-Base-Baseline\\n(a) Perplxity\\n0 5 10 15 20 25 30\\nLayer id\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nPerplexity\\n Baichuan2-7B-Base\\nLlama2-7B-Base\\nLlama2-7B-Base-Baseline\\nBaichuan2-7B-Base-Baseline (b) MMLU\\nFigure 1: Performance of removing certain layer from LLMs. We can see that certain layers\\nare redundant, and their removal results in minimal performance degradation.\\nyet effective pruning method ShortGPT, which identifies and removes layers with lower BI\\nscores, significantly reducing model size without sacrificing much performance.\\nTo evaluate our approach, we conducted evaluation across comprehensive benchmarks. Our\\nexperiments revealed that our method exhibits a smaller performance decrement compared\\nto the previous methods. For instance, removing 10 layers (25% of the total 40 layers)\\nfrom the LLaMA 2-13B model resulted in only a slight drop in performance on the MMLU'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 1}, page_content='from the LLaMA 2-13B model resulted in only a slight drop in performance on the MMLU\\nbenchmark (Hendrycks et al., 2020), from 55.0 to 52.2. Our findings highlight substantial\\nredundancy in current LLMs and suggest potential avenues for improving the efficiency of\\nmodel training by reducing inherent redundancy in the future.\\nThe main contributions of our paper are summarized as follows:\\n• We analyze the redundancy in large language models (LLMs) and find that they\\nexhibit significant redundancy at the layer level. This finding inspire us to prune\\nLLMs by simply removing redundant layers.\\n• We propose a metric called Block Influence (BI) as an indicator of layer importance.\\nBased on BI, our layer removal method maintains approximately 90% performance\\nwhile reducing approximately 25% of parameters, outperforming previous state-of-\\nthe-art methods.\\n• Furthermore, we demonstrate that our layer pruning approach is orthogonal to'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 1}, page_content='the-art methods.\\n• Furthermore, we demonstrate that our layer pruning approach is orthogonal to\\nquantization methods, meaning it can be combined with quantization techniques to\\nfurther reduce the deployment overhead of LLMs.\\n2 Motivation\\n2.1 Background\\nThe predominant LLMs are primarily based on the Transformer architecture (Vaswani et al.,\\n2017), with the pre-norm configuration being the most commonly adopted, as in models\\nlike LLaMA (Touvron et al., 2023). The pre-norm configuration, where layer normalization\\nis applied before the self-attention and feed-forward layers, offers several advantages such\\nas faster convergence, improved training stability, and better scalability for deeper networks\\n(Xiong et al., 2020; Liu et al., 2020; Wang et al., 2024). Due to these benefits, the pre-norm\\napproach has been adopted even in non-transformer models, such as Mamba (Gu & Dao,\\n2023) and RWKV (Peng et al., 2023). For the sake of simplicity in descriptions, our analysis'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 1}, page_content='2023) and RWKV (Peng et al., 2023). For the sake of simplicity in descriptions, our analysis\\nprimarily focuses on the Transformer architecture, though we extend our experiments to\\nnon-Transformer structures in Section 4.4.\\nHowever, we observe that when pre-norm is adopted, the similarity between the input and\\noutput of transformer layers tends to be higher, as illustrated in Figure 2. This high similarity\\nindicates that certain layers induce minimal changes to the hidden states, suggesting they\\ncontribute little to the model’s overall function. A detailed mathematical explanation for\\nthis phenomenon is provided in Appendix A. Which suggests that the deep layers of the\\n2'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 2}, page_content='0 5 10 15 20 25 30 35 40 45 50\\nTokens(B)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSimilarity\\npre norm 0-th layer\\npre norm 1-th layer\\npre norm 3-th layer\\npre norm 7-th layer\\npre norm 15-th layer\\npre norm 31-th layer\\npost norm 0-th layer\\npost norm 1-th layer\\npost norm 3-th layer\\npost norm 7-th layer\\npost norm 15-th layer\\npost norm 31-th layer\\nFigure 2: The cosine similarity between a layer’s input and output during the training\\nprocess. The horizontal axis (X-axis) represents the number of training tokens, while the\\nvertical axis (Y-axis) depicts the degree of similarity. Notably, the model employing post-\\nnormalization exhibits divergence after approximately ∼26B tokens of training. Training\\nsetting is provided in E.\\nmodel with pre-norm might not play a critical role in the overall function, and that the\\nlayers in large language models could be more redundant than expected, which motivates\\nthe layer-removal based pruning method we explore in the next section.\\n2.2 Layer redundancy'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 2}, page_content='the layer-removal based pruning method we explore in the next section.\\n2.2 Layer redundancy\\nTable 1: Ablation of removing FFN and\\nAttention of Llama2-7B-Base. We sample\\n100 instances from PG19 (Rae et al., 2019)\\nto calculate PPL.\\nDelete PPL\\nNone 7.60\\nThe whole last layer 13.37\\nAttention of the last layer 7.65\\nFFN of the last layer 12.35\\nAs discussed in the previous section, we spec-\\nulate that the LLMs exhibit layer redundancy.\\nTo verify this, we assess the performance degra-\\ndation caused by removing individual layers\\nof two popular models, Llama2-7B-Base (Tou-\\nvron et al., 2023), an English based LLMs, and\\nBaichuan2-7B-Base (Yang et al., 2023) which is\\nmainly focused on Chinese. Figure 1 confirms\\nour speculation, which reveals that some lay-\\ners do not play a crucial role in LLMs, causing\\nlittle degradation when omitting them individ-\\nually. Moreover, this redundancy is primarily\\nmanifested in the middle to later layers of the\\nnetwork, with the initial layers and the last layer'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 2}, page_content='manifested in the middle to later layers of the\\nnetwork, with the initial layers and the last layer\\noften being more critical. Notably, we found the last layer to be particularly important,\\naligning with findings from LLM Pruner (Ma et al., 2024). This observation contradicts\\nour mathematical explanation in Appendix A which suggests that deeper layers tend to\\nbe more redundant. We posit that this discrepancy arises because the final FFN effectively\\nfunctions as part of the token classifier and should be considered in conjunction with the\\nlanguage model head.To verify our hypothesis, we conducted further investigation, detailed\\nin Table 1. The results show that within the last layer, the FFN component is crucial, while\\nthe Attention module is less significant. This finding supports our interpretation of the final\\nlayer’s importance.\\n3'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 3}, page_content='0 5 10 15 20 25 30\\nLayer id\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\nBI score\\n0\\n5\\n10\\n15\\n20\\nPerplexity\\nBI score Perplexity\\n(a) Llama2 7B\\n0 5 10 15 20 25 30\\nLayer id\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nBI score\\n0\\n5\\n10\\n15\\n20\\n25\\nPerplexity\\nBI score Perplexity (b) Baichuan2 7B\\nFigure 3: The BI score of a layer and the PPL after removing the layer.\\n3 Methodology\\nIn this section, we present the methodological framework of our layer removal approach\\nfor LLMs, elucidating the underlying principles and techniques employed. We begin\\nby introducing Block Influence (BI), a novel metric designed to assess the hidden states\\ntransformation of each layer. Leveraging BI, we then detail our layer removal method.\\n3.1 Layer importance\\nAs outlined in the preceding section, the layers of LLMs exhibit redundancy, with varying\\ndegrees of redundancy across different layers. To capture this, we introduce a new metric,\\nBlock Influence (BI), to measure the degree of transformation performed by each layer. The'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 3}, page_content='Block Influence (BI), to measure the degree of transformation performed by each layer. The\\nBI score of ith layer can be calculated as follows:\\nBIi = 1 − EX,t\\nXT\\ni,tXi+1,t\\n||Xi,t||2||Xi+1,t||2\\n, (1)\\nwhere Xi,t means the tth row of hidden states of ith layer. Lower BI score imply that Xi and\\nXi+1 exhibit high cosine similarity, suggesting that the layer makes minimal transformations\\nto the hidden states and is therefore less important. We plot the BI scores of a single layer\\nand the PPL after removing it separately, as shown in the Figure 3. The results demonstrate\\na positive correlation between the BI score and the importance of a layer.\\n3.2 Layer Removal\\nOur goal is to obtain a pruned model that remains as close as possible to the original model.\\nSince an LLM functions as a series of transformations applied to hidden states across its\\nlayers and we can determine the importance of each layer, we propose a straightforward'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 3}, page_content='layers and we can determine the importance of each layer, we propose a straightforward\\npruning method: layer removal, which we refer to as ShortGPT. We delete certain layers\\nin LLMs based on BI score. First of all, we construct a calibration set, which is a set of\\nunlabelled text samples such as PG19 (Rae et al., 2019). Then we collect the hidden states of\\neach layer during inference on these samples. Next, we calculate the BI score based on the\\ncollected hidden states. Finally, we sort layers in ascending order according to the BI, and\\ndelete the layers with the lower BI score. The number of layers to be deleted can vary to\\ntrade off the speed and performance. The details of our layer removal setting can be found\\nin Appendix D.\\n4 Experiments\\n4.1 Experimental Setup\\nModels. To validate the effectiveness of our method, we conducted experiments on ex-\\nisting popular open-source language models, including Llama2-7B (Touvron et al., 2023),'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 3}, page_content='isting popular open-source language models, including Llama2-7B (Touvron et al., 2023),\\nLlama2-13B, Baichuan2-7B, and Baichuan2-13B. They are all large language models based\\n4'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 4}, page_content='Table 2: Comparison of pruning methods on multiple natural language benchmarks. The\\nresults of LLMPrun., SliceGPT and LaCo are reported from LaCo. The last column reports\\nthe relative performance retention.\\nLLM Method Ratio Benchmarks Ave. Per.\\nCMNLI HeSw PIQA CHID WSC CoQA BoolQ Race-H Race-M XSum C3 MMLU CMMLU\\nLlama2-7B\\nDense 0.00% 32.99 71.26 77.91 41.66 50.00 64.62 71.62 35.71 34.19 19.40 43.56 45.39 32.92 47.78 100.00\\nLLMPrun. 27.0% 34.33 56.46 71.22 25.25 36.54 42.51 55.20 22.56 22.35 11.51 25.64 23.33 25.25 34.78 72.79\\nSliceGPT 26.4% 31.70 50.27 66.21 20.79 36.54 41.36 38.32 21.07 21.66 4.89 39.78 28.92 25.37 32.84 68.73\\nLaCo 27.1% 34.43 55.69 69.80 36.14 40.38 45.70 64.07 22.61 23.61 15.64 39.67 26.45 25.24 38.41 80.39\\nShortGPT 27.1% 32.95 53.02 66.43 24.68 52.46 47.99 74.71 32.25 35.17 0.67 39.62 43.96 32.25 41.24 86.31\\nLlama2-13B\\nDense 0.00% 32.99 74.78 79.71 47.35 50.00 66.91 82.39 57.95 60.38 23.45 47.51 55.00 38.40 55.14 100.00'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 4}, page_content='LLMPrun. 24.4% 33.03 67.76 76.66 35.64 40.38 50.86 56.42 22.47 22.08 19.17 32.33 25.21 24.71 38.97 70.67\\nSliceGPT 23.6% 29.82 55.71 69.04 19.31 36.54 47.26 37.86 23.41 24.03 5.27 41.92 37.14 25.79 34.85 63.20\\nLaCo 24.6% 32.86 64.39 63.20 40.10 52.88 52.66 63.98 54.49 56.55 14.45 44.93 45.93 32.62 47.62 86.36\\nShortGPT 24.6% 33.00 66.64 73.45 36.61 50.00 58.64 62.48 58.35 60.17 17.59 46.90 54.69 38.38 50.53 91.64\\nBaichuan2-7B\\nDense 0.00% 33.37 67.56 76.17 85.56 50.00 63.14 74.10 52.63 51.04 20.82 64.55 53.87 56.95 57.67 100.00\\nLLMPrun. 24.2% 32.28 53.66 71.82 69.80 53.85 47.83 61.19 21.96 22.28 15.98 41.64 24.93 25.69 41.76 72.41\\nSliceGPT 22.2% 32.07 25.29 50.33 14.85 36.54 19.57 39.30 23.53 22.49 0.00 26.58 25.18 25.25 26.23 45.48\\nLaCo 24.2% 33.00 52.28 68.50 76.24 42.31 47.26 56.15 28.99 27.72 12.03 50.85 31.53 31.24 42.93 74.44\\nShortGPT 24.2% 33.30 56.96 67.68 65.63 50.00 46.70 67.83 53.26 46.76 0.04 56.33 45.77 47.87 49.08 85.10\\nBaichuan2-13B'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 4}, page_content='Baichuan2-13B\\nDense 0.00% 33.21 71.10 78.07 86.51 50.00 65.6 77.89 67.27 68.94 25.02 65.64 59.50 61.30 62.31 100.00\\nLLMPrun. 24.3% 33.80 53.57 71.82 72.77 37.50 38.82 56.54 21.17 21.61 13.67 39.89 23.19 25.18 39.20 62.91\\nSliceGPT 22.8% 32.07 25.85 51.03 10.40 36.54 18.02 37.83 21.56 21.52 0.00 24.99 22.95 25.26 25.23 40.49\\nLaCo 24.7% 33.03 60.71 68.88 76.73 44.23 55.45 62.35 56.92 57.80 12.32 61.10 51.35 53.65 53.43 85.75\\nShortGPT 24.7% 32.81 60.55 71.60 80.17 47.13 54.30 62.54 55.77 56.41 15.14 60.16 52.11 58.86 54.43 87.35\\non the decoder-only Transformer architecture. LLaMA 2 was trained on more than 2 trillion\\ntokens. Baichuan-series was mainly trained in Chinese and its 13-Billion model replaced the\\nRoPE (Su et al., 2024) positional embedding with ALiBi (Press et al., 2021).\\nBenchmarks. In order to comprehensively evaluate the changes in the ability of large\\nlanguage models before and after pruning, we conducted comprehensive evaluation from'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 4}, page_content='language models before and after pruning, we conducted comprehensive evaluation from\\nfive aspect: Reasoning: CMNLI (Li et al., 2024), HellaSwag (HeSw) (Zellers et al., 2019),\\nPIQA (Bisk et al., 2020). Language: CHID (Zheng et al., 2019), WSC (Levesque et al., 2012).\\nKnowledge: CommonSenseQA (CoQA) (Reddy et al., 2019), BoolQ (Clark et al., 2019).\\nExamination: MMLU (Hendrycks et al., 2020), CMMLU (Li et al., 2024). Understanding:\\nRace-High/Middle (H/M) (Lai et al., 2017), XSum (Hasan et al., 2021), C3 (Sun et al., 2020)\\nand PG19 (Rae et al., 2019). For more details, please refer to Appendix G\\nBaselines. To evaluate the effectiveness of our method, we compared several structured\\npruning methods for large language models, including:\\n1) LLMPru (Ma et al., 2024), which adopts structural pruning that selectively removes\\nnon-critical coupled structures based on gradient information, maximally preserving the'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 4}, page_content='non-critical coupled structures based on gradient information, maximally preserving the\\nmajority of the LLM’s functionality. LLMPru. applies post training to the pruned model,\\nbut for fair comparison, we do not apply post training to it.\\n2) SliceGPT (Ashkboos et al., 2024), which is a post-training sparsification scheme that\\nreplaces each weight matrix with a smaller matrix, reducing the embedding dimension\\nof the network. Specifically, they applied PCA to the hidden representation from shallow\\nto deep layers, and incorporated the dimension reduction matrix into existing network\\nparameters.\\n3) LaCo (Yang et al., 2024), which is a pruning method for large language models based\\non reducing layers. LaCo gradually merges similar layers from deep to shallow and sets a\\nthreshold to avoid continuously merging too many layers.\\nFor our evaluation, we use PG19 for layer importance and perplexity calculation. The\\nmodels, baselines and evaluate benchmarks is the same as LaCo.\\n5'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 5}, page_content='4.2 Main Results\\nTo validate the efficacy of our proposed method, we conducted comparative experiments\\nagainst baseline techniques commonly employed in large language model evaluation.\\nConsidering the current structured pruning methods generally reduce parameters by no\\nmore than 30%, we performed experiments with approximately 1/4 of the parameters\\npruned. The experimental results are presented in Table 2. Additional experiments exploring\\ndifferent parameter reduction proportions will be discussed in the subsequent section.\\nThe results demonstrate that the performance of the model pruned by our method signif-\\nicantly surpasses that of the baseline methods, maintaining most of the large language\\nmodel’s capabilities. Furthermore, we note that the approach of reducing the number of\\nlayers (ShortGPT/LaCo) outperforms the method of reducing the embedding dimensions\\n(LLMPru./SliceGPT), implying that the model exhibits more redundancy in depth than in'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 5}, page_content='(LLMPru./SliceGPT), implying that the model exhibits more redundancy in depth than in\\nwidth. Further experimental analysis will be presented in the ensuing section.\\nIn Table 2, we fully adopted the benchmark, model, and pruning ratio in the LaCo paper.\\nIn order to make a more fair comparison with LLMprun. and SliceGPT, we compared\\nthem with the same benchmark, model, and pruning ratio in their original paper. The\\nexperimental results are shown in Appendix C. Consistent with our findings in Table 2,\\nthese experiments further demonstrate the significant layer redundancy present in existing\\nlarge language models, and ShortGPT achieves superior performance compared to other\\npruning methods.\\nThe results show that coarse-grained pruning methods, such as removing entire layers,\\noften outperform fine-grained approaches like Slice GPT or LLM Pruner. We speculate that\\nthe reason is that the large language model is actually very robust, as shown in Figure 1,'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 5}, page_content='the reason is that the large language model is actually very robust, as shown in Figure 1,\\nremoving any deep layer individually actually has very little impact on the final output,\\nwhich means it is difficult to define the importance of a finer grained module and perform\\npruning.\\n4.3 Varying metric and pruning ratio\\nThe core principle of our method is to rank layers by their importance and remove the less\\nsignificant ones. The choice of importance metric significantly influences the outcome. In\\nthis section, we define and compare several different importance metrics:\\n• Sequential: The importance is directly proportional to the sequence order, with\\nshallower layers being less important. This can be implemented by assigning the\\nnegative value of each layer’s index as its importance metric.\\n• Norm/Reverse-order: This metric posits that importance is inversely proportional\\nto the sequence order. It assigns higher importance scores to the shallower layers.'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 5}, page_content='to the sequence order. It assigns higher importance scores to the shallower layers.\\nThis method gives the same order as measuring importance by hidden states norm\\nas Figure 4 shows.\\n• Relative Magnitude: Proposed in Samragh et al. (2023), this metric assumes layers\\nwith larger || f (x)\\nx+ f (x) || are of higher importance, where f is the layer transformation\\nfunction.\\n• BI: we calculate the BI score mentioned in Section 3.1 as importance metric.\\nFigure 4 demonstrates the different metrics. We observe that shallower layers in the LLM\\nnetwork are more crucial than deeper ones. Figure 5 shows the results of removing layers\\nby different metrics, demonstrating that Our proposed BI outperforms other metrics. The\\nmethod of Relative Magnitude is highly competitive, indicating that relative values can also\\nreflect the importance to some extent. It is worth noting that the hidden states norm seems\\nto be a good metric when only considering the MMLU benchmark, but the perplexity is'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 5}, page_content='to be a good metric when only considering the MMLU benchmark, but the perplexity is\\nrelatively poor.\\nAs a pruning method, we further validated the effects of different pruning ratios on model\\nperformance. Experiments were conducted on the Llama2 and Baichuan2 models, observing\\n6'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 6}, page_content='0 5 10 15 20 25 30\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nBI\\nBI\\nBaichuan-7B-Base\\nLlama2-7B-Base\\n0 5 10 15 20 25 30\\n0\\n5\\n10\\n15\\n20\\n25\\nNorm\\nNorm\\nBaichuan-7B-Base\\nLlama2-7B-Base\\n0 5 10 15 20 25 30\\nlayer_id\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRelative magnitude\\nRelative magnitude\\nBaichuan-7B-Base\\nLlama2-7B-Base\\n0 5 10 15 20 25 30\\nlayer_id\\n101\\n102\\n103\\n104\\nPerplexity\\nPerplexity\\nBaichuan-7B-Base\\nLlama2-7B-Base\\nFigure 4: Comparison of different importance metrics. Perplexity is calculated by removing\\neach single layer, other metrics is calculated by hidden states of each layer.\\n0 9 19 28 38 47 56 66 75 84 94\\n101\\n102\\n103\\n104\\n105\\n106\\nPerplexity\\nLlama2-7B-Base\\n0 8 15 22 30 38 45 52 60 68 75 82 90 98\\n101\\n102\\n103\\n104\\n105\\n106\\nLlama2-13B-Base\\n0 9 19 28 38 47 56 66 75 84 94\\nPruning Ratio(%)\\n25\\n30\\n35\\n40\\n45\\nMMLU\\n0 8 15 22 30 38 45 52 60 68 75 82 90 98\\nPruning Ratio(%)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nSequential Reverse-order Relative Magnitude BI\\nFigure 5: Performance of MMLU and perplexity when we prune by different metrics, with'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 6}, page_content='Figure 5: Performance of MMLU and perplexity when we prune by different metrics, with\\nincreasing pruning ratio. We can see that as the pruning ratio increases, the performance of\\nthe model declines.\\nthe Perplexity and MMLU. The results for Llama2, as shown in Figure 5, demonstrate that\\nthe model’s performance generally declines as the pruning ratio increases. However, we\\nobserve a notable phenomenon: the MMLU score exhibits a sharp drop at a specific layer.\\n7'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 7}, page_content='Table 3: ShortGPT pruning on RWKV and Mamba.\\nModel Pruning ratio CMNLI HeSw PIQA CHID WSC CoQA BoolQ Race-H Race-M XSum C3 MMLU CMMLU Ave. Per.\\nMamba-2.8B\\n0% 35.97 61.84 75.52 35.56 49.69 56.35 60.67 24.9 25.3 15.03 42.08 26.29 25.32 41.12 100.00\\n10.9% 32.95 59.71 73.01 32.52 49.28 52.66 51.41 24.27 25.21 14.95 41.1 26.01 25.00 39.08 95.04\\n20.3% 31.29 55.69 69.64 29.12 48.36 48.32 62.2 23.61 23.61 14.71 41.59 25.69 25.37 38.36 93.29\\n25% 29.96 52.38 68.77 26.02 48.26 44.96 62.2 23.67 23.26 14.00 40.71 24.32 24.89 37.18 90.42\\n31.3% 28.25 47.02 64.91 21.38 49.69 44.96 62.17 21.87 22.77 13.77 40.44 24.48 24.77 35.59 86.55\\nRWKV-7B\\n0% 32.07 65.98 77.09 85.36 50.00 62.65 62.72 38.56 45.47 16.5 57.97 31.85 28.54 50.37 100.00\\n9.4% 32.6 56.41 73.94 78.12 50.00 49.55 62.35 25.9 25.77 9.57 54.68 27.29 25.03 43.94 87.23\\n18.8% 32.11 49.47 71.55 65.63 50.00 40.54 61.19 22.04 23.75 8.13 49.15 26.35 25 40.38 80.17'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 7}, page_content='18.8% 32.11 49.47 71.55 65.63 50.00 40.54 61.19 22.04 23.75 8.13 49.15 26.35 25 40.38 80.17\\n25% 32.41 39.73 65.13 52.6 50.00 29.65 60.92 22.56 21.59 12.02 41.86 25.52 25.08 36.85 73.16\\n28.1% 33.11 32.22 60.01 32.47 50.1 28.34 60.85 22.27 21.31 10.43 37.81 25.64 25.15 33.82 67.14\\nTable 4: Layer removal results on Llama2-7B-Base-GPTQ.\\nModel Ratio/Layer Perplexity MMLU Throughput (speed up)\\nBaseline 0%/32 8.03 43.17 4331.23 Token/s (1.00x)\\n3.1%/31 8.37 42.88 4399.31 Token/s (1.02x)\\n9.4%/29 9.44 42.31 4602.26 Token/s (1.06x)\\nShortGPT 12.5%/28 10.24 41.62 4680.68 Token/s (1.08x)\\n15.6%/27 11.42 43.17 4756.94 Token/s (1.10x)\\n25.0%/24 22.29 41.68 5045.59 Token/s (1.16x)\\n27.1%/23 40.78 43.35 5146.99 Token/s (1.19x)\\nThis sudden decrease suggests the presence of certain critical layers within the network\\nthat play a particularly important role in maintaining performance. Similar patterns are\\nobserved in the Baichuan2 model, as illustrated in Appendix B.\\n4.4 Redundancy on non-transformer LLM'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 7}, page_content='4.4 Redundancy on non-transformer LLM\\nTo determine whether the observed depth redundancy is specific to the Transformer archi-\\ntecture, we extended our investigation to include two popular non-Transformer models,\\nRWKV-7B1 (Peng et al., 2023) and Mamba-2.8B2 (Gu & Dao, 2023). Our experiments revealed\\nthat these models also exhibit resilience to layer removal, maintaining performance despite\\nthe elimination of certain layers. This finding suggests that the redundancy phenomenon\\nmay not be unique to Transformer-based models, but rather a common characteristic across\\ncurrent large language models. Table 3 shows that our method is applicable and effective\\nfor both Mamba and RWKV models, suggesting that the redundancy is universal across\\ncurrent LLMs. However, it is worth noting that the RWKV model appears less redundant\\nthan Mamba and Transformer models, which warrants further investigation.\\n4.5 Orthogonal to Quantization'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 7}, page_content='4.5 Orthogonal to Quantization\\nIn this section, we show that our method is orthogonal to quantization methods. We apply\\nour method to Llama2-7B 3 quantized by GPTQ algorithm. Table 4 shows that our method is\\ncompatible with the quantization-like method. In addition, we compared the performance\\nof applying pruning before quantization 4. The results shown in the Table 5 further indicates\\nthat quantization and ShortGPT are orthogonal operations.\\n1We use rwkv-v5-world-7B from https://huggingface.co/RWKV/v5-Eagle-7B-HF\\n2We take the model from https://huggingface.co/state-spaces/mamba-2.8b-hf\\n3We take the model from https://huggingface.co/TheBloke/Llama-2-7B-GPTQ\\n4We use GPTQ algorithm for quantization from https://github.com/AutoGPTQ/AutoGPTQ\\n8'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 8}, page_content='Table 5: Performance comparison of different methods\\nMethod MMLU CMMLU\\nLlama2-7B-Baseline 45.4 32.9\\n4-bit quantization 44.9 32.5\\nLayer removal (27.1%) 44.0 32.3\\n4-bit quantization then layer removal 42.4 31.0\\nLayer removal then 4-bit quantization 41.2 30.5\\n4.6 Post training to restore performance\\nTo mitigate the performance loss resulting from layer removal, we explored post-training\\nstrategies inspired by Chen et al. (2024). Our approach comprised two key steps: 1)Replace-\\nment: We substituted the removed layers with lightweight Multi-Layer Perceptron (MLP)\\nmodules. 2)Retraining: We subsequently retrained the modified model. The results in Table\\n6 demonstrate the potential of post-train in recover performance loss. Appendix F list the\\ntraining details.\\nTable 6: Post-train Llama2-7B to restore performance.\\nMethod Avg. Ratio CMNLI HeSw PIQA CHID WSC CoQA BoolQ Race-H Race-M XSum C3 MMLU CMMLU\\nDense 47.78 0% 32.99 71.26 77.91 41.66 50.00 64.62 71.62 35.71 34.19 19.40 43.56 45.39 32.92'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 8}, page_content='Dense 47.78 0% 32.99 71.26 77.91 41.66 50.00 64.62 71.62 35.71 34.19 19.40 43.56 45.39 32.92\\nShortGPT 41.22 27.1% 32.95 53.02 66.43 24.68 52.46 47.99 74.41 32.25 35.17 0.67 39.62 43.96 32.25\\nShortGPT+post-train 43.16 24.0% 32.99 54.83 68.12 31.82 51.37 58.32 72.36 34.18 34.68 4.89 40.37 44.47 32.73\\n5 Limitation\\nAlthough our method demonstrates strong competitiveness compared to current pruning\\nmethods, there are some phenomena that have not been explained. Our experiments reveal\\nthat the negative effect of layer removal is more significant on generative tasks compared to\\nmultiple-choice tasks. When we remove 25% layers from Llama2-7B or Baichuan2-7B, the\\nperformance in generative tasks such as XSum and C3 deceases to nearly zero, although\\nthe performance decline was not as significant on the larger model of the 13B. We speculate\\nthat compared to multiple-choice tasks, generative tasks face the problem of accumulated'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 8}, page_content='that compared to multiple-choice tasks, generative tasks face the problem of accumulated\\nerrors and large model is more robust than small one. The reasons behind it still need to\\nbe explored. The post-training techniques discussed in Section 4.6 have the potential to\\nmitigate this issue and warrant further exploration.\\n6 Related works\\nTo reduce the inference cost of large language models and increase their practical applica-\\ntions, there have been many recent works on compressing models, which can be classified\\ninto two categories: model pruning and quantization. Besides, there are some works aim to\\nstudy the redundancy of model which is essential for compressing models.\\nModel pruning: model pruning (LeCun et al., 1989; Han et al., 2015) is a classic and effective\\nmethod of reducing model redundancy modules to compress models. The model pruning\\nmethods mainly include unstructured pruning and structured pruning. The unstructured'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 8}, page_content='methods mainly include unstructured pruning and structured pruning. The unstructured\\npruning simplifies an LLM by removing specific parameters without considering its internal\\nstructure, such as SparseGPT (Frantar & Alistarh, 2023) and LoRAPrune (Zhang et al., 2023).\\nHowever, this method disregards the overall LLM structure, resulting in an irregular sparse\\nmodel composition. Another more practical approach is structured pruning, GUM(Syed\\n9'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 9}, page_content='et al., 2023) makes an analysis of several structured pruning methods for decoder-only\\nLLMs. LLM-Pruner (Ma et al., 2024) selectively removes non-critical structures according to\\ngradient information. ShearedLLaMA (Xia et al., 2023) employs targeted structured pruning\\nand dynamic batch loading. LaCo (Yang et al., 2024) used layer merging to compress the\\nmodel. Compared to the previous method, our method is a simple and efficient structured\\npruning method.\\nQuantization: quantization (Liu et al., 2021; Gholami et al., 2022; Dettmers et al., 2022; 2024)\\nis a widely accepted technique in the field of model compression, which can significantly\\nsave the storage and computational costs of deep learning models. Traditional models are\\ngenerally stored as floating-point numbers, but quantization converts them into integers or\\nother discrete forms. LUT-GEMM (Park et al., 2022) quantifies only weights and optimizes'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 9}, page_content='other discrete forms. LUT-GEMM (Park et al., 2022) quantifies only weights and optimizes\\nmatrix multiplication in LLM using BCQ format. SPQR (Dettmers et al., 2023) identifies\\nand isolates abnormal weights, stores them with higher accuracy, and compresses all other\\nweights into 3-4 bits. Our model pruning method and quantization method are orthogonal,\\nwhich means quantification based on our pruned model can further compress the model.\\nModel redundancy: researchers have long noticed the significant redundancy in nonlinear\\nmodels (Catchpole & Morgan, 1997). In recent years, the transformer model architecture\\nhas been widely applied, and researchers have also studied its redundancy. In (Bian et al.,\\n2021), researchers analyzed redundancy in attention mechanisms, in which clear and similar\\nredundancy patterns (cluster structure) are observed among attention heads. In (Dalvi et al.,\\n2020), researchers dissect two pre-trained models, BERT (Devlin et al., 2018) and XLNet'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 9}, page_content='2020), researchers dissect two pre-trained models, BERT (Devlin et al., 2018) and XLNet\\n(Yang et al., 2019), studying how much redundancy they exhibit at a representation level\\nand a more fine-grained neuron-level. However, the redundancy in current large language\\nmodels based on decoder-only structures still needs to be explored.\\n7 Conclusion\\nIn this work, we uncovered the significant layer-wise redundancy of LLMs, Our research\\ndemonstrates that certain layers contribute minimally to overall network functionality\\nand can be removed without substantially compromising model performance. Based\\non our observation, We introduce Block influence to quantify the importance of each\\nlayer and propose a simple and straightforward pruning method: layer removal. Our\\nexperiments demonstrates that it is possible to maintain up to approximately 90% of a\\nLLM’s performance while reducing the model’s parameter amount and computational'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 9}, page_content='LLM’s performance while reducing the model’s parameter amount and computational\\nrequirements by approximately 25%. Besides, our method is orthogonal to quantization\\nmethods and can be further improved by continual training. We hope that our work\\ncan provide some insight for future model compression techniques. Moreover, our work\\nsuggests potential avenues for improving the efficiency of model training by reducing\\ninherent redundancy in the future.\\n10'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 10}, page_content='References\\nSaleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler,\\nand James Hensman. Slicegpt: Compress large language models by deleting rows and\\ncolumns. arXiv preprint arXiv:2401.15024, 2024.\\nYuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan, and Kenneth Church. On attention\\nredundancy: A comprehensive study. In Proceedings of the 2021 conference of the north\\namerican chapter of the association for computational linguistics: human language technologies,\\npp. 930–945, 2021.\\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\\nintelligence, pp. 7432–7439, 2020.\\nEdward A Catchpole and Byron JT Morgan. Detecting parameter redundancy. Biometrika,\\n84(1):187–196, 1997.\\nXiaodong Chen, Yuxuan Hu, and Jing Zhang. Compressing large language models by\\nstreamlining the unimportant layer. arXiv preprint arXiv:2403.19135, 2024.'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 10}, page_content='streamlining the unimportant layer. arXiv preprint arXiv:2403.19135, 2024.\\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\\npp. 2924–2936, 2019.\\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy\\nin pretrained transformer models, 2020.\\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\\nTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar,\\nSaleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 10}, page_content='Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A\\nsparse-quantized representation for near-lossless llm weight compression. arXiv preprint\\narXiv:2306.03078, 2023.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\\nfinetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805, 2018.\\nElias Frantar and Dan Alistarh. Massive language models can be accurately pruned in\\none-shot. arXiv preprint arXiv:2301.00774, 2023.\\nAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt\\nKeutzer. A survey of quantization methods for efficient neural network inference. In\\nLow-Power Computer Vision, pp. 291–326. Chapman and Hall/CRC, 2022.'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 10}, page_content='Low-Power Computer Vision, pp. 291–326. Chapman and Hall/CRC, 2022.\\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.\\narXiv preprint arXiv:2312.00752, 2023.\\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections\\nfor efficient neural network. Advances in neural information processing systems, 28, 2015.\\nTahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-\\nBin Kang, M Sohel Rahman, and Rifat Shahriyar. Xl-sum: Large-scale multilingual\\nabstractive summarization for 44 languages. In Findings of the Association for Computational\\nLinguistics: ACL-IJCNLP 2021, pp. 4693–4703, 2021.\\n11'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 11}, page_content='Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300, 2020.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\\nTom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\\nand Laurent Sifre. Training compute-optimal large language models, 2022.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\\nlanguage models, 2020.\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale\\nreading comprehension dataset from examinations. In Proceedings of the 2017 Conference'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 11}, page_content='reading comprehension dataset from examinations. In Proceedings of the 2017 Conference\\non Empirical Methods in Natural Language Processing, pp. 785–794, 2017.\\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural\\ninformation processing systems, 2, 1989.\\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and\\nTimothy Baldwin. Cmmlu: Measuring massive multitask language understanding in\\nchinese, 2024.\\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding\\nthe difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP), pp. 5747–5763, 2020.\\nZhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training\\nquantization for vision transformer. Advances in Neural Information Processing Systems, 34:\\n28092–28103, 2021.\\nXinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 11}, page_content='Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of\\nlarge language models. Advances in neural information processing systems, 36, 2024.\\nGunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo\\nLee. nuqmm: Quantized matmul for efficient inference of large-scale generative language\\nmodels. arXiv preprint arXiv:2206.09557, 2022.\\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao,\\nXin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , et al. Rwkv: Reinventing\\nrnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.\\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear\\nbiases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.\\nCompressive transformers for long-range sequence modelling. In International Conference'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 11}, page_content='Compressive transformers for long-range sequence modelling. In International Conference\\non Learning Representations, 2019.\\nSiva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question\\nanswering challenge. Transactions of the Association for Computational Linguistics, 7:249–266,\\n2019.\\nMohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash\\nFaghri, Devang Naik, Oncel Tuzel, and Mohammad Rastegari. Weight subcloning: direct\\ninitialization of transformers using larger pretrained ones, 2023.\\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\\nEnhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\\n12'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 12}, page_content='Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging\\nchinese machine reading comprehension. Transactions of the Association for Computational\\nLinguistics, 8:141–155, 2020.\\nAaquib Syed, Phillip Huang Guo, and Vijaykaarti Sundarapandiyan. Prune and tune:\\nImproving efficient pruning techniques for massive language models. Arxiv, 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\\ntion processing systems, 30, 2017.\\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 12}, page_content='Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.\\nDeepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 2024.\\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating\\nlanguage model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.\\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai\\nZhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the trans-\\nformer architecture. In International Conference on Machine Learning , pp. 10524–10533.\\nPMLR, 2020.\\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian\\nYu, Cong Yu, et al. Clue: A chinese language understanding evaluation benchmark. In\\nProceedings of the 28th International Conference on Computational Linguistics, pp. 4762–4772,\\n2020.\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv,'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 12}, page_content='2020.\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv,\\nDa Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models.\\narXiv preprint arXiv:2309.10305, 2023.\\nYifei Yang, Zouying Cao, and Hai Zhao. Laco: Large language model pruning via layer\\ncollapse, 2024.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. Advances\\nin neural information processing systems, 32, 2019.\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\\na machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics, pp. 4791–4800, 2019.\\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural\\nInformation Processing Systems, 32, 2019.\\nMingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al.'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 12}, page_content='Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al.\\nPruning meets low-rank parameter-efficient fine-tuning. arXiv preprint arXiv:2305.18403,\\n2023.\\nChujie Zheng, Minlie Huang, and Aixin Sun. Chid: A large-scale chinese idiom dataset\\nfor cloze test. In Proceedings of the 57th Annual Meeting of the Association for Computational\\nLinguistics, pp. 778–787, 2019.\\nXunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression\\nfor large language models, 2023.\\n13'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 13}, page_content='A Mathematical explanation for why pre-norm brings high similarity\\nWe provide a simple explanation here about how pre-norm leads to high deep similarity in\\nthis section, here we adopt RMSNorm (Zhang & Sennrich, 2019) for convenient, which is\\nalso the popular pre-norm used in many recent LLMs, such as Llama and Mamba.\\nLemma 1 (Xiong et al., 2020) At initialization, for the Pre-LN Transformer, (1 + L\\n2 )d ≤\\nE(||xL,i||2\\n2) ≤ (1 + 3L\\n2 )d for all L > 0 and i. Expectations are taken over the input and the\\nrandomness of initialization, where the hidden state of Lth layer is xL.\\nFrom Lemma 1, the hidden state of the pre-norm model will continuously increase as the\\nnumber of layers increases. And under the assumption of each component of xl has a mean\\nof 0, we can obtain ||xL|| = Θ(\\n√\\nL).\\nThen we consider xL+1 = xL + fL(xL, θL), where fL is a operation such as Attention or MLP ,\\nθL is learnable parameters. Then fL(xL, θL) =O(1) respect to L, for Attention as example,'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 13}, page_content='θL is learnable parameters. Then fL(xL, θL) =O(1) respect to L, for Attention as example,\\n||fL(xL, θL)|| = ||(so f tmax(QTK)XL/||XL|| ·(σrms ))WvWq|| = O(||σrms ||||Wv||||Wo||) =\\nO(1) respect to L.\\nThen we can get:\\ncos similarity(XL+1, XL) = xL+1xL\\n||xL+1||||xL|| = ||xL||2\\n||xL+1||||xL|| + fL(xL, θ)xL\\n||xL+1||||xL|| (2)\\n≥ ||xL||2\\n||xL+1||||xL|| − ||fL(xL, θ)||||xL||\\n||xL+1||||xL|| (3)\\n= ||xL||\\n||xL+1|| − ||fL(xL, θ)||\\nxL+1\\n= Θ(\\nr\\nL\\nL + 1 ) − O(\\nr\\n1\\nL + 1 ) (4)\\nThis means that as the number of layers L increases, the similarity between the input and\\noutput of the layer will be high. This means that the role of fL may be relatively small, and\\nremoving it from the network may have a relatively small impact to the model.\\nAlthough the above theoretical analysis is only for randomly initialized models, this phe-\\nnomenon that deep layer has similar input and output exists in both our own trained models\\nshown in Figure 2 and existing models in Figure 4.'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 13}, page_content='shown in Figure 2 and existing models in Figure 4.\\nB Layer Removal on Baichuan2-series Model\\n0 9 19 28 38 47 56 66 75 84 94\\n102\\n104\\n106\\n108\\nPerplexity\\nBaichuan2-7B-Base\\n0 8 15 22 30 38 45 52 60 68 75 82 90 98\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\nBaichuan2-13B-Base\\n0 9 19 28 38 47 56 66 75 84 94\\nPruning Ratio(%)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nMMLU\\n0 8 15 22 30 38 45 52 60 68 75 82 90 98\\nPruning Ratio(%)\\n30\\n40\\n50\\n60\\nSequential Reverse-order Relative Magnitude BI\\nFigure 6: Pruning by different metrics on Baichuan2-series model.\\n14'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 14}, page_content='C A Fair comparison with SliceGPT and LLMprun.\\nIn Table 2, we fully adopted the benchmark, model, and pruning ratio in the LaCo’s paper.\\nFor a fair comparison with LLM pruner and SliceGPT, we do the same experiments in the\\noriginal paper of LLM pruner and SliceGPT. The results is provided in Table 7 and Table 8.\\nWe take the same benchmarks, models and pruning ratio as the corresponding original\\npaper. The results demonstrate that our method is highly competitive.\\nTable 7: Comparison between ShortGPT and LLM-pruner. The Table is corresponding\\nto the Table 1 of LLM pruner(Zhang et al., 2023).\\nModel Pruning ratio Method BoolQ PIQA Hellaswag Winogrande Arc-e Arc-c OBQA Avg.\\nLlama-7B\\nRatio=0% Baseline 73.18 78.35 72.99 67.01 67.45 41.38 42.4 63.25\\nRatio=20% LLM-pruner 59.39 75.57 65.34 61.33 59.18 37.12 39.80 56.82\\nRatio=21.9 % ShortGPT 68.26 72.28 61.7 63.77 60.22 39 41.6 58.12\\nLlama-13B\\nRatio=0% Baseline 68.47 78.89 76.24 70.09 74.58 44.54 42.00 64.97'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 14}, page_content='Llama-13B\\nRatio=0% Baseline 68.47 78.89 76.24 70.09 74.58 44.54 42.00 64.97\\nRatio=20% LLM-pruner 67.68 77.15 73.41 65.11 68.35 38.4 42.4 61.79\\nRatio=20% ShortGPT 68.41 76.36 72.9 67.4 68.62 39.2 41 61.98\\nTable 8: Comparison between ShortGPT and SliceGPT. The Table is\\ncorresponding to the Table 7 of SliceGPT(Ashkboos et al., 2024).\\nModel Pruning ratio Method PIQA Hellaswag Winogrande Arc-e Arc-c Avg.\\nLlama-2-7B\\n0% Baseline 79.11 75.99 69.06 74.58 46.25 69\\n20% SliceGPT 71.87 58.1 63.04 69.87 43.09 63.45\\n25% SliceGPT 68.55 58.1 62.04 57.46 35.07 56.15\\n30% SliceGPT 66.1 52.69 56.82 35.07 56.82 56.15\\n21.9% ShortGPT 72.76 66.39 66.27 59.39 39.85 60.93\\n25% ShortGPT 70.53 62.68 64.7 58.39 39.51 59.16\\n31.6% ShortGPT 67.87 62.19 64.38 56.57 40.86 58.37\\nLlama-2-13B\\n0% Baseline 80.47 79.39 72.22 77.48 49.23 71.76\\n20% SliceGPT 71.87 69.38 63.04 69.87 43.09 63.45\\n25% SliceGPT 68.55 67.48 58.1 62.5 37.88 58.9\\n30% SliceGPT 66.1 65.11 52.69 56.82 35.07 55.16'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 14}, page_content='25% SliceGPT 68.55 67.48 58.1 62.5 37.88 58.9\\n30% SliceGPT 66.1 65.11 52.69 56.82 35.07 55.16\\n20% ShortGPT 76.95 74.67 71.14 69.56 45.63 67.59\\n25% ShortGPT 74.39 71.65 70.98 67.09 43.93 65.61\\n30% ShortGPT 72.11 71.93 67.19 61.09 40.88 62.64\\nLlama-2-70B\\n0% Baseline 82.7 83.84 77.98 80.98 57.34 76.57\\n20% SliceGPT 76.61 72.98 74.92 80.51 55.2 72.34\\n25% SliceGPT 74.92 68.74 74.92 77.9 51.71 69.75\\n30% SliceGPT 72.31 63.69 73.4 51.71 47.61 66.11\\n20% ShortGPT 76.02 78.87 71.69 76.02 52.95 71.68\\n25% ShortGPT 73.2 76.72 71.85 73.2 49.9 69.79\\n30% ShortGPT 74.44 75.31 72.33 74.44 49.22 69.4\\nD Detailed Strategies for Layer Removal\\nWe list the details of different layer removal strategies in Table 10. The concrete removed\\nlayers by ShortGPT in Table 2 are listed in Table 9\\nTable 9: Setup of Removed Layers for Benchmark Models.\\nModel Removed Layers\\nLlama-2-7B 27, 26, 25, 28, 24, 29, 23, 21, 22\\nLlama-2-13B 33, 31, 32, 30, 29, 34, 28, 35, 27, 26\\nBaichuan-2-7B 26, 27, 25, 28, 24, 29, 23, 22, 30'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 14}, page_content='Llama-2-13B 33, 31, 32, 30, 29, 34, 28, 35, 27, 26\\nBaichuan-2-7B 26, 27, 25, 28, 24, 29, 23, 22, 30\\nBaichuan-2-13B 32, 31, 33, 30, 34, 29, 28, 35, 27, 26\\n15'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 15}, page_content='Table 10: Strategies for Layer Removal in Models.\\nStrategy Description\\nSequential Layers are removed sequentially from the beginning of the\\nmodel. The process starts with layer 0 and progressively\\nincludes more layers for removal (e.g., {0}, {0, 1}, . . . ).\\nReverse-order This strategy involves starting from the model’s final layer\\nand progressively removing layers in reverse order (e.g.,\\n{-1}, {-1, -2}, . . . ).\\nRelative Magnitude Layers are removed in ascending order based on their Rel-\\native Magnitude values. The removal process accumulates\\nlayers from those with the smallest to the largest values,\\nmirroring the sequential strategy’s accumulation method.\\nBI (Block Influence) Follows a similar accumulation approach as the Sequential\\nstrategy, but layers are ordered and removed according to\\ntheir BI values, starting from the lowest and moving to the\\nhighest.\\nE Setup for training post-norm model and pre-norm model'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 15}, page_content='highest.\\nE Setup for training post-norm model and pre-norm model\\nWe have listed the specific training settings for pre norm and post norm in Table 11.\\nTable 11: Training Parameters.\\nParameter Value\\nGlobal Batch Size 2048\\nSequence length 4096\\nPrecision bf16\\nLearning Rate Scheduler cosine\\nMax Learning Rate 4e-4\\nMin Learning Rate 5e-5\\nWarm-up steps 3000\\nTraining Tokens 200B\\nWeight Decay 0.1\\nAdam Beta1 0.9\\nAdam Beta2 0.98\\nGradient Clip 1.0\\nTokenizer Llama2\\nLayers 32\\nHidden state 2048\\nAttention heads 32\\nHead dim 64\\nFFN size 5504\\nActivation function Silu\\nF post-training settings\\nWe replace the removed layer with a lightweight gated MLP layer with hidden size = 2048.\\nTable 12 show the post training settings.\\n16'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 16}, page_content='Table 12: Post-training Parameters.\\nParameter Value\\nGlobal Batch Size 2048\\nSequence length 4096\\nPrecision bf16\\nLearning Rate Scheduler cosine\\nMax Learning Rate 2e-5\\nMin Learning Rate 1e-5\\nWarm-up steps 3000\\nTraining Tokens 50B\\nWeight Decay 0.1\\nAdam Beta1 0.9\\nAdam Beta2 0.98\\nGradient Clip 1.0\\nG Evaluation Benchmarks\\nIn order to comprehensively evaluate the changes in the ability of large language models\\nbefore and after pruning, we conducted evaluations on the most commonly used Benchmark\\nMMLU Hendrycks et al. (2020), CMMLU Li et al. (2024) for evaluating large models. In\\naddition, we also followed LaCo Yang et al. (2024) to evaluate a wider dataset.\\nMMLU Hendrycks et al. (2020) is a benchmark aimed at measuring the knowledge acquired\\nduring pre-training by specifically evaluating models in zero-shot and few-shot settings.\\nThis makes benchmarks more challenging and similar to the way we evaluate humans. This'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 16}, page_content='This makes benchmarks more challenging and similar to the way we evaluate humans. This\\nbenchmark covers 57 subjects including STEM, humanities, social sciences, etc. Its difficulty\\nranges from beginner to advanced professional level, and it tests world knowledge and\\nproblem-solving ability.\\nCMMLU Li et al. (2024) is a comprehensive Chinese language assessment dataset designed\\nspecifically to evaluate LLM’s advanced knowledge and reasoning abilities in the context\\nof Chinese language and culture. CMMLU covers 67 topics, from elementary school to\\nuniversity or professional level. Including natural sciences, as well as humanities and social\\nsciences, it also includes many contents with Chinese characteristics.\\nCMNLI Xu et al. (2020) is part of the Chinese language understanding assessment bench-\\nmark. It consists of two parts: XNLI and MNLI. HellaSwag (HeSw) Zellers et al. (2019)\\nis a challenging dataset for evaluating commonsense NLI that is especially hard for state-'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 16}, page_content='is a challenging dataset for evaluating commonsense NLI that is especially hard for state-\\nof-the-art models, though its questions are trivial for humans. PIQA Bisk et al. (2020) is\\na multi-choice question and answer dataset that focuses on daily scenarios. This dataset\\nexplores the model’s grasp of the laws of the real physical world through daily scenarios.\\nCHID Zheng et al. (2019) is an idiom cloze test dataset that mainly focuses on the selection of\\ncandidate words and the representation of idioms. CoQA Reddy et al. (2019) is a large-scale\\ndataset used for conversational question-answering tasks, containing over 127000 questions\\nand their corresponding answers. BoolQ Clark et al. (2019) is a question-answer dataset\\ncontaining 15942 examples of yes/no questions. These problems occur naturally - they are\\ngenerated in an environment that is silent and unconstrained. Race Lai et al. (2017) is a\\nlarge-scale reading comprehension dataset collected from English examinations in China,'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 16}, page_content='large-scale reading comprehension dataset collected from English examinations in China,\\nwhich are designed for middle school and high school students. XSumHasan et al. (2021)\\nis used to evaluate abstract single document summarization systems. The goal is to create\\na short, one-sentence new summary of what the article is about. C3 Sun et al. (2020) is a\\nmachine reading comprehension dataset with multiple choices, consisting of multiple-choice\\nquestions, reading materials from Chinese proficiency exams, and ethnic Chinese exams.\\nPG19 Rae et al. (2019) is a long document dataset from books used to test the effectiveness\\nof language modeling.\\nH Hardware Environment\\nThe platform we use to experiment is GPU heterogeneous platform. The hardware of our\\nplatform is shown in Table 13\\n17'),\n",
              " Document(metadata={'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj', 'page': 17}, page_content='Table 13: Setup of Removed Layers for Benchmark Models.\\nName Details\\nCPU 2x Intel(R) Xeon(R) Gold 6430 CPU @ 2.1GHz\\nGPU 8x NVIDIA A100-80GB Tensor Core GPU\\n18')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Images:"
      ],
      "metadata": {
        "id": "u4Epcyz-fnkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm_for_img = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "4U6A_1k5fkqg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def generate_docs_from_img(img_url):\n",
        "    message = HumanMessage(\n",
        "    content=[\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"Give me a summary of what you see in the image. It must be 3 detailed paragraphs.\",\n",
        "            },\n",
        "            {\"type\": \"image_url\", \"image_url\": img_url},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = llm_for_img.invoke([message]).content\n",
        "        print(f\"Generated summary: {response}\")\n",
        "        docs = Document(page_content=response, metadata={\"source\": img_url})\n",
        "        split_docs = splitter.split_documents([docs])\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the request due to Invalid Content or Invalid Image URL\")\n",
        "        raise e\n",
        "\n",
        "    return split_docs"
      ],
      "metadata": {
        "id": "v9wlAQcDfpH7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_docs = generate_docs_from_img(\"https://datasciencedojo.com/wp-content/uploads/Architecture-of-transformer-models-1030x579.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6yemEoOfqqc",
        "outputId": "1f16d7b9-cdd5-46ca-bde2-c6522c2651cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated summary: This image diagrams the architecture of a Transformer, a neural network architecture commonly used in natural language processing.  The diagram is a high-level overview, showing the main components and their interconnections.  The core of the Transformer lies in the \"Multi-head attention\" mechanism, which is further broken down into the \"Scaled dot-product attention\" for a more detailed understanding.  The entire process starts with input embeddings, which are then processed through multiple layers of the encoder and decoder, each incorporating multiple \"Multi-head attention\" blocks and \"Feed Forward\" networks.\n",
            "\n",
            "The encoder section, on the left, processes the input sequence. It begins with positional encoding added to the input embeddings, providing information about the word order.  This is then fed into multiple layers, each containing a \"Multi-Head Attention\" layer and a \"Feed Forward\" layer, with \"Add & Norm\" operations between them for normalization and residual connections. The \"Masked Multi-Head Attention\" is used in specific applications, such as language modeling, where the model shouldn't \"peek\" at future tokens.  These layers work together to generate a contextual representation of the input sequence.\n",
            "\n",
            "The decoder section, also on the left, takes the encoder's output as input along with the positional encoding of the target sequence and processes it through similar layers.  The key difference is that the decoder uses a \"Masked Multi-Head Attention\" layer to prevent it from attending to future tokens in the target sequence during training. This helps in predicting the next word in a sequence. The final output probabilities are then generated through a linear layer and a softmax function. The diagram also provides zoomed-in views of the \"Multi-head attention\" and \"Scaled dot-product attention\" mechanisms, illustrating the matrix multiplications and softmax operations involved in calculating the attention weights.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6QUOQyPfzcf",
        "outputId": "472655e3-6330-4a52-9594-32360268c414"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://datasciencedojo.com/wp-content/uploads/Architecture-of-transformer-models-1030x579.jpg'}, page_content='This image diagrams the architecture of a Transformer, a neural network architecture commonly used in natural language processing.  The diagram is a high-level overview, showing the main components and their interconnections.  The core of the Transformer lies in the \"Multi-head attention\" mechanism, which is further broken down into the \"Scaled dot-product attention\" for a more detailed understanding.  The entire process starts with input embeddings, which are then processed through multiple layers of the encoder and decoder, each incorporating multiple \"Multi-head attention\" blocks and \"Feed Forward\" networks.'),\n",
              " Document(metadata={'source': 'https://datasciencedojo.com/wp-content/uploads/Architecture-of-transformer-models-1030x579.jpg'}, page_content='The encoder section, on the left, processes the input sequence. It begins with positional encoding added to the input embeddings, providing information about the word order.  This is then fed into multiple layers, each containing a \"Multi-Head Attention\" layer and a \"Feed Forward\" layer, with \"Add & Norm\" operations between them for normalization and residual connections. The \"Masked Multi-Head Attention\" is used in specific applications, such as language modeling, where the model shouldn\\'t \"peek\" at future tokens.  These layers work together to generate a contextual representation of the input sequence.'),\n",
              " Document(metadata={'source': 'https://datasciencedojo.com/wp-content/uploads/Architecture-of-transformer-models-1030x579.jpg'}, page_content='The decoder section, also on the left, takes the encoder\\'s output as input along with the positional encoding of the target sequence and processes it through similar layers.  The key difference is that the decoder uses a \"Masked Multi-Head Attention\" layer to prevent it from attending to future tokens in the target sequence during training. This helps in predicting the next word in a sequence. The final output probabilities are then generated through a linear layer and a softmax function. The diagram also provides zoomed-in views of the \"Multi-head attention\" and \"Scaled dot-product attention\" mechanisms, illustrating the matrix multiplications and softmax operations involved in calculating the attention weights.')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Audio:"
      ],
      "metadata": {
        "id": "ulAQxGx5fw3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import AssemblyAIAudioTranscriptLoader\n",
        "def load_audio_documents(path: str):\n",
        "    audio_loader = AssemblyAIAudioTranscriptLoader(file_path=path)\n",
        "\n",
        "    docs = audio_loader.load()\n",
        "\n",
        "    if docs:\n",
        "        split_docs = splitter.split_documents(docs)\n",
        "\n",
        "        return split_docs"
      ],
      "metadata": {
        "id": "-fPF5YFYfxu8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_docs = load_audio_documents(\"/content/RAG for LLMs explained in 3 minutes.mp3\")"
      ],
      "metadata": {
        "id": "ppnxOcONf4Ld"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRZCkPmHgHA-",
        "outputId": "3fa01bc8-d93d-4e27-ab5a-6911c5ddfd7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'language_code': 'en_us', 'audio_url': 'https://cdn.assemblyai.com/upload/0c0fbf9b-b080-48a9-8611-eae24774762d', 'punctuate': True, 'format_text': True, 'dual_channel': None, 'multichannel': None, 'audio_channels': None, 'webhook_url': None, 'webhook_auth_header_name': None, 'webhook_auth_header_value': None, 'audio_start_from': None, 'audio_end_at': None, 'word_boost': [], 'boost_param': None, 'filter_profanity': False, 'redact_pii': False, 'redact_pii_audio': False, 'redact_pii_audio_quality': None, 'redact_pii_policies': None, 'redact_pii_sub': None, 'speaker_labels': False, 'speakers_expected': None, 'content_safety': False, 'content_safety_confidence': None, 'iab_categories': False, 'custom_spelling': None, 'disfluencies': False, 'sentiment_analysis': False, 'auto_chapters': False, 'entity_detection': False, 'summarization': False, 'summary_model': None, 'summary_type': None, 'auto_highlights': False, 'language_detection': False, 'language_confidence_threshold': None, 'language_confidence': None, 'speech_threshold': None, 'speech_model': None, 'id': '656ac874-63b0-48ef-808e-e9c01d6f394f', 'status': <TranscriptStatus.completed: 'completed'>, 'error': None, 'text': \"Large language models and AI assistants are taking the consumer world by storm. But what happens when you try to bring these large language models into business, into enterprise? Well, we have three issues, three issues that we have to overcome. Let's walk through them. Number one issue is this lack of domain knowledge. Remember, these large language models have been trained on publicly available data sets. That means that they don't have Access to your SOPs, your standard operating procedures, they don't have access to your own ip, your own records. So they really can't answer a lot of questions and have that response be tailored to your particular business. And you lose a lot of performance and effectiveness because of that. Issue number two is around hallucinations. These models will give you responses. They look really credible, but they're way off. And if you run with them, then you might have a problem. And then issue number three, which is becoming a little bit of a less of an issue with search. But we have training data cutoff dates. So for a while there you had chatgpt missing months of training data because it hadn't been updated in a while and in part because it takes a lot of compute to train these models. So you have these three issues that are keeping you from getting a lot of performance from your LLMs as you bring them in house. So let's talk about one pattern that has emerged as being particularly helpful here, and that is RAG retrieval, augmented generation. You may have heard this term toss around, but let's talk about what's going on here. First, let me give you some context. Here's what happens when you send a prompt to your standard AI assistant. So you have your. Your prompt goes into the AI assistant, it generates a response and then gives it back to you. Right? In a RAG implementation, you're adding an additional step here. Before that prompt goes into the AI assistant, we have a search that hits a corpus of data. Now this is going to be your data, your own documents and other relevant information that you want to make available to the AI assistant. There's going to be a retrieval that's done and that context is going to be added in addition to your original prompt. And so the large language model is going to get your prompt and then also any relevant information that that was found during this process. And then everything else proceeds the same way. The AI system will process that and generate typically a better response for you as a user. So the retrieval here is this retrieval function here where we're grabbing the information. The augmented part here is we're augmenting that to the original prompt and the generation here is that we're generation generating the same prompt here from the LLM. It turns out that this is a robust, efficient way to tackle these problems that we're seeing with LLMs and industry and business. So I hope that this helps explain the RAG framework. If you have any questions, any comments if I missed anything, please drop them below. If you're a practitioner and you want to add something to this conversation, please drop that below as well. There are a lot of people that are going to watch this, and I'm sure that they would benefit from your expertise and your knowledge. Thanks and talk to you soon.\", 'words': [{'text': 'Large', 'start': 240, 'end': 448, 'confidence': 0.99654, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 464, 'end': 776, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 808, 'end': 1016, 'confidence': 0.99879, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 1048, 'end': 1192, 'confidence': 0.91664, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 1216, 'end': 1496, 'confidence': 0.99191, 'speaker': None, 'channel': None}, {'text': 'assistants', 'start': 1528, 'end': 2200, 'confidence': 0.83958, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 2280, 'end': 2520, 'confidence': 0.99864, 'speaker': None, 'channel': None}, {'text': 'taking', 'start': 2560, 'end': 2856, 'confidence': 0.99972, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 2928, 'end': 3208, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'consumer', 'start': 3264, 'end': 3960, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'world', 'start': 4040, 'end': 4376, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'by', 'start': 4448, 'end': 4824, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'storm.', 'start': 4912, 'end': 5800, 'confidence': 0.99589, 'speaker': None, 'channel': None}, {'text': 'But', 'start': 5960, 'end': 6280, 'confidence': 0.99517, 'speaker': None, 'channel': None}, {'text': 'what', 'start': 6320, 'end': 6520, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'happens', 'start': 6560, 'end': 6856, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'when', 'start': 6888, 'end': 7032, 'confidence': 0.99985, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 7056, 'end': 7192, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'try', 'start': 7216, 'end': 7400, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 7440, 'end': 7592, 'confidence': 0.99887, 'speaker': None, 'channel': None}, {'text': 'bring', 'start': 7616, 'end': 7896, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 7968, 'end': 8152, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 8176, 'end': 8408, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 8464, 'end': 8824, 'confidence': 0.99961, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 8872, 'end': 9304, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 9352, 'end': 9656, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'business,', 'start': 9728, 'end': 10152, 'confidence': 0.99863, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 10256, 'end': 10664, 'confidence': 0.54956, 'speaker': None, 'channel': None}, {'text': 'enterprise?', 'start': 10752, 'end': 11496, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'Well,', 'start': 11608, 'end': 11832, 'confidence': 0.85014, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 11856, 'end': 11992, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 12016, 'end': 12296, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 12368, 'end': 12696, 'confidence': 0.9522, 'speaker': None, 'channel': None}, {'text': 'issues,', 'start': 12768, 'end': 13432, 'confidence': 0.99097, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 13616, 'end': 13960, 'confidence': 0.98567, 'speaker': None, 'channel': None}, {'text': 'issues', 'start': 14000, 'end': 14536, 'confidence': 0.99866, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 14688, 'end': 15048, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 15104, 'end': 15320, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 15360, 'end': 15560, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 15600, 'end': 15752, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'overcome.', 'start': 15776, 'end': 16344, 'confidence': 0.99912, 'speaker': None, 'channel': None}, {'text': \"Let's\", 'start': 16392, 'end': 16616, 'confidence': 0.77592, 'speaker': None, 'channel': None}, {'text': 'walk', 'start': 16648, 'end': 16840, 'confidence': 0.99926, 'speaker': None, 'channel': None}, {'text': 'through', 'start': 16880, 'end': 17080, 'confidence': 0.99726, 'speaker': None, 'channel': None}, {'text': 'them.', 'start': 17120, 'end': 17464, 'confidence': 0.9914, 'speaker': None, 'channel': None}, {'text': 'Number', 'start': 17552, 'end': 17800, 'confidence': 0.97466, 'speaker': None, 'channel': None}, {'text': 'one', 'start': 17840, 'end': 18040, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 18080, 'end': 18568, 'confidence': 0.99624, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 18704, 'end': 19048, 'confidence': 0.99853, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 19104, 'end': 19416, 'confidence': 0.99831, 'speaker': None, 'channel': None}, {'text': 'lack', 'start': 19488, 'end': 19960, 'confidence': 0.94781, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 20040, 'end': 20616, 'confidence': 0.99901, 'speaker': None, 'channel': None}, {'text': 'domain', 'start': 20768, 'end': 21672, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'knowledge.', 'start': 21816, 'end': 22600, 'confidence': 0.87386, 'speaker': None, 'channel': None}, {'text': 'Remember,', 'start': 22760, 'end': 23192, 'confidence': 0.85016, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 23256, 'end': 23528, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 23584, 'end': 23800, 'confidence': 0.99781, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 23840, 'end': 24216, 'confidence': 0.87562, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 24248, 'end': 24504, 'confidence': 0.53107, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 24552, 'end': 24712, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'been', 'start': 24736, 'end': 24872, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'trained', 'start': 24896, 'end': 25144, 'confidence': 0.96243, 'speaker': None, 'channel': None}, {'text': 'on', 'start': 25192, 'end': 25400, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'publicly', 'start': 25440, 'end': 25944, 'confidence': 0.97609, 'speaker': None, 'channel': None}, {'text': 'available', 'start': 25992, 'end': 26344, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 26432, 'end': 26680, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'sets.', 'start': 26720, 'end': 27064, 'confidence': 0.92469, 'speaker': None, 'channel': None}, {'text': 'That', 'start': 27112, 'end': 27272, 'confidence': 0.9986, 'speaker': None, 'channel': None}, {'text': 'means', 'start': 27296, 'end': 27480, 'confidence': 0.9982, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 27520, 'end': 27672, 'confidence': 0.9872, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 27696, 'end': 27832, 'confidence': 0.99926, 'speaker': None, 'channel': None}, {'text': \"don't\", 'start': 27856, 'end': 28056, 'confidence': 0.99771, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 28088, 'end': 28280, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'Access', 'start': 28320, 'end': 28616, 'confidence': 0.99859, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 28688, 'end': 29250, 'confidence': 0.99093, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 29400, 'end': 29758, 'confidence': 0.99267, 'speaker': None, 'channel': None}, {'text': 'SOPs,', 'start': 29814, 'end': 30334, 'confidence': 0.89151, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 30382, 'end': 30542, 'confidence': 0.96304, 'speaker': None, 'channel': None}, {'text': 'standard', 'start': 30566, 'end': 30846, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'operating', 'start': 30878, 'end': 31262, 'confidence': 0.63512, 'speaker': None, 'channel': None}, {'text': 'procedures,', 'start': 31326, 'end': 32046, 'confidence': 0.99754, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 32158, 'end': 32382, 'confidence': 0.99858, 'speaker': None, 'channel': None}, {'text': \"don't\", 'start': 32406, 'end': 32606, 'confidence': 0.9977, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 32638, 'end': 32782, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'access', 'start': 32806, 'end': 33086, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 33158, 'end': 33342, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 33366, 'end': 33502, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 33526, 'end': 33710, 'confidence': 0.99756, 'speaker': None, 'channel': None}, {'text': 'ip,', 'start': 33750, 'end': 34110, 'confidence': 0.81378, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 34190, 'end': 34382, 'confidence': 0.99392, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 34406, 'end': 34590, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'records.', 'start': 34630, 'end': 35374, 'confidence': 0.99821, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 35502, 'end': 35742, 'confidence': 0.99239, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 35766, 'end': 35950, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'really', 'start': 35990, 'end': 36238, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': \"can't\", 'start': 36294, 'end': 36574, 'confidence': 0.9967, 'speaker': None, 'channel': None}, {'text': 'answer', 'start': 36622, 'end': 37054, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 37102, 'end': 37358, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 37414, 'end': 37582, 'confidence': 0.99962, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 37606, 'end': 37838, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'questions', 'start': 37894, 'end': 38334, 'confidence': 0.99873, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 38382, 'end': 38638, 'confidence': 0.67912, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 38694, 'end': 38910, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 38950, 'end': 39294, 'confidence': 0.99897, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 39382, 'end': 39806, 'confidence': 0.99261, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 39838, 'end': 39982, 'confidence': 0.99732, 'speaker': None, 'channel': None}, {'text': 'tailored', 'start': 40006, 'end': 40286, 'confidence': 0.9928, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 40318, 'end': 40462, 'confidence': 0.99842, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 40486, 'end': 40622, 'confidence': 0.99934, 'speaker': None, 'channel': None}, {'text': 'particular', 'start': 40646, 'end': 40926, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'business.', 'start': 40998, 'end': 41326, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 41398, 'end': 41726, 'confidence': 0.9878, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 41798, 'end': 41982, 'confidence': 0.99939, 'speaker': None, 'channel': None}, {'text': 'lose', 'start': 42006, 'end': 42302, 'confidence': 0.99593, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 42366, 'end': 42542, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 42566, 'end': 42702, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 42726, 'end': 42862, 'confidence': 0.99875, 'speaker': None, 'channel': None}, {'text': 'performance', 'start': 42886, 'end': 43214, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 43302, 'end': 43502, 'confidence': 0.99142, 'speaker': None, 'channel': None}, {'text': 'effectiveness', 'start': 43526, 'end': 44142, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 44206, 'end': 44430, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 44470, 'end': 44622, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'that.', 'start': 44646, 'end': 45022, 'confidence': 0.99677, 'speaker': None, 'channel': None}, {'text': 'Issue', 'start': 45126, 'end': 45486, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'number', 'start': 45558, 'end': 45790, 'confidence': 0.99098, 'speaker': None, 'channel': None}, {'text': 'two', 'start': 45830, 'end': 46030, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 46070, 'end': 46270, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'around', 'start': 46310, 'end': 46558, 'confidence': 0.99851, 'speaker': None, 'channel': None}, {'text': 'hallucinations.', 'start': 46614, 'end': 47646, 'confidence': 0.98398, 'speaker': None, 'channel': None}, {'text': 'These', 'start': 47758, 'end': 48030, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 48070, 'end': 48366, 'confidence': 0.99799, 'speaker': None, 'channel': None}, {'text': 'will', 'start': 48398, 'end': 48542, 'confidence': 0.99816, 'speaker': None, 'channel': None}, {'text': 'give', 'start': 48566, 'end': 48750, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 48790, 'end': 48990, 'confidence': 0.99943, 'speaker': None, 'channel': None}, {'text': 'responses.', 'start': 49030, 'end': 49582, 'confidence': 0.99881, 'speaker': None, 'channel': None}, {'text': 'They', 'start': 49646, 'end': 49870, 'confidence': 0.99913, 'speaker': None, 'channel': None}, {'text': 'look', 'start': 49910, 'end': 50206, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'really', 'start': 50278, 'end': 50558, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'credible,', 'start': 50614, 'end': 50974, 'confidence': 0.99936, 'speaker': None, 'channel': None}, {'text': 'but', 'start': 51022, 'end': 51182, 'confidence': 0.99972, 'speaker': None, 'channel': None}, {'text': \"they're\", 'start': 51206, 'end': 51454, 'confidence': 0.92674, 'speaker': None, 'channel': None}, {'text': 'way', 'start': 51502, 'end': 51710, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'off.', 'start': 51750, 'end': 51950, 'confidence': 0.998, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 51990, 'end': 52142, 'confidence': 0.9933, 'speaker': None, 'channel': None}, {'text': 'if', 'start': 52166, 'end': 52254, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 52262, 'end': 52430, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'run', 'start': 52470, 'end': 52670, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 52710, 'end': 52862, 'confidence': 0.99948, 'speaker': None, 'channel': None}, {'text': 'them,', 'start': 52886, 'end': 53070, 'confidence': 0.99864, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 53110, 'end': 53214, 'confidence': 0.99107, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 53222, 'end': 53438, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'might', 'start': 53494, 'end': 53662, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 53686, 'end': 53822, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 53846, 'end': 54030, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': 'problem.', 'start': 54070, 'end': 54558, 'confidence': 0.99916, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 54694, 'end': 54942, 'confidence': 0.97108, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 54966, 'end': 55102, 'confidence': 0.61985, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 55126, 'end': 55358, 'confidence': 0.9991, 'speaker': None, 'channel': None}, {'text': 'number', 'start': 55414, 'end': 55726, 'confidence': 0.99224, 'speaker': None, 'channel': None}, {'text': 'three,', 'start': 55798, 'end': 56030, 'confidence': 0.99929, 'speaker': None, 'channel': None}, {'text': 'which', 'start': 56070, 'end': 56222, 'confidence': 0.99927, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 56246, 'end': 56430, 'confidence': 0.8425, 'speaker': None, 'channel': None}, {'text': 'becoming', 'start': 56470, 'end': 56782, 'confidence': 0.99673, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 56846, 'end': 56974, 'confidence': 0.99887, 'speaker': None, 'channel': None}, {'text': 'little', 'start': 56982, 'end': 57102, 'confidence': 0.99835, 'speaker': None, 'channel': None}, {'text': 'bit', 'start': 57126, 'end': 57214, 'confidence': 0.99398, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 57222, 'end': 57342, 'confidence': 0.90287, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 57366, 'end': 57598, 'confidence': 0.90365, 'speaker': None, 'channel': None}, {'text': 'less', 'start': 57654, 'end': 57822, 'confidence': 0.99863, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 57846, 'end': 57982, 'confidence': 0.99669, 'speaker': None, 'channel': None}, {'text': 'an', 'start': 58006, 'end': 58094, 'confidence': 0.99907, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 58102, 'end': 58270, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 58310, 'end': 58510, 'confidence': 0.99753, 'speaker': None, 'channel': None}, {'text': 'search.', 'start': 58550, 'end': 59120, 'confidence': 0.97262, 'speaker': None, 'channel': None}, {'text': 'But', 'start': 59270, 'end': 59628, 'confidence': 0.9973, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 59684, 'end': 59900, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 59940, 'end': 60236, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'training', 'start': 60308, 'end': 60652, 'confidence': 0.93588, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 60716, 'end': 61036, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'cutoff', 'start': 61108, 'end': 61564, 'confidence': 0.67416, 'speaker': None, 'channel': None}, {'text': 'dates.', 'start': 61612, 'end': 62028, 'confidence': 0.90929, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 62124, 'end': 62284, 'confidence': 0.98489, 'speaker': None, 'channel': None}, {'text': 'for', 'start': 62292, 'end': 62412, 'confidence': 0.99934, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 62436, 'end': 62572, 'confidence': 0.96572, 'speaker': None, 'channel': None}, {'text': 'while', 'start': 62596, 'end': 62828, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'there', 'start': 62884, 'end': 63244, 'confidence': 0.99507, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 63332, 'end': 63532, 'confidence': 0.99865, 'speaker': None, 'channel': None}, {'text': 'had', 'start': 63556, 'end': 63692, 'confidence': 0.98363, 'speaker': None, 'channel': None}, {'text': 'chatgpt', 'start': 63716, 'end': 64540, 'confidence': 0.7027, 'speaker': None, 'channel': None}, {'text': 'missing', 'start': 64700, 'end': 65356, 'confidence': 0.52967, 'speaker': None, 'channel': None}, {'text': 'months', 'start': 65468, 'end': 65884, 'confidence': 0.99858, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 65972, 'end': 66268, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'training', 'start': 66324, 'end': 66604, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 66652, 'end': 66908, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 66964, 'end': 67228, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 67284, 'end': 67452, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': \"hadn't\", 'start': 67476, 'end': 67708, 'confidence': 0.99567, 'speaker': None, 'channel': None}, {'text': 'been', 'start': 67724, 'end': 67900, 'confidence': 0.99973, 'speaker': None, 'channel': None}, {'text': 'updated', 'start': 67940, 'end': 68380, 'confidence': 0.99944, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 68460, 'end': 68652, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 68676, 'end': 68812, 'confidence': 0.99028, 'speaker': None, 'channel': None}, {'text': 'while', 'start': 68836, 'end': 68972, 'confidence': 0.99902, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 68996, 'end': 69132, 'confidence': 0.98259, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 69156, 'end': 69292, 'confidence': 0.99689, 'speaker': None, 'channel': None}, {'text': 'part', 'start': 69316, 'end': 69452, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 69476, 'end': 69612, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 69636, 'end': 69772, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'takes', 'start': 69796, 'end': 69996, 'confidence': 0.99916, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 70028, 'end': 70124, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 70132, 'end': 70204, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 70212, 'end': 70332, 'confidence': 0.99906, 'speaker': None, 'channel': None}, {'text': 'compute', 'start': 70356, 'end': 70636, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 70668, 'end': 70812, 'confidence': 0.99705, 'speaker': None, 'channel': None}, {'text': 'train', 'start': 70836, 'end': 71228, 'confidence': 0.88696, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 71324, 'end': 71628, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'models.', 'start': 71684, 'end': 72140, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 72220, 'end': 72412, 'confidence': 0.9952, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 72436, 'end': 72524, 'confidence': 0.98723, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 72532, 'end': 72652, 'confidence': 0.9988, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 72676, 'end': 72860, 'confidence': 0.99944, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 72900, 'end': 73100, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'issues', 'start': 73140, 'end': 73580, 'confidence': 0.99689, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 73700, 'end': 73932, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 73956, 'end': 74092, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'keeping', 'start': 74116, 'end': 74444, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 74492, 'end': 74700, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 74740, 'end': 74940, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'getting', 'start': 74980, 'end': 75372, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 75476, 'end': 75788, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 75844, 'end': 76204, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 76292, 'end': 76636, 'confidence': 0.99877, 'speaker': None, 'channel': None}, {'text': 'performance', 'start': 76708, 'end': 77180, 'confidence': 0.99961, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 77300, 'end': 77580, 'confidence': 0.99973, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 77620, 'end': 77820, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'LLMs', 'start': 77860, 'end': 78364, 'confidence': 0.9317, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 78412, 'end': 78572, 'confidence': 0.99976, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 78596, 'end': 78732, 'confidence': 0.99871, 'speaker': None, 'channel': None}, {'text': 'bring', 'start': 78756, 'end': 78940, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'them', 'start': 78980, 'end': 79180, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 79220, 'end': 79420, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'house.', 'start': 79460, 'end': 79708, 'confidence': 0.97514, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 79764, 'end': 79884, 'confidence': 0.97828, 'speaker': None, 'channel': None}, {'text': \"let's\", 'start': 79892, 'end': 80076, 'confidence': 0.91209, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 80108, 'end': 80252, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'about', 'start': 80276, 'end': 80652, 'confidence': 0.99282, 'speaker': None, 'channel': None}, {'text': 'one', 'start': 80756, 'end': 81068, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'pattern', 'start': 81124, 'end': 81484, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 81532, 'end': 81836, 'confidence': 0.99866, 'speaker': None, 'channel': None}, {'text': 'has', 'start': 81908, 'end': 82332, 'confidence': 0.9945, 'speaker': None, 'channel': None}, {'text': 'emerged', 'start': 82436, 'end': 83036, 'confidence': 0.99078, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 83068, 'end': 83260, 'confidence': 0.99755, 'speaker': None, 'channel': None}, {'text': 'being', 'start': 83300, 'end': 83500, 'confidence': 0.99941, 'speaker': None, 'channel': None}, {'text': 'particularly', 'start': 83540, 'end': 83964, 'confidence': 0.99895, 'speaker': None, 'channel': None}, {'text': 'helpful', 'start': 84012, 'end': 84316, 'confidence': 0.99905, 'speaker': None, 'channel': None}, {'text': 'here,', 'start': 84348, 'end': 84894, 'confidence': 0.98289, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 85052, 'end': 85322, 'confidence': 0.98498, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 85346, 'end': 85482, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 85506, 'end': 85786, 'confidence': 0.99891, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 85858, 'end': 86506, 'confidence': 0.77335, 'speaker': None, 'channel': None}, {'text': 'retrieval,', 'start': 86618, 'end': 87546, 'confidence': 0.98123, 'speaker': None, 'channel': None}, {'text': 'augmented', 'start': 87658, 'end': 88538, 'confidence': 0.53074, 'speaker': None, 'channel': None}, {'text': 'generation.', 'start': 88634, 'end': 89290, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'You', 'start': 89370, 'end': 89562, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'may', 'start': 89586, 'end': 89674, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 89682, 'end': 89802, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'heard', 'start': 89826, 'end': 90026, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 90058, 'end': 90250, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'term', 'start': 90290, 'end': 90538, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'toss', 'start': 90594, 'end': 90874, 'confidence': 0.99748, 'speaker': None, 'channel': None}, {'text': 'around,', 'start': 90922, 'end': 91178, 'confidence': 0.99271, 'speaker': None, 'channel': None}, {'text': 'but', 'start': 91234, 'end': 91402, 'confidence': 0.99938, 'speaker': None, 'channel': None}, {'text': \"let's\", 'start': 91426, 'end': 91626, 'confidence': 0.99912, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 91658, 'end': 91802, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'about', 'start': 91826, 'end': 92154, 'confidence': 0.99563, 'speaker': None, 'channel': None}, {'text': \"what's\", 'start': 92242, 'end': 92538, 'confidence': 0.99939, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 92554, 'end': 92682, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'on', 'start': 92706, 'end': 92938, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'here.', 'start': 92994, 'end': 93258, 'confidence': 0.99897, 'speaker': None, 'channel': None}, {'text': 'First,', 'start': 93314, 'end': 93722, 'confidence': 0.99867, 'speaker': None, 'channel': None}, {'text': 'let', 'start': 93826, 'end': 94042, 'confidence': 0.99742, 'speaker': None, 'channel': None}, {'text': 'me', 'start': 94066, 'end': 94154, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'give', 'start': 94162, 'end': 94282, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 94306, 'end': 94442, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'some', 'start': 94466, 'end': 94602, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'context.', 'start': 94626, 'end': 95178, 'confidence': 0.70344, 'speaker': None, 'channel': None}, {'text': \"Here's\", 'start': 95274, 'end': 95626, 'confidence': 0.99802, 'speaker': None, 'channel': None}, {'text': 'what', 'start': 95658, 'end': 95850, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'happens', 'start': 95890, 'end': 96266, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'when', 'start': 96298, 'end': 96586, 'confidence': 0.75484, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 96658, 'end': 96938, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'send', 'start': 96994, 'end': 97322, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 97386, 'end': 97658, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 97714, 'end': 98218, 'confidence': 0.99724, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 98314, 'end': 98522, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 98546, 'end': 98730, 'confidence': 0.9992, 'speaker': None, 'channel': None}, {'text': 'standard', 'start': 98770, 'end': 99114, 'confidence': 0.9985, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 99162, 'end': 99594, 'confidence': 0.99334, 'speaker': None, 'channel': None}, {'text': 'assistant.', 'start': 99642, 'end': 100394, 'confidence': 0.99849, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 100522, 'end': 100858, 'confidence': 0.98196, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 100914, 'end': 101082, 'confidence': 0.99964, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 101106, 'end': 101386, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'your.', 'start': 101458, 'end': 101978, 'confidence': 0.9397, 'speaker': None, 'channel': None}, {'text': 'Your', 'start': 102114, 'end': 102410, 'confidence': 0.98402, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 102450, 'end': 102890, 'confidence': 0.98313, 'speaker': None, 'channel': None}, {'text': 'goes', 'start': 102970, 'end': 103226, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 103258, 'end': 103402, 'confidence': 0.99919, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 103426, 'end': 103562, 'confidence': 0.99794, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 103586, 'end': 103786, 'confidence': 0.64565, 'speaker': None, 'channel': None}, {'text': 'assistant,', 'start': 103818, 'end': 104410, 'confidence': 0.99771, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 104490, 'end': 104682, 'confidence': 0.99358, 'speaker': None, 'channel': None}, {'text': 'generates', 'start': 104706, 'end': 105114, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 105162, 'end': 105274, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 105282, 'end': 105626, 'confidence': 0.99908, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 105658, 'end': 105802, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 105826, 'end': 105962, 'confidence': 0.99529, 'speaker': None, 'channel': None}, {'text': 'gives', 'start': 105986, 'end': 106218, 'confidence': 0.99578, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 106234, 'end': 106410, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'back', 'start': 106450, 'end': 106650, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 106690, 'end': 106794, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'you.', 'start': 106802, 'end': 106970, 'confidence': 0.99915, 'speaker': None, 'channel': None}, {'text': 'Right?', 'start': 107010, 'end': 107546, 'confidence': 0.99317, 'speaker': None, 'channel': None}, {'text': 'In', 'start': 107698, 'end': 108106, 'confidence': 0.99157, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 108178, 'end': 108410, 'confidence': 0.99803, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 108450, 'end': 108810, 'confidence': 0.76677, 'speaker': None, 'channel': None}, {'text': 'implementation,', 'start': 108890, 'end': 109562, 'confidence': 0.96229, 'speaker': None, 'channel': None}, {'text': \"you're\", 'start': 109626, 'end': 109866, 'confidence': 0.99715, 'speaker': None, 'channel': None}, {'text': 'adding', 'start': 109898, 'end': 110282, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'an', 'start': 110346, 'end': 110570, 'confidence': 0.95987, 'speaker': None, 'channel': None}, {'text': 'additional', 'start': 110610, 'end': 111242, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'step', 'start': 111306, 'end': 111626, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'here.', 'start': 111698, 'end': 112212, 'confidence': 0.99461, 'speaker': None, 'channel': None}, {'text': 'Before', 'start': 112346, 'end': 112832, 'confidence': 0.99811, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 112936, 'end': 113248, 'confidence': 0.99491, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 113304, 'end': 113808, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'goes', 'start': 113904, 'end': 114480, 'confidence': 0.99764, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 114560, 'end': 114848, 'confidence': 0.99927, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 114904, 'end': 115120, 'confidence': 0.99885, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 115160, 'end': 115504, 'confidence': 0.51374, 'speaker': None, 'channel': None}, {'text': 'assistant,', 'start': 115552, 'end': 116304, 'confidence': 0.95104, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 116432, 'end': 116672, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 116696, 'end': 116928, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 116984, 'end': 117248, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'search', 'start': 117304, 'end': 117728, 'confidence': 0.98986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 117824, 'end': 118080, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'hits', 'start': 118120, 'end': 118464, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 118512, 'end': 118672, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'corpus', 'start': 118696, 'end': 119104, 'confidence': 0.98243, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 119152, 'end': 119264, 'confidence': 0.9984, 'speaker': None, 'channel': None}, {'text': 'data.', 'start': 119272, 'end': 119632, 'confidence': 0.99849, 'speaker': None, 'channel': None}, {'text': 'Now', 'start': 119736, 'end': 119952, 'confidence': 0.99222, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 119976, 'end': 120112, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 120136, 'end': 120272, 'confidence': 0.99754, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 120296, 'end': 120384, 'confidence': 0.89301, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 120392, 'end': 120464, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 120472, 'end': 120592, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 120616, 'end': 120800, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'data,', 'start': 120840, 'end': 121136, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 121208, 'end': 121440, 'confidence': 0.999, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 121480, 'end': 121680, 'confidence': 0.99943, 'speaker': None, 'channel': None}, {'text': 'documents', 'start': 121720, 'end': 122512, 'confidence': 0.98816, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 122656, 'end': 123344, 'confidence': 0.9881, 'speaker': None, 'channel': None}, {'text': 'other', 'start': 123512, 'end': 123888, 'confidence': 0.99373, 'speaker': None, 'channel': None}, {'text': 'relevant', 'start': 123944, 'end': 124528, 'confidence': 0.99962, 'speaker': None, 'channel': None}, {'text': 'information', 'start': 124624, 'end': 125024, 'confidence': 0.99997, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 125112, 'end': 125312, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 125336, 'end': 125472, 'confidence': 0.99993, 'speaker': None, 'channel': None}, {'text': 'want', 'start': 125496, 'end': 125632, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 125656, 'end': 125744, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'make', 'start': 125752, 'end': 125872, 'confidence': 0.99993, 'speaker': None, 'channel': None}, {'text': 'available', 'start': 125896, 'end': 126224, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 126312, 'end': 126464, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 126472, 'end': 126592, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 126616, 'end': 126896, 'confidence': 0.8474, 'speaker': None, 'channel': None}, {'text': 'assistant.', 'start': 126928, 'end': 127520, 'confidence': 0.99676, 'speaker': None, 'channel': None}, {'text': \"There's\", 'start': 127600, 'end': 127888, 'confidence': 0.985, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 127904, 'end': 127984, 'confidence': 0.99442, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 127992, 'end': 128160, 'confidence': 0.94329, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 128200, 'end': 128496, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 128568, 'end': 128848, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 128904, 'end': 129584, 'confidence': 0.96697, 'speaker': None, 'channel': None}, {'text': \"that's\", 'start': 129632, 'end': 129936, 'confidence': 0.99703, 'speaker': None, 'channel': None}, {'text': 'done', 'start': 129968, 'end': 130304, 'confidence': 0.99821, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 130392, 'end': 130592, 'confidence': 0.98417, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 130616, 'end': 130848, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'context', 'start': 130904, 'end': 131392, 'confidence': 0.95338, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 131456, 'end': 131632, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 131656, 'end': 131840, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 131880, 'end': 131984, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 131992, 'end': 132208, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'added', 'start': 132264, 'end': 132816, 'confidence': 0.99819, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 132968, 'end': 133280, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'addition', 'start': 133320, 'end': 133616, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 133688, 'end': 133920, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 133960, 'end': 134160, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'original', 'start': 134200, 'end': 134752, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'prompt.', 'start': 134816, 'end': 135328, 'confidence': 0.99169, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 135424, 'end': 135728, 'confidence': 0.95505, 'speaker': None, 'channel': None}, {'text': 'so', 'start': 135784, 'end': 136096, 'confidence': 0.99938, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 136168, 'end': 136352, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 136376, 'end': 136560, 'confidence': 0.99812, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 136600, 'end': 136944, 'confidence': 0.95847, 'speaker': None, 'channel': None}, {'text': 'model', 'start': 136992, 'end': 137376, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 137408, 'end': 137552, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 137576, 'end': 137712, 'confidence': 0.99827, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 137736, 'end': 137824, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'get', 'start': 137832, 'end': 137952, 'confidence': 0.9986, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 137976, 'end': 138160, 'confidence': 0.99948, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 138200, 'end': 138544, 'confidence': 0.99894, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 138592, 'end': 138704, 'confidence': 0.99819, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 138712, 'end': 138880, 'confidence': 0.99716, 'speaker': None, 'channel': None}, {'text': 'also', 'start': 138920, 'end': 139264, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 139352, 'end': 139600, 'confidence': 0.99914, 'speaker': None, 'channel': None}, {'text': 'relevant', 'start': 139640, 'end': 140064, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'information', 'start': 140112, 'end': 140608, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 140744, 'end': 141022, 'confidence': 0.38553, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 141056, 'end': 141298, 'confidence': 0.99536, 'speaker': None, 'channel': None}, {'text': 'was', 'start': 141354, 'end': 141762, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'found', 'start': 141866, 'end': 142226, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'during', 'start': 142298, 'end': 142722, 'confidence': 0.99985, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 142826, 'end': 143138, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'process.', 'start': 143194, 'end': 143746, 'confidence': 0.99862, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 143898, 'end': 144162, 'confidence': 0.98826, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 144186, 'end': 144514, 'confidence': 0.9983, 'speaker': None, 'channel': None}, {'text': 'everything', 'start': 144602, 'end': 144898, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'else', 'start': 144954, 'end': 145186, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'proceeds', 'start': 145218, 'end': 145746, 'confidence': 0.81719, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 145778, 'end': 145922, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'same', 'start': 145946, 'end': 146130, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'way.', 'start': 146170, 'end': 146322, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'The', 'start': 146346, 'end': 146482, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 146506, 'end': 146786, 'confidence': 0.93205, 'speaker': None, 'channel': None}, {'text': 'system', 'start': 146818, 'end': 147106, 'confidence': 0.56095, 'speaker': None, 'channel': None}, {'text': 'will', 'start': 147178, 'end': 147362, 'confidence': 0.99839, 'speaker': None, 'channel': None}, {'text': 'process', 'start': 147386, 'end': 147618, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 147674, 'end': 148034, 'confidence': 0.9741, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 148122, 'end': 148418, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'generate', 'start': 148474, 'end': 148962, 'confidence': 0.99905, 'speaker': None, 'channel': None}, {'text': 'typically', 'start': 149026, 'end': 149394, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 149442, 'end': 149650, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'better', 'start': 149690, 'end': 149938, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 149994, 'end': 150434, 'confidence': 0.75474, 'speaker': None, 'channel': None}, {'text': 'for', 'start': 150482, 'end': 150690, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 150730, 'end': 151026, 'confidence': 0.9998, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 151098, 'end': 151378, 'confidence': 0.99923, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 151434, 'end': 151698, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'user.', 'start': 151754, 'end': 152562, 'confidence': 0.99782, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 152706, 'end': 152962, 'confidence': 0.99749, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 152986, 'end': 153122, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 153146, 'end': 153666, 'confidence': 0.91142, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 153698, 'end': 153986, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 154058, 'end': 154338, 'confidence': 0.99941, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 154394, 'end': 154610, 'confidence': 0.99915, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 154650, 'end': 155186, 'confidence': 0.95684, 'speaker': None, 'channel': None}, {'text': 'function', 'start': 155218, 'end': 155506, 'confidence': 0.99932, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 155538, 'end': 155778, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'where', 'start': 155834, 'end': 156002, 'confidence': 0.9853, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 156026, 'end': 156226, 'confidence': 0.53037, 'speaker': None, 'channel': None}, {'text': 'grabbing', 'start': 156258, 'end': 156818, 'confidence': 0.99696, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 156914, 'end': 157074, 'confidence': 0.99853, 'speaker': None, 'channel': None}, {'text': 'information.', 'start': 157082, 'end': 157394, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'The', 'start': 157482, 'end': 157730, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'augmented', 'start': 157770, 'end': 158386, 'confidence': 0.98869, 'speaker': None, 'channel': None}, {'text': 'part', 'start': 158418, 'end': 158610, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 158650, 'end': 158850, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 158890, 'end': 159042, 'confidence': 0.99869, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 159066, 'end': 159266, 'confidence': 0.98866, 'speaker': None, 'channel': None}, {'text': 'augmenting', 'start': 159298, 'end': 159826, 'confidence': 0.99323, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 159858, 'end': 160002, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 160026, 'end': 160114, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 160122, 'end': 160194, 'confidence': 0.9998, 'speaker': None, 'channel': None}, {'text': 'original', 'start': 160202, 'end': 160562, 'confidence': 0.9976, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 160626, 'end': 160962, 'confidence': 0.72399, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 161026, 'end': 161250, 'confidence': 0.59248, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 161290, 'end': 161442, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'generation', 'start': 161466, 'end': 161874, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 161922, 'end': 162082, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 162106, 'end': 162242, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 162266, 'end': 162354, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 162362, 'end': 162546, 'confidence': 0.9947, 'speaker': None, 'channel': None}, {'text': 'generation', 'start': 162578, 'end': 163186, 'confidence': 0.70973, 'speaker': None, 'channel': None}, {'text': 'generating', 'start': 163298, 'end': 163986, 'confidence': 0.99562, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 164098, 'end': 164322, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'same', 'start': 164346, 'end': 164530, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 164570, 'end': 164786, 'confidence': 0.99625, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 164818, 'end': 165058, 'confidence': 0.99757, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 165114, 'end': 165282, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 165306, 'end': 165490, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'LLM.', 'start': 165530, 'end': 166280, 'confidence': 0.92993, 'speaker': None, 'channel': None}, {'text': 'It', 'start': 166410, 'end': 166652, 'confidence': 0.98598, 'speaker': None, 'channel': None}, {'text': 'turns', 'start': 166676, 'end': 166908, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'out', 'start': 166924, 'end': 167052, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 167076, 'end': 167212, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 167236, 'end': 167372, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 167396, 'end': 167580, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 167620, 'end': 167964, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'robust,', 'start': 168052, 'end': 168844, 'confidence': 0.87242, 'speaker': None, 'channel': None}, {'text': 'efficient', 'start': 168972, 'end': 169612, 'confidence': 0.9238, 'speaker': None, 'channel': None}, {'text': 'way', 'start': 169676, 'end': 170044, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 170132, 'end': 170428, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'tackle', 'start': 170484, 'end': 171116, 'confidence': 0.99742, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 171228, 'end': 171500, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'problems', 'start': 171540, 'end': 171868, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 171884, 'end': 172012, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 172036, 'end': 172236, 'confidence': 0.98818, 'speaker': None, 'channel': None}, {'text': 'seeing', 'start': 172268, 'end': 172476, 'confidence': 0.99286, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 172508, 'end': 172700, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'LLMs', 'start': 172740, 'end': 173244, 'confidence': 0.91888, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 173292, 'end': 173452, 'confidence': 0.64282, 'speaker': None, 'channel': None}, {'text': 'industry', 'start': 173476, 'end': 173996, 'confidence': 0.99828, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 174148, 'end': 174556, 'confidence': 0.99779, 'speaker': None, 'channel': None}, {'text': 'business.', 'start': 174628, 'end': 175196, 'confidence': 0.99822, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 175348, 'end': 175564, 'confidence': 0.99535, 'speaker': None, 'channel': None}, {'text': 'I', 'start': 175572, 'end': 175692, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'hope', 'start': 175716, 'end': 175948, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 176004, 'end': 176172, 'confidence': 0.99881, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 176196, 'end': 176332, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'helps', 'start': 176356, 'end': 176684, 'confidence': 0.99932, 'speaker': None, 'channel': None}, {'text': 'explain', 'start': 176732, 'end': 177260, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 177340, 'end': 177676, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 177748, 'end': 178268, 'confidence': 0.9594, 'speaker': None, 'channel': None}, {'text': 'framework.', 'start': 178364, 'end': 178892, 'confidence': 0.99665, 'speaker': None, 'channel': None}, {'text': 'If', 'start': 178956, 'end': 179132, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 179156, 'end': 179244, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 179252, 'end': 179372, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 179396, 'end': 179580, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'questions,', 'start': 179620, 'end': 180172, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 180236, 'end': 180460, 'confidence': 0.99753, 'speaker': None, 'channel': None}, {'text': 'comments', 'start': 180500, 'end': 180796, 'confidence': 0.99908, 'speaker': None, 'channel': None}, {'text': 'if', 'start': 180868, 'end': 181052, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'I', 'start': 181076, 'end': 181212, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'missed', 'start': 181236, 'end': 181516, 'confidence': 0.73098, 'speaker': None, 'channel': None}, {'text': 'anything,', 'start': 181548, 'end': 182108, 'confidence': 0.73069, 'speaker': None, 'channel': None}, {'text': 'please', 'start': 182204, 'end': 182556, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'drop', 'start': 182628, 'end': 182876, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'them', 'start': 182908, 'end': 183052, 'confidence': 0.99788, 'speaker': None, 'channel': None}, {'text': 'below.', 'start': 183076, 'end': 183500, 'confidence': 0.99883, 'speaker': None, 'channel': None}, {'text': 'If', 'start': 183620, 'end': 183852, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': \"you're\", 'start': 183876, 'end': 184076, 'confidence': 0.97379, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 184108, 'end': 184204, 'confidence': 0.96876, 'speaker': None, 'channel': None}, {'text': 'practitioner', 'start': 184212, 'end': 184636, 'confidence': 0.99526, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 184668, 'end': 184764, 'confidence': 0.7334, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 184772, 'end': 184892, 'confidence': 0.98492, 'speaker': None, 'channel': None}, {'text': 'want', 'start': 184916, 'end': 185052, 'confidence': 0.77671, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 185076, 'end': 185212, 'confidence': 0.99902, 'speaker': None, 'channel': None}, {'text': 'add', 'start': 185236, 'end': 185420, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'something', 'start': 185460, 'end': 185660, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 185700, 'end': 185852, 'confidence': 0.99901, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 185876, 'end': 186012, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'conversation,', 'start': 186036, 'end': 186492, 'confidence': 0.99848, 'speaker': None, 'channel': None}, {'text': 'please', 'start': 186556, 'end': 186780, 'confidence': 0.99988, 'speaker': None, 'channel': None}, {'text': 'drop', 'start': 186820, 'end': 187036, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 187068, 'end': 187212, 'confidence': 0.99649, 'speaker': None, 'channel': None}, {'text': 'below', 'start': 187236, 'end': 187420, 'confidence': 0.99976, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 187460, 'end': 187564, 'confidence': 0.99834, 'speaker': None, 'channel': None}, {'text': 'well.', 'start': 187572, 'end': 187692, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'There', 'start': 187716, 'end': 187804, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 187812, 'end': 187884, 'confidence': 0.99067, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 187892, 'end': 187964, 'confidence': 0.99872, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 187972, 'end': 188044, 'confidence': 0.9999, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 188052, 'end': 188124, 'confidence': 0.99834, 'speaker': None, 'channel': None}, {'text': 'people', 'start': 188132, 'end': 188252, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 188276, 'end': 188412, 'confidence': 0.99836, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 188436, 'end': 188524, 'confidence': 0.99876, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 188532, 'end': 188652, 'confidence': 0.90387, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 188676, 'end': 188764, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'watch', 'start': 188772, 'end': 188892, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'this,', 'start': 188916, 'end': 189148, 'confidence': 0.99878, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 189204, 'end': 189372, 'confidence': 0.99528, 'speaker': None, 'channel': None}, {'text': \"I'm\", 'start': 189396, 'end': 189548, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'sure', 'start': 189564, 'end': 189692, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 189716, 'end': 189852, 'confidence': 0.99673, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 189876, 'end': 190012, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'would', 'start': 190036, 'end': 190172, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'benefit', 'start': 190196, 'end': 190732, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 190796, 'end': 191068, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 191124, 'end': 191340, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'expertise', 'start': 191380, 'end': 191884, 'confidence': 0.97094, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 191932, 'end': 192044, 'confidence': 0.99733, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 192052, 'end': 192172, 'confidence': 0.99792, 'speaker': None, 'channel': None}, {'text': 'knowledge.', 'start': 192196, 'end': 192668, 'confidence': 0.99784, 'speaker': None, 'channel': None}, {'text': 'Thanks', 'start': 192764, 'end': 193084, 'confidence': 0.9973, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 193132, 'end': 193436, 'confidence': 0.73072, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 193508, 'end': 193692, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 193716, 'end': 193852, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 193876, 'end': 193964, 'confidence': 0.9965, 'speaker': None, 'channel': None}, {'text': 'soon.', 'start': 193972, 'end': 194020, 'confidence': 0.99913, 'speaker': None, 'channel': None}], 'utterances': None, 'confidence': 0.9741621, 'audio_duration': 195, 'webhook_status_code': None, 'webhook_auth': False, 'summary': None, 'auto_highlights_result': None, 'content_safety_labels': None, 'iab_categories_result': None, 'chapters': None, 'sentiment_analysis_results': None, 'entities': None}, page_content=\"Large language models and AI assistants are taking the consumer world by storm. But what happens when you try to bring these large language models into business, into enterprise? Well, we have three issues, three issues that we have to overcome. Let's walk through them. Number one issue is this lack of domain knowledge. Remember, these large language models have been trained on publicly available data sets. That means that they don't have Access to your SOPs, your standard operating procedures, they don't have access to your own ip, your own records. So they really can't answer a lot of questions and have that response be tailored to your particular business. And you lose a lot of performance and effectiveness because of that. Issue number two is around hallucinations. These models will give you responses. They look really credible, but they're way off. And if you run with them, then you might have a problem. And then issue number three, which is becoming a little bit of a less of an\"),\n",
              " Document(metadata={'language_code': 'en_us', 'audio_url': 'https://cdn.assemblyai.com/upload/0c0fbf9b-b080-48a9-8611-eae24774762d', 'punctuate': True, 'format_text': True, 'dual_channel': None, 'multichannel': None, 'audio_channels': None, 'webhook_url': None, 'webhook_auth_header_name': None, 'webhook_auth_header_value': None, 'audio_start_from': None, 'audio_end_at': None, 'word_boost': [], 'boost_param': None, 'filter_profanity': False, 'redact_pii': False, 'redact_pii_audio': False, 'redact_pii_audio_quality': None, 'redact_pii_policies': None, 'redact_pii_sub': None, 'speaker_labels': False, 'speakers_expected': None, 'content_safety': False, 'content_safety_confidence': None, 'iab_categories': False, 'custom_spelling': None, 'disfluencies': False, 'sentiment_analysis': False, 'auto_chapters': False, 'entity_detection': False, 'summarization': False, 'summary_model': None, 'summary_type': None, 'auto_highlights': False, 'language_detection': False, 'language_confidence_threshold': None, 'language_confidence': None, 'speech_threshold': None, 'speech_model': None, 'id': '656ac874-63b0-48ef-808e-e9c01d6f394f', 'status': <TranscriptStatus.completed: 'completed'>, 'error': None, 'text': \"Large language models and AI assistants are taking the consumer world by storm. But what happens when you try to bring these large language models into business, into enterprise? Well, we have three issues, three issues that we have to overcome. Let's walk through them. Number one issue is this lack of domain knowledge. Remember, these large language models have been trained on publicly available data sets. That means that they don't have Access to your SOPs, your standard operating procedures, they don't have access to your own ip, your own records. So they really can't answer a lot of questions and have that response be tailored to your particular business. And you lose a lot of performance and effectiveness because of that. Issue number two is around hallucinations. These models will give you responses. They look really credible, but they're way off. And if you run with them, then you might have a problem. And then issue number three, which is becoming a little bit of a less of an issue with search. But we have training data cutoff dates. So for a while there you had chatgpt missing months of training data because it hadn't been updated in a while and in part because it takes a lot of compute to train these models. So you have these three issues that are keeping you from getting a lot of performance from your LLMs as you bring them in house. So let's talk about one pattern that has emerged as being particularly helpful here, and that is RAG retrieval, augmented generation. You may have heard this term toss around, but let's talk about what's going on here. First, let me give you some context. Here's what happens when you send a prompt to your standard AI assistant. So you have your. Your prompt goes into the AI assistant, it generates a response and then gives it back to you. Right? In a RAG implementation, you're adding an additional step here. Before that prompt goes into the AI assistant, we have a search that hits a corpus of data. Now this is going to be your data, your own documents and other relevant information that you want to make available to the AI assistant. There's going to be a retrieval that's done and that context is going to be added in addition to your original prompt. And so the large language model is going to get your prompt and then also any relevant information that that was found during this process. And then everything else proceeds the same way. The AI system will process that and generate typically a better response for you as a user. So the retrieval here is this retrieval function here where we're grabbing the information. The augmented part here is we're augmenting that to the original prompt and the generation here is that we're generation generating the same prompt here from the LLM. It turns out that this is a robust, efficient way to tackle these problems that we're seeing with LLMs and industry and business. So I hope that this helps explain the RAG framework. If you have any questions, any comments if I missed anything, please drop them below. If you're a practitioner and you want to add something to this conversation, please drop that below as well. There are a lot of people that are going to watch this, and I'm sure that they would benefit from your expertise and your knowledge. Thanks and talk to you soon.\", 'words': [{'text': 'Large', 'start': 240, 'end': 448, 'confidence': 0.99654, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 464, 'end': 776, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 808, 'end': 1016, 'confidence': 0.99879, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 1048, 'end': 1192, 'confidence': 0.91664, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 1216, 'end': 1496, 'confidence': 0.99191, 'speaker': None, 'channel': None}, {'text': 'assistants', 'start': 1528, 'end': 2200, 'confidence': 0.83958, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 2280, 'end': 2520, 'confidence': 0.99864, 'speaker': None, 'channel': None}, {'text': 'taking', 'start': 2560, 'end': 2856, 'confidence': 0.99972, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 2928, 'end': 3208, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'consumer', 'start': 3264, 'end': 3960, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'world', 'start': 4040, 'end': 4376, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'by', 'start': 4448, 'end': 4824, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'storm.', 'start': 4912, 'end': 5800, 'confidence': 0.99589, 'speaker': None, 'channel': None}, {'text': 'But', 'start': 5960, 'end': 6280, 'confidence': 0.99517, 'speaker': None, 'channel': None}, {'text': 'what', 'start': 6320, 'end': 6520, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'happens', 'start': 6560, 'end': 6856, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'when', 'start': 6888, 'end': 7032, 'confidence': 0.99985, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 7056, 'end': 7192, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'try', 'start': 7216, 'end': 7400, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 7440, 'end': 7592, 'confidence': 0.99887, 'speaker': None, 'channel': None}, {'text': 'bring', 'start': 7616, 'end': 7896, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 7968, 'end': 8152, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 8176, 'end': 8408, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 8464, 'end': 8824, 'confidence': 0.99961, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 8872, 'end': 9304, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 9352, 'end': 9656, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'business,', 'start': 9728, 'end': 10152, 'confidence': 0.99863, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 10256, 'end': 10664, 'confidence': 0.54956, 'speaker': None, 'channel': None}, {'text': 'enterprise?', 'start': 10752, 'end': 11496, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'Well,', 'start': 11608, 'end': 11832, 'confidence': 0.85014, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 11856, 'end': 11992, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 12016, 'end': 12296, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 12368, 'end': 12696, 'confidence': 0.9522, 'speaker': None, 'channel': None}, {'text': 'issues,', 'start': 12768, 'end': 13432, 'confidence': 0.99097, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 13616, 'end': 13960, 'confidence': 0.98567, 'speaker': None, 'channel': None}, {'text': 'issues', 'start': 14000, 'end': 14536, 'confidence': 0.99866, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 14688, 'end': 15048, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 15104, 'end': 15320, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 15360, 'end': 15560, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 15600, 'end': 15752, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'overcome.', 'start': 15776, 'end': 16344, 'confidence': 0.99912, 'speaker': None, 'channel': None}, {'text': \"Let's\", 'start': 16392, 'end': 16616, 'confidence': 0.77592, 'speaker': None, 'channel': None}, {'text': 'walk', 'start': 16648, 'end': 16840, 'confidence': 0.99926, 'speaker': None, 'channel': None}, {'text': 'through', 'start': 16880, 'end': 17080, 'confidence': 0.99726, 'speaker': None, 'channel': None}, {'text': 'them.', 'start': 17120, 'end': 17464, 'confidence': 0.9914, 'speaker': None, 'channel': None}, {'text': 'Number', 'start': 17552, 'end': 17800, 'confidence': 0.97466, 'speaker': None, 'channel': None}, {'text': 'one', 'start': 17840, 'end': 18040, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 18080, 'end': 18568, 'confidence': 0.99624, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 18704, 'end': 19048, 'confidence': 0.99853, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 19104, 'end': 19416, 'confidence': 0.99831, 'speaker': None, 'channel': None}, {'text': 'lack', 'start': 19488, 'end': 19960, 'confidence': 0.94781, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 20040, 'end': 20616, 'confidence': 0.99901, 'speaker': None, 'channel': None}, {'text': 'domain', 'start': 20768, 'end': 21672, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'knowledge.', 'start': 21816, 'end': 22600, 'confidence': 0.87386, 'speaker': None, 'channel': None}, {'text': 'Remember,', 'start': 22760, 'end': 23192, 'confidence': 0.85016, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 23256, 'end': 23528, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 23584, 'end': 23800, 'confidence': 0.99781, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 23840, 'end': 24216, 'confidence': 0.87562, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 24248, 'end': 24504, 'confidence': 0.53107, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 24552, 'end': 24712, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'been', 'start': 24736, 'end': 24872, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'trained', 'start': 24896, 'end': 25144, 'confidence': 0.96243, 'speaker': None, 'channel': None}, {'text': 'on', 'start': 25192, 'end': 25400, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'publicly', 'start': 25440, 'end': 25944, 'confidence': 0.97609, 'speaker': None, 'channel': None}, {'text': 'available', 'start': 25992, 'end': 26344, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 26432, 'end': 26680, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'sets.', 'start': 26720, 'end': 27064, 'confidence': 0.92469, 'speaker': None, 'channel': None}, {'text': 'That', 'start': 27112, 'end': 27272, 'confidence': 0.9986, 'speaker': None, 'channel': None}, {'text': 'means', 'start': 27296, 'end': 27480, 'confidence': 0.9982, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 27520, 'end': 27672, 'confidence': 0.9872, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 27696, 'end': 27832, 'confidence': 0.99926, 'speaker': None, 'channel': None}, {'text': \"don't\", 'start': 27856, 'end': 28056, 'confidence': 0.99771, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 28088, 'end': 28280, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'Access', 'start': 28320, 'end': 28616, 'confidence': 0.99859, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 28688, 'end': 29250, 'confidence': 0.99093, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 29400, 'end': 29758, 'confidence': 0.99267, 'speaker': None, 'channel': None}, {'text': 'SOPs,', 'start': 29814, 'end': 30334, 'confidence': 0.89151, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 30382, 'end': 30542, 'confidence': 0.96304, 'speaker': None, 'channel': None}, {'text': 'standard', 'start': 30566, 'end': 30846, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'operating', 'start': 30878, 'end': 31262, 'confidence': 0.63512, 'speaker': None, 'channel': None}, {'text': 'procedures,', 'start': 31326, 'end': 32046, 'confidence': 0.99754, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 32158, 'end': 32382, 'confidence': 0.99858, 'speaker': None, 'channel': None}, {'text': \"don't\", 'start': 32406, 'end': 32606, 'confidence': 0.9977, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 32638, 'end': 32782, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'access', 'start': 32806, 'end': 33086, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 33158, 'end': 33342, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 33366, 'end': 33502, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 33526, 'end': 33710, 'confidence': 0.99756, 'speaker': None, 'channel': None}, {'text': 'ip,', 'start': 33750, 'end': 34110, 'confidence': 0.81378, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 34190, 'end': 34382, 'confidence': 0.99392, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 34406, 'end': 34590, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'records.', 'start': 34630, 'end': 35374, 'confidence': 0.99821, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 35502, 'end': 35742, 'confidence': 0.99239, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 35766, 'end': 35950, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'really', 'start': 35990, 'end': 36238, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': \"can't\", 'start': 36294, 'end': 36574, 'confidence': 0.9967, 'speaker': None, 'channel': None}, {'text': 'answer', 'start': 36622, 'end': 37054, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 37102, 'end': 37358, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 37414, 'end': 37582, 'confidence': 0.99962, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 37606, 'end': 37838, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'questions', 'start': 37894, 'end': 38334, 'confidence': 0.99873, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 38382, 'end': 38638, 'confidence': 0.67912, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 38694, 'end': 38910, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 38950, 'end': 39294, 'confidence': 0.99897, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 39382, 'end': 39806, 'confidence': 0.99261, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 39838, 'end': 39982, 'confidence': 0.99732, 'speaker': None, 'channel': None}, {'text': 'tailored', 'start': 40006, 'end': 40286, 'confidence': 0.9928, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 40318, 'end': 40462, 'confidence': 0.99842, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 40486, 'end': 40622, 'confidence': 0.99934, 'speaker': None, 'channel': None}, {'text': 'particular', 'start': 40646, 'end': 40926, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'business.', 'start': 40998, 'end': 41326, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 41398, 'end': 41726, 'confidence': 0.9878, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 41798, 'end': 41982, 'confidence': 0.99939, 'speaker': None, 'channel': None}, {'text': 'lose', 'start': 42006, 'end': 42302, 'confidence': 0.99593, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 42366, 'end': 42542, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 42566, 'end': 42702, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 42726, 'end': 42862, 'confidence': 0.99875, 'speaker': None, 'channel': None}, {'text': 'performance', 'start': 42886, 'end': 43214, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 43302, 'end': 43502, 'confidence': 0.99142, 'speaker': None, 'channel': None}, {'text': 'effectiveness', 'start': 43526, 'end': 44142, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 44206, 'end': 44430, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 44470, 'end': 44622, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'that.', 'start': 44646, 'end': 45022, 'confidence': 0.99677, 'speaker': None, 'channel': None}, {'text': 'Issue', 'start': 45126, 'end': 45486, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'number', 'start': 45558, 'end': 45790, 'confidence': 0.99098, 'speaker': None, 'channel': None}, {'text': 'two', 'start': 45830, 'end': 46030, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 46070, 'end': 46270, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'around', 'start': 46310, 'end': 46558, 'confidence': 0.99851, 'speaker': None, 'channel': None}, {'text': 'hallucinations.', 'start': 46614, 'end': 47646, 'confidence': 0.98398, 'speaker': None, 'channel': None}, {'text': 'These', 'start': 47758, 'end': 48030, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 48070, 'end': 48366, 'confidence': 0.99799, 'speaker': None, 'channel': None}, {'text': 'will', 'start': 48398, 'end': 48542, 'confidence': 0.99816, 'speaker': None, 'channel': None}, {'text': 'give', 'start': 48566, 'end': 48750, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 48790, 'end': 48990, 'confidence': 0.99943, 'speaker': None, 'channel': None}, {'text': 'responses.', 'start': 49030, 'end': 49582, 'confidence': 0.99881, 'speaker': None, 'channel': None}, {'text': 'They', 'start': 49646, 'end': 49870, 'confidence': 0.99913, 'speaker': None, 'channel': None}, {'text': 'look', 'start': 49910, 'end': 50206, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'really', 'start': 50278, 'end': 50558, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'credible,', 'start': 50614, 'end': 50974, 'confidence': 0.99936, 'speaker': None, 'channel': None}, {'text': 'but', 'start': 51022, 'end': 51182, 'confidence': 0.99972, 'speaker': None, 'channel': None}, {'text': \"they're\", 'start': 51206, 'end': 51454, 'confidence': 0.92674, 'speaker': None, 'channel': None}, {'text': 'way', 'start': 51502, 'end': 51710, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'off.', 'start': 51750, 'end': 51950, 'confidence': 0.998, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 51990, 'end': 52142, 'confidence': 0.9933, 'speaker': None, 'channel': None}, {'text': 'if', 'start': 52166, 'end': 52254, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 52262, 'end': 52430, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'run', 'start': 52470, 'end': 52670, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 52710, 'end': 52862, 'confidence': 0.99948, 'speaker': None, 'channel': None}, {'text': 'them,', 'start': 52886, 'end': 53070, 'confidence': 0.99864, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 53110, 'end': 53214, 'confidence': 0.99107, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 53222, 'end': 53438, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'might', 'start': 53494, 'end': 53662, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 53686, 'end': 53822, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 53846, 'end': 54030, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': 'problem.', 'start': 54070, 'end': 54558, 'confidence': 0.99916, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 54694, 'end': 54942, 'confidence': 0.97108, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 54966, 'end': 55102, 'confidence': 0.61985, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 55126, 'end': 55358, 'confidence': 0.9991, 'speaker': None, 'channel': None}, {'text': 'number', 'start': 55414, 'end': 55726, 'confidence': 0.99224, 'speaker': None, 'channel': None}, {'text': 'three,', 'start': 55798, 'end': 56030, 'confidence': 0.99929, 'speaker': None, 'channel': None}, {'text': 'which', 'start': 56070, 'end': 56222, 'confidence': 0.99927, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 56246, 'end': 56430, 'confidence': 0.8425, 'speaker': None, 'channel': None}, {'text': 'becoming', 'start': 56470, 'end': 56782, 'confidence': 0.99673, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 56846, 'end': 56974, 'confidence': 0.99887, 'speaker': None, 'channel': None}, {'text': 'little', 'start': 56982, 'end': 57102, 'confidence': 0.99835, 'speaker': None, 'channel': None}, {'text': 'bit', 'start': 57126, 'end': 57214, 'confidence': 0.99398, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 57222, 'end': 57342, 'confidence': 0.90287, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 57366, 'end': 57598, 'confidence': 0.90365, 'speaker': None, 'channel': None}, {'text': 'less', 'start': 57654, 'end': 57822, 'confidence': 0.99863, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 57846, 'end': 57982, 'confidence': 0.99669, 'speaker': None, 'channel': None}, {'text': 'an', 'start': 58006, 'end': 58094, 'confidence': 0.99907, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 58102, 'end': 58270, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 58310, 'end': 58510, 'confidence': 0.99753, 'speaker': None, 'channel': None}, {'text': 'search.', 'start': 58550, 'end': 59120, 'confidence': 0.97262, 'speaker': None, 'channel': None}, {'text': 'But', 'start': 59270, 'end': 59628, 'confidence': 0.9973, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 59684, 'end': 59900, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 59940, 'end': 60236, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'training', 'start': 60308, 'end': 60652, 'confidence': 0.93588, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 60716, 'end': 61036, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'cutoff', 'start': 61108, 'end': 61564, 'confidence': 0.67416, 'speaker': None, 'channel': None}, {'text': 'dates.', 'start': 61612, 'end': 62028, 'confidence': 0.90929, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 62124, 'end': 62284, 'confidence': 0.98489, 'speaker': None, 'channel': None}, {'text': 'for', 'start': 62292, 'end': 62412, 'confidence': 0.99934, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 62436, 'end': 62572, 'confidence': 0.96572, 'speaker': None, 'channel': None}, {'text': 'while', 'start': 62596, 'end': 62828, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'there', 'start': 62884, 'end': 63244, 'confidence': 0.99507, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 63332, 'end': 63532, 'confidence': 0.99865, 'speaker': None, 'channel': None}, {'text': 'had', 'start': 63556, 'end': 63692, 'confidence': 0.98363, 'speaker': None, 'channel': None}, {'text': 'chatgpt', 'start': 63716, 'end': 64540, 'confidence': 0.7027, 'speaker': None, 'channel': None}, {'text': 'missing', 'start': 64700, 'end': 65356, 'confidence': 0.52967, 'speaker': None, 'channel': None}, {'text': 'months', 'start': 65468, 'end': 65884, 'confidence': 0.99858, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 65972, 'end': 66268, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'training', 'start': 66324, 'end': 66604, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 66652, 'end': 66908, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 66964, 'end': 67228, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 67284, 'end': 67452, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': \"hadn't\", 'start': 67476, 'end': 67708, 'confidence': 0.99567, 'speaker': None, 'channel': None}, {'text': 'been', 'start': 67724, 'end': 67900, 'confidence': 0.99973, 'speaker': None, 'channel': None}, {'text': 'updated', 'start': 67940, 'end': 68380, 'confidence': 0.99944, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 68460, 'end': 68652, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 68676, 'end': 68812, 'confidence': 0.99028, 'speaker': None, 'channel': None}, {'text': 'while', 'start': 68836, 'end': 68972, 'confidence': 0.99902, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 68996, 'end': 69132, 'confidence': 0.98259, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 69156, 'end': 69292, 'confidence': 0.99689, 'speaker': None, 'channel': None}, {'text': 'part', 'start': 69316, 'end': 69452, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 69476, 'end': 69612, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 69636, 'end': 69772, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'takes', 'start': 69796, 'end': 69996, 'confidence': 0.99916, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 70028, 'end': 70124, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 70132, 'end': 70204, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 70212, 'end': 70332, 'confidence': 0.99906, 'speaker': None, 'channel': None}, {'text': 'compute', 'start': 70356, 'end': 70636, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 70668, 'end': 70812, 'confidence': 0.99705, 'speaker': None, 'channel': None}, {'text': 'train', 'start': 70836, 'end': 71228, 'confidence': 0.88696, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 71324, 'end': 71628, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'models.', 'start': 71684, 'end': 72140, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 72220, 'end': 72412, 'confidence': 0.9952, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 72436, 'end': 72524, 'confidence': 0.98723, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 72532, 'end': 72652, 'confidence': 0.9988, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 72676, 'end': 72860, 'confidence': 0.99944, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 72900, 'end': 73100, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'issues', 'start': 73140, 'end': 73580, 'confidence': 0.99689, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 73700, 'end': 73932, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 73956, 'end': 74092, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'keeping', 'start': 74116, 'end': 74444, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 74492, 'end': 74700, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 74740, 'end': 74940, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'getting', 'start': 74980, 'end': 75372, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 75476, 'end': 75788, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 75844, 'end': 76204, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 76292, 'end': 76636, 'confidence': 0.99877, 'speaker': None, 'channel': None}, {'text': 'performance', 'start': 76708, 'end': 77180, 'confidence': 0.99961, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 77300, 'end': 77580, 'confidence': 0.99973, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 77620, 'end': 77820, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'LLMs', 'start': 77860, 'end': 78364, 'confidence': 0.9317, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 78412, 'end': 78572, 'confidence': 0.99976, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 78596, 'end': 78732, 'confidence': 0.99871, 'speaker': None, 'channel': None}, {'text': 'bring', 'start': 78756, 'end': 78940, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'them', 'start': 78980, 'end': 79180, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 79220, 'end': 79420, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'house.', 'start': 79460, 'end': 79708, 'confidence': 0.97514, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 79764, 'end': 79884, 'confidence': 0.97828, 'speaker': None, 'channel': None}, {'text': \"let's\", 'start': 79892, 'end': 80076, 'confidence': 0.91209, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 80108, 'end': 80252, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'about', 'start': 80276, 'end': 80652, 'confidence': 0.99282, 'speaker': None, 'channel': None}, {'text': 'one', 'start': 80756, 'end': 81068, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'pattern', 'start': 81124, 'end': 81484, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 81532, 'end': 81836, 'confidence': 0.99866, 'speaker': None, 'channel': None}, {'text': 'has', 'start': 81908, 'end': 82332, 'confidence': 0.9945, 'speaker': None, 'channel': None}, {'text': 'emerged', 'start': 82436, 'end': 83036, 'confidence': 0.99078, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 83068, 'end': 83260, 'confidence': 0.99755, 'speaker': None, 'channel': None}, {'text': 'being', 'start': 83300, 'end': 83500, 'confidence': 0.99941, 'speaker': None, 'channel': None}, {'text': 'particularly', 'start': 83540, 'end': 83964, 'confidence': 0.99895, 'speaker': None, 'channel': None}, {'text': 'helpful', 'start': 84012, 'end': 84316, 'confidence': 0.99905, 'speaker': None, 'channel': None}, {'text': 'here,', 'start': 84348, 'end': 84894, 'confidence': 0.98289, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 85052, 'end': 85322, 'confidence': 0.98498, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 85346, 'end': 85482, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 85506, 'end': 85786, 'confidence': 0.99891, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 85858, 'end': 86506, 'confidence': 0.77335, 'speaker': None, 'channel': None}, {'text': 'retrieval,', 'start': 86618, 'end': 87546, 'confidence': 0.98123, 'speaker': None, 'channel': None}, {'text': 'augmented', 'start': 87658, 'end': 88538, 'confidence': 0.53074, 'speaker': None, 'channel': None}, {'text': 'generation.', 'start': 88634, 'end': 89290, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'You', 'start': 89370, 'end': 89562, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'may', 'start': 89586, 'end': 89674, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 89682, 'end': 89802, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'heard', 'start': 89826, 'end': 90026, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 90058, 'end': 90250, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'term', 'start': 90290, 'end': 90538, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'toss', 'start': 90594, 'end': 90874, 'confidence': 0.99748, 'speaker': None, 'channel': None}, {'text': 'around,', 'start': 90922, 'end': 91178, 'confidence': 0.99271, 'speaker': None, 'channel': None}, {'text': 'but', 'start': 91234, 'end': 91402, 'confidence': 0.99938, 'speaker': None, 'channel': None}, {'text': \"let's\", 'start': 91426, 'end': 91626, 'confidence': 0.99912, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 91658, 'end': 91802, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'about', 'start': 91826, 'end': 92154, 'confidence': 0.99563, 'speaker': None, 'channel': None}, {'text': \"what's\", 'start': 92242, 'end': 92538, 'confidence': 0.99939, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 92554, 'end': 92682, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'on', 'start': 92706, 'end': 92938, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'here.', 'start': 92994, 'end': 93258, 'confidence': 0.99897, 'speaker': None, 'channel': None}, {'text': 'First,', 'start': 93314, 'end': 93722, 'confidence': 0.99867, 'speaker': None, 'channel': None}, {'text': 'let', 'start': 93826, 'end': 94042, 'confidence': 0.99742, 'speaker': None, 'channel': None}, {'text': 'me', 'start': 94066, 'end': 94154, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'give', 'start': 94162, 'end': 94282, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 94306, 'end': 94442, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'some', 'start': 94466, 'end': 94602, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'context.', 'start': 94626, 'end': 95178, 'confidence': 0.70344, 'speaker': None, 'channel': None}, {'text': \"Here's\", 'start': 95274, 'end': 95626, 'confidence': 0.99802, 'speaker': None, 'channel': None}, {'text': 'what', 'start': 95658, 'end': 95850, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'happens', 'start': 95890, 'end': 96266, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'when', 'start': 96298, 'end': 96586, 'confidence': 0.75484, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 96658, 'end': 96938, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'send', 'start': 96994, 'end': 97322, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 97386, 'end': 97658, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 97714, 'end': 98218, 'confidence': 0.99724, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 98314, 'end': 98522, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 98546, 'end': 98730, 'confidence': 0.9992, 'speaker': None, 'channel': None}, {'text': 'standard', 'start': 98770, 'end': 99114, 'confidence': 0.9985, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 99162, 'end': 99594, 'confidence': 0.99334, 'speaker': None, 'channel': None}, {'text': 'assistant.', 'start': 99642, 'end': 100394, 'confidence': 0.99849, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 100522, 'end': 100858, 'confidence': 0.98196, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 100914, 'end': 101082, 'confidence': 0.99964, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 101106, 'end': 101386, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'your.', 'start': 101458, 'end': 101978, 'confidence': 0.9397, 'speaker': None, 'channel': None}, {'text': 'Your', 'start': 102114, 'end': 102410, 'confidence': 0.98402, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 102450, 'end': 102890, 'confidence': 0.98313, 'speaker': None, 'channel': None}, {'text': 'goes', 'start': 102970, 'end': 103226, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 103258, 'end': 103402, 'confidence': 0.99919, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 103426, 'end': 103562, 'confidence': 0.99794, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 103586, 'end': 103786, 'confidence': 0.64565, 'speaker': None, 'channel': None}, {'text': 'assistant,', 'start': 103818, 'end': 104410, 'confidence': 0.99771, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 104490, 'end': 104682, 'confidence': 0.99358, 'speaker': None, 'channel': None}, {'text': 'generates', 'start': 104706, 'end': 105114, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 105162, 'end': 105274, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 105282, 'end': 105626, 'confidence': 0.99908, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 105658, 'end': 105802, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 105826, 'end': 105962, 'confidence': 0.99529, 'speaker': None, 'channel': None}, {'text': 'gives', 'start': 105986, 'end': 106218, 'confidence': 0.99578, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 106234, 'end': 106410, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'back', 'start': 106450, 'end': 106650, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 106690, 'end': 106794, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'you.', 'start': 106802, 'end': 106970, 'confidence': 0.99915, 'speaker': None, 'channel': None}, {'text': 'Right?', 'start': 107010, 'end': 107546, 'confidence': 0.99317, 'speaker': None, 'channel': None}, {'text': 'In', 'start': 107698, 'end': 108106, 'confidence': 0.99157, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 108178, 'end': 108410, 'confidence': 0.99803, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 108450, 'end': 108810, 'confidence': 0.76677, 'speaker': None, 'channel': None}, {'text': 'implementation,', 'start': 108890, 'end': 109562, 'confidence': 0.96229, 'speaker': None, 'channel': None}, {'text': \"you're\", 'start': 109626, 'end': 109866, 'confidence': 0.99715, 'speaker': None, 'channel': None}, {'text': 'adding', 'start': 109898, 'end': 110282, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'an', 'start': 110346, 'end': 110570, 'confidence': 0.95987, 'speaker': None, 'channel': None}, {'text': 'additional', 'start': 110610, 'end': 111242, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'step', 'start': 111306, 'end': 111626, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'here.', 'start': 111698, 'end': 112212, 'confidence': 0.99461, 'speaker': None, 'channel': None}, {'text': 'Before', 'start': 112346, 'end': 112832, 'confidence': 0.99811, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 112936, 'end': 113248, 'confidence': 0.99491, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 113304, 'end': 113808, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'goes', 'start': 113904, 'end': 114480, 'confidence': 0.99764, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 114560, 'end': 114848, 'confidence': 0.99927, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 114904, 'end': 115120, 'confidence': 0.99885, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 115160, 'end': 115504, 'confidence': 0.51374, 'speaker': None, 'channel': None}, {'text': 'assistant,', 'start': 115552, 'end': 116304, 'confidence': 0.95104, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 116432, 'end': 116672, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 116696, 'end': 116928, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 116984, 'end': 117248, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'search', 'start': 117304, 'end': 117728, 'confidence': 0.98986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 117824, 'end': 118080, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'hits', 'start': 118120, 'end': 118464, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 118512, 'end': 118672, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'corpus', 'start': 118696, 'end': 119104, 'confidence': 0.98243, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 119152, 'end': 119264, 'confidence': 0.9984, 'speaker': None, 'channel': None}, {'text': 'data.', 'start': 119272, 'end': 119632, 'confidence': 0.99849, 'speaker': None, 'channel': None}, {'text': 'Now', 'start': 119736, 'end': 119952, 'confidence': 0.99222, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 119976, 'end': 120112, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 120136, 'end': 120272, 'confidence': 0.99754, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 120296, 'end': 120384, 'confidence': 0.89301, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 120392, 'end': 120464, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 120472, 'end': 120592, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 120616, 'end': 120800, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'data,', 'start': 120840, 'end': 121136, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 121208, 'end': 121440, 'confidence': 0.999, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 121480, 'end': 121680, 'confidence': 0.99943, 'speaker': None, 'channel': None}, {'text': 'documents', 'start': 121720, 'end': 122512, 'confidence': 0.98816, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 122656, 'end': 123344, 'confidence': 0.9881, 'speaker': None, 'channel': None}, {'text': 'other', 'start': 123512, 'end': 123888, 'confidence': 0.99373, 'speaker': None, 'channel': None}, {'text': 'relevant', 'start': 123944, 'end': 124528, 'confidence': 0.99962, 'speaker': None, 'channel': None}, {'text': 'information', 'start': 124624, 'end': 125024, 'confidence': 0.99997, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 125112, 'end': 125312, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 125336, 'end': 125472, 'confidence': 0.99993, 'speaker': None, 'channel': None}, {'text': 'want', 'start': 125496, 'end': 125632, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 125656, 'end': 125744, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'make', 'start': 125752, 'end': 125872, 'confidence': 0.99993, 'speaker': None, 'channel': None}, {'text': 'available', 'start': 125896, 'end': 126224, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 126312, 'end': 126464, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 126472, 'end': 126592, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 126616, 'end': 126896, 'confidence': 0.8474, 'speaker': None, 'channel': None}, {'text': 'assistant.', 'start': 126928, 'end': 127520, 'confidence': 0.99676, 'speaker': None, 'channel': None}, {'text': \"There's\", 'start': 127600, 'end': 127888, 'confidence': 0.985, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 127904, 'end': 127984, 'confidence': 0.99442, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 127992, 'end': 128160, 'confidence': 0.94329, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 128200, 'end': 128496, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 128568, 'end': 128848, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 128904, 'end': 129584, 'confidence': 0.96697, 'speaker': None, 'channel': None}, {'text': \"that's\", 'start': 129632, 'end': 129936, 'confidence': 0.99703, 'speaker': None, 'channel': None}, {'text': 'done', 'start': 129968, 'end': 130304, 'confidence': 0.99821, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 130392, 'end': 130592, 'confidence': 0.98417, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 130616, 'end': 130848, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'context', 'start': 130904, 'end': 131392, 'confidence': 0.95338, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 131456, 'end': 131632, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 131656, 'end': 131840, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 131880, 'end': 131984, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 131992, 'end': 132208, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'added', 'start': 132264, 'end': 132816, 'confidence': 0.99819, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 132968, 'end': 133280, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'addition', 'start': 133320, 'end': 133616, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 133688, 'end': 133920, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 133960, 'end': 134160, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'original', 'start': 134200, 'end': 134752, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'prompt.', 'start': 134816, 'end': 135328, 'confidence': 0.99169, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 135424, 'end': 135728, 'confidence': 0.95505, 'speaker': None, 'channel': None}, {'text': 'so', 'start': 135784, 'end': 136096, 'confidence': 0.99938, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 136168, 'end': 136352, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 136376, 'end': 136560, 'confidence': 0.99812, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 136600, 'end': 136944, 'confidence': 0.95847, 'speaker': None, 'channel': None}, {'text': 'model', 'start': 136992, 'end': 137376, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 137408, 'end': 137552, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 137576, 'end': 137712, 'confidence': 0.99827, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 137736, 'end': 137824, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'get', 'start': 137832, 'end': 137952, 'confidence': 0.9986, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 137976, 'end': 138160, 'confidence': 0.99948, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 138200, 'end': 138544, 'confidence': 0.99894, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 138592, 'end': 138704, 'confidence': 0.99819, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 138712, 'end': 138880, 'confidence': 0.99716, 'speaker': None, 'channel': None}, {'text': 'also', 'start': 138920, 'end': 139264, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 139352, 'end': 139600, 'confidence': 0.99914, 'speaker': None, 'channel': None}, {'text': 'relevant', 'start': 139640, 'end': 140064, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'information', 'start': 140112, 'end': 140608, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 140744, 'end': 141022, 'confidence': 0.38553, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 141056, 'end': 141298, 'confidence': 0.99536, 'speaker': None, 'channel': None}, {'text': 'was', 'start': 141354, 'end': 141762, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'found', 'start': 141866, 'end': 142226, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'during', 'start': 142298, 'end': 142722, 'confidence': 0.99985, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 142826, 'end': 143138, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'process.', 'start': 143194, 'end': 143746, 'confidence': 0.99862, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 143898, 'end': 144162, 'confidence': 0.98826, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 144186, 'end': 144514, 'confidence': 0.9983, 'speaker': None, 'channel': None}, {'text': 'everything', 'start': 144602, 'end': 144898, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'else', 'start': 144954, 'end': 145186, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'proceeds', 'start': 145218, 'end': 145746, 'confidence': 0.81719, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 145778, 'end': 145922, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'same', 'start': 145946, 'end': 146130, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'way.', 'start': 146170, 'end': 146322, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'The', 'start': 146346, 'end': 146482, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 146506, 'end': 146786, 'confidence': 0.93205, 'speaker': None, 'channel': None}, {'text': 'system', 'start': 146818, 'end': 147106, 'confidence': 0.56095, 'speaker': None, 'channel': None}, {'text': 'will', 'start': 147178, 'end': 147362, 'confidence': 0.99839, 'speaker': None, 'channel': None}, {'text': 'process', 'start': 147386, 'end': 147618, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 147674, 'end': 148034, 'confidence': 0.9741, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 148122, 'end': 148418, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'generate', 'start': 148474, 'end': 148962, 'confidence': 0.99905, 'speaker': None, 'channel': None}, {'text': 'typically', 'start': 149026, 'end': 149394, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 149442, 'end': 149650, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'better', 'start': 149690, 'end': 149938, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 149994, 'end': 150434, 'confidence': 0.75474, 'speaker': None, 'channel': None}, {'text': 'for', 'start': 150482, 'end': 150690, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 150730, 'end': 151026, 'confidence': 0.9998, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 151098, 'end': 151378, 'confidence': 0.99923, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 151434, 'end': 151698, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'user.', 'start': 151754, 'end': 152562, 'confidence': 0.99782, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 152706, 'end': 152962, 'confidence': 0.99749, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 152986, 'end': 153122, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 153146, 'end': 153666, 'confidence': 0.91142, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 153698, 'end': 153986, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 154058, 'end': 154338, 'confidence': 0.99941, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 154394, 'end': 154610, 'confidence': 0.99915, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 154650, 'end': 155186, 'confidence': 0.95684, 'speaker': None, 'channel': None}, {'text': 'function', 'start': 155218, 'end': 155506, 'confidence': 0.99932, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 155538, 'end': 155778, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'where', 'start': 155834, 'end': 156002, 'confidence': 0.9853, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 156026, 'end': 156226, 'confidence': 0.53037, 'speaker': None, 'channel': None}, {'text': 'grabbing', 'start': 156258, 'end': 156818, 'confidence': 0.99696, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 156914, 'end': 157074, 'confidence': 0.99853, 'speaker': None, 'channel': None}, {'text': 'information.', 'start': 157082, 'end': 157394, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'The', 'start': 157482, 'end': 157730, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'augmented', 'start': 157770, 'end': 158386, 'confidence': 0.98869, 'speaker': None, 'channel': None}, {'text': 'part', 'start': 158418, 'end': 158610, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 158650, 'end': 158850, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 158890, 'end': 159042, 'confidence': 0.99869, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 159066, 'end': 159266, 'confidence': 0.98866, 'speaker': None, 'channel': None}, {'text': 'augmenting', 'start': 159298, 'end': 159826, 'confidence': 0.99323, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 159858, 'end': 160002, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 160026, 'end': 160114, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 160122, 'end': 160194, 'confidence': 0.9998, 'speaker': None, 'channel': None}, {'text': 'original', 'start': 160202, 'end': 160562, 'confidence': 0.9976, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 160626, 'end': 160962, 'confidence': 0.72399, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 161026, 'end': 161250, 'confidence': 0.59248, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 161290, 'end': 161442, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'generation', 'start': 161466, 'end': 161874, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 161922, 'end': 162082, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 162106, 'end': 162242, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 162266, 'end': 162354, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 162362, 'end': 162546, 'confidence': 0.9947, 'speaker': None, 'channel': None}, {'text': 'generation', 'start': 162578, 'end': 163186, 'confidence': 0.70973, 'speaker': None, 'channel': None}, {'text': 'generating', 'start': 163298, 'end': 163986, 'confidence': 0.99562, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 164098, 'end': 164322, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'same', 'start': 164346, 'end': 164530, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 164570, 'end': 164786, 'confidence': 0.99625, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 164818, 'end': 165058, 'confidence': 0.99757, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 165114, 'end': 165282, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 165306, 'end': 165490, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'LLM.', 'start': 165530, 'end': 166280, 'confidence': 0.92993, 'speaker': None, 'channel': None}, {'text': 'It', 'start': 166410, 'end': 166652, 'confidence': 0.98598, 'speaker': None, 'channel': None}, {'text': 'turns', 'start': 166676, 'end': 166908, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'out', 'start': 166924, 'end': 167052, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 167076, 'end': 167212, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 167236, 'end': 167372, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 167396, 'end': 167580, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 167620, 'end': 167964, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'robust,', 'start': 168052, 'end': 168844, 'confidence': 0.87242, 'speaker': None, 'channel': None}, {'text': 'efficient', 'start': 168972, 'end': 169612, 'confidence': 0.9238, 'speaker': None, 'channel': None}, {'text': 'way', 'start': 169676, 'end': 170044, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 170132, 'end': 170428, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'tackle', 'start': 170484, 'end': 171116, 'confidence': 0.99742, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 171228, 'end': 171500, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'problems', 'start': 171540, 'end': 171868, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 171884, 'end': 172012, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 172036, 'end': 172236, 'confidence': 0.98818, 'speaker': None, 'channel': None}, {'text': 'seeing', 'start': 172268, 'end': 172476, 'confidence': 0.99286, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 172508, 'end': 172700, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'LLMs', 'start': 172740, 'end': 173244, 'confidence': 0.91888, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 173292, 'end': 173452, 'confidence': 0.64282, 'speaker': None, 'channel': None}, {'text': 'industry', 'start': 173476, 'end': 173996, 'confidence': 0.99828, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 174148, 'end': 174556, 'confidence': 0.99779, 'speaker': None, 'channel': None}, {'text': 'business.', 'start': 174628, 'end': 175196, 'confidence': 0.99822, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 175348, 'end': 175564, 'confidence': 0.99535, 'speaker': None, 'channel': None}, {'text': 'I', 'start': 175572, 'end': 175692, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'hope', 'start': 175716, 'end': 175948, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 176004, 'end': 176172, 'confidence': 0.99881, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 176196, 'end': 176332, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'helps', 'start': 176356, 'end': 176684, 'confidence': 0.99932, 'speaker': None, 'channel': None}, {'text': 'explain', 'start': 176732, 'end': 177260, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 177340, 'end': 177676, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 177748, 'end': 178268, 'confidence': 0.9594, 'speaker': None, 'channel': None}, {'text': 'framework.', 'start': 178364, 'end': 178892, 'confidence': 0.99665, 'speaker': None, 'channel': None}, {'text': 'If', 'start': 178956, 'end': 179132, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 179156, 'end': 179244, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 179252, 'end': 179372, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 179396, 'end': 179580, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'questions,', 'start': 179620, 'end': 180172, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 180236, 'end': 180460, 'confidence': 0.99753, 'speaker': None, 'channel': None}, {'text': 'comments', 'start': 180500, 'end': 180796, 'confidence': 0.99908, 'speaker': None, 'channel': None}, {'text': 'if', 'start': 180868, 'end': 181052, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'I', 'start': 181076, 'end': 181212, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'missed', 'start': 181236, 'end': 181516, 'confidence': 0.73098, 'speaker': None, 'channel': None}, {'text': 'anything,', 'start': 181548, 'end': 182108, 'confidence': 0.73069, 'speaker': None, 'channel': None}, {'text': 'please', 'start': 182204, 'end': 182556, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'drop', 'start': 182628, 'end': 182876, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'them', 'start': 182908, 'end': 183052, 'confidence': 0.99788, 'speaker': None, 'channel': None}, {'text': 'below.', 'start': 183076, 'end': 183500, 'confidence': 0.99883, 'speaker': None, 'channel': None}, {'text': 'If', 'start': 183620, 'end': 183852, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': \"you're\", 'start': 183876, 'end': 184076, 'confidence': 0.97379, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 184108, 'end': 184204, 'confidence': 0.96876, 'speaker': None, 'channel': None}, {'text': 'practitioner', 'start': 184212, 'end': 184636, 'confidence': 0.99526, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 184668, 'end': 184764, 'confidence': 0.7334, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 184772, 'end': 184892, 'confidence': 0.98492, 'speaker': None, 'channel': None}, {'text': 'want', 'start': 184916, 'end': 185052, 'confidence': 0.77671, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 185076, 'end': 185212, 'confidence': 0.99902, 'speaker': None, 'channel': None}, {'text': 'add', 'start': 185236, 'end': 185420, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'something', 'start': 185460, 'end': 185660, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 185700, 'end': 185852, 'confidence': 0.99901, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 185876, 'end': 186012, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'conversation,', 'start': 186036, 'end': 186492, 'confidence': 0.99848, 'speaker': None, 'channel': None}, {'text': 'please', 'start': 186556, 'end': 186780, 'confidence': 0.99988, 'speaker': None, 'channel': None}, {'text': 'drop', 'start': 186820, 'end': 187036, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 187068, 'end': 187212, 'confidence': 0.99649, 'speaker': None, 'channel': None}, {'text': 'below', 'start': 187236, 'end': 187420, 'confidence': 0.99976, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 187460, 'end': 187564, 'confidence': 0.99834, 'speaker': None, 'channel': None}, {'text': 'well.', 'start': 187572, 'end': 187692, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'There', 'start': 187716, 'end': 187804, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 187812, 'end': 187884, 'confidence': 0.99067, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 187892, 'end': 187964, 'confidence': 0.99872, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 187972, 'end': 188044, 'confidence': 0.9999, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 188052, 'end': 188124, 'confidence': 0.99834, 'speaker': None, 'channel': None}, {'text': 'people', 'start': 188132, 'end': 188252, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 188276, 'end': 188412, 'confidence': 0.99836, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 188436, 'end': 188524, 'confidence': 0.99876, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 188532, 'end': 188652, 'confidence': 0.90387, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 188676, 'end': 188764, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'watch', 'start': 188772, 'end': 188892, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'this,', 'start': 188916, 'end': 189148, 'confidence': 0.99878, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 189204, 'end': 189372, 'confidence': 0.99528, 'speaker': None, 'channel': None}, {'text': \"I'm\", 'start': 189396, 'end': 189548, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'sure', 'start': 189564, 'end': 189692, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 189716, 'end': 189852, 'confidence': 0.99673, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 189876, 'end': 190012, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'would', 'start': 190036, 'end': 190172, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'benefit', 'start': 190196, 'end': 190732, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 190796, 'end': 191068, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 191124, 'end': 191340, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'expertise', 'start': 191380, 'end': 191884, 'confidence': 0.97094, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 191932, 'end': 192044, 'confidence': 0.99733, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 192052, 'end': 192172, 'confidence': 0.99792, 'speaker': None, 'channel': None}, {'text': 'knowledge.', 'start': 192196, 'end': 192668, 'confidence': 0.99784, 'speaker': None, 'channel': None}, {'text': 'Thanks', 'start': 192764, 'end': 193084, 'confidence': 0.9973, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 193132, 'end': 193436, 'confidence': 0.73072, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 193508, 'end': 193692, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 193716, 'end': 193852, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 193876, 'end': 193964, 'confidence': 0.9965, 'speaker': None, 'channel': None}, {'text': 'soon.', 'start': 193972, 'end': 194020, 'confidence': 0.99913, 'speaker': None, 'channel': None}], 'utterances': None, 'confidence': 0.9741621, 'audio_duration': 195, 'webhook_status_code': None, 'webhook_auth': False, 'summary': None, 'auto_highlights_result': None, 'content_safety_labels': None, 'iab_categories_result': None, 'chapters': None, 'sentiment_analysis_results': None, 'entities': None}, page_content=\"might have a problem. And then issue number three, which is becoming a little bit of a less of an issue with search. But we have training data cutoff dates. So for a while there you had chatgpt missing months of training data because it hadn't been updated in a while and in part because it takes a lot of compute to train these models. So you have these three issues that are keeping you from getting a lot of performance from your LLMs as you bring them in house. So let's talk about one pattern that has emerged as being particularly helpful here, and that is RAG retrieval, augmented generation. You may have heard this term toss around, but let's talk about what's going on here. First, let me give you some context. Here's what happens when you send a prompt to your standard AI assistant. So you have your. Your prompt goes into the AI assistant, it generates a response and then gives it back to you. Right? In a RAG implementation, you're adding an additional step here. Before that prompt\"),\n",
              " Document(metadata={'language_code': 'en_us', 'audio_url': 'https://cdn.assemblyai.com/upload/0c0fbf9b-b080-48a9-8611-eae24774762d', 'punctuate': True, 'format_text': True, 'dual_channel': None, 'multichannel': None, 'audio_channels': None, 'webhook_url': None, 'webhook_auth_header_name': None, 'webhook_auth_header_value': None, 'audio_start_from': None, 'audio_end_at': None, 'word_boost': [], 'boost_param': None, 'filter_profanity': False, 'redact_pii': False, 'redact_pii_audio': False, 'redact_pii_audio_quality': None, 'redact_pii_policies': None, 'redact_pii_sub': None, 'speaker_labels': False, 'speakers_expected': None, 'content_safety': False, 'content_safety_confidence': None, 'iab_categories': False, 'custom_spelling': None, 'disfluencies': False, 'sentiment_analysis': False, 'auto_chapters': False, 'entity_detection': False, 'summarization': False, 'summary_model': None, 'summary_type': None, 'auto_highlights': False, 'language_detection': False, 'language_confidence_threshold': None, 'language_confidence': None, 'speech_threshold': None, 'speech_model': None, 'id': '656ac874-63b0-48ef-808e-e9c01d6f394f', 'status': <TranscriptStatus.completed: 'completed'>, 'error': None, 'text': \"Large language models and AI assistants are taking the consumer world by storm. But what happens when you try to bring these large language models into business, into enterprise? Well, we have three issues, three issues that we have to overcome. Let's walk through them. Number one issue is this lack of domain knowledge. Remember, these large language models have been trained on publicly available data sets. That means that they don't have Access to your SOPs, your standard operating procedures, they don't have access to your own ip, your own records. So they really can't answer a lot of questions and have that response be tailored to your particular business. And you lose a lot of performance and effectiveness because of that. Issue number two is around hallucinations. These models will give you responses. They look really credible, but they're way off. And if you run with them, then you might have a problem. And then issue number three, which is becoming a little bit of a less of an issue with search. But we have training data cutoff dates. So for a while there you had chatgpt missing months of training data because it hadn't been updated in a while and in part because it takes a lot of compute to train these models. So you have these three issues that are keeping you from getting a lot of performance from your LLMs as you bring them in house. So let's talk about one pattern that has emerged as being particularly helpful here, and that is RAG retrieval, augmented generation. You may have heard this term toss around, but let's talk about what's going on here. First, let me give you some context. Here's what happens when you send a prompt to your standard AI assistant. So you have your. Your prompt goes into the AI assistant, it generates a response and then gives it back to you. Right? In a RAG implementation, you're adding an additional step here. Before that prompt goes into the AI assistant, we have a search that hits a corpus of data. Now this is going to be your data, your own documents and other relevant information that you want to make available to the AI assistant. There's going to be a retrieval that's done and that context is going to be added in addition to your original prompt. And so the large language model is going to get your prompt and then also any relevant information that that was found during this process. And then everything else proceeds the same way. The AI system will process that and generate typically a better response for you as a user. So the retrieval here is this retrieval function here where we're grabbing the information. The augmented part here is we're augmenting that to the original prompt and the generation here is that we're generation generating the same prompt here from the LLM. It turns out that this is a robust, efficient way to tackle these problems that we're seeing with LLMs and industry and business. So I hope that this helps explain the RAG framework. If you have any questions, any comments if I missed anything, please drop them below. If you're a practitioner and you want to add something to this conversation, please drop that below as well. There are a lot of people that are going to watch this, and I'm sure that they would benefit from your expertise and your knowledge. Thanks and talk to you soon.\", 'words': [{'text': 'Large', 'start': 240, 'end': 448, 'confidence': 0.99654, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 464, 'end': 776, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 808, 'end': 1016, 'confidence': 0.99879, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 1048, 'end': 1192, 'confidence': 0.91664, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 1216, 'end': 1496, 'confidence': 0.99191, 'speaker': None, 'channel': None}, {'text': 'assistants', 'start': 1528, 'end': 2200, 'confidence': 0.83958, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 2280, 'end': 2520, 'confidence': 0.99864, 'speaker': None, 'channel': None}, {'text': 'taking', 'start': 2560, 'end': 2856, 'confidence': 0.99972, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 2928, 'end': 3208, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'consumer', 'start': 3264, 'end': 3960, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'world', 'start': 4040, 'end': 4376, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'by', 'start': 4448, 'end': 4824, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'storm.', 'start': 4912, 'end': 5800, 'confidence': 0.99589, 'speaker': None, 'channel': None}, {'text': 'But', 'start': 5960, 'end': 6280, 'confidence': 0.99517, 'speaker': None, 'channel': None}, {'text': 'what', 'start': 6320, 'end': 6520, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'happens', 'start': 6560, 'end': 6856, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'when', 'start': 6888, 'end': 7032, 'confidence': 0.99985, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 7056, 'end': 7192, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'try', 'start': 7216, 'end': 7400, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 7440, 'end': 7592, 'confidence': 0.99887, 'speaker': None, 'channel': None}, {'text': 'bring', 'start': 7616, 'end': 7896, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 7968, 'end': 8152, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 8176, 'end': 8408, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 8464, 'end': 8824, 'confidence': 0.99961, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 8872, 'end': 9304, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 9352, 'end': 9656, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'business,', 'start': 9728, 'end': 10152, 'confidence': 0.99863, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 10256, 'end': 10664, 'confidence': 0.54956, 'speaker': None, 'channel': None}, {'text': 'enterprise?', 'start': 10752, 'end': 11496, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'Well,', 'start': 11608, 'end': 11832, 'confidence': 0.85014, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 11856, 'end': 11992, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 12016, 'end': 12296, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 12368, 'end': 12696, 'confidence': 0.9522, 'speaker': None, 'channel': None}, {'text': 'issues,', 'start': 12768, 'end': 13432, 'confidence': 0.99097, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 13616, 'end': 13960, 'confidence': 0.98567, 'speaker': None, 'channel': None}, {'text': 'issues', 'start': 14000, 'end': 14536, 'confidence': 0.99866, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 14688, 'end': 15048, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 15104, 'end': 15320, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 15360, 'end': 15560, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 15600, 'end': 15752, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'overcome.', 'start': 15776, 'end': 16344, 'confidence': 0.99912, 'speaker': None, 'channel': None}, {'text': \"Let's\", 'start': 16392, 'end': 16616, 'confidence': 0.77592, 'speaker': None, 'channel': None}, {'text': 'walk', 'start': 16648, 'end': 16840, 'confidence': 0.99926, 'speaker': None, 'channel': None}, {'text': 'through', 'start': 16880, 'end': 17080, 'confidence': 0.99726, 'speaker': None, 'channel': None}, {'text': 'them.', 'start': 17120, 'end': 17464, 'confidence': 0.9914, 'speaker': None, 'channel': None}, {'text': 'Number', 'start': 17552, 'end': 17800, 'confidence': 0.97466, 'speaker': None, 'channel': None}, {'text': 'one', 'start': 17840, 'end': 18040, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 18080, 'end': 18568, 'confidence': 0.99624, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 18704, 'end': 19048, 'confidence': 0.99853, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 19104, 'end': 19416, 'confidence': 0.99831, 'speaker': None, 'channel': None}, {'text': 'lack', 'start': 19488, 'end': 19960, 'confidence': 0.94781, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 20040, 'end': 20616, 'confidence': 0.99901, 'speaker': None, 'channel': None}, {'text': 'domain', 'start': 20768, 'end': 21672, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'knowledge.', 'start': 21816, 'end': 22600, 'confidence': 0.87386, 'speaker': None, 'channel': None}, {'text': 'Remember,', 'start': 22760, 'end': 23192, 'confidence': 0.85016, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 23256, 'end': 23528, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 23584, 'end': 23800, 'confidence': 0.99781, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 23840, 'end': 24216, 'confidence': 0.87562, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 24248, 'end': 24504, 'confidence': 0.53107, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 24552, 'end': 24712, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'been', 'start': 24736, 'end': 24872, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'trained', 'start': 24896, 'end': 25144, 'confidence': 0.96243, 'speaker': None, 'channel': None}, {'text': 'on', 'start': 25192, 'end': 25400, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'publicly', 'start': 25440, 'end': 25944, 'confidence': 0.97609, 'speaker': None, 'channel': None}, {'text': 'available', 'start': 25992, 'end': 26344, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 26432, 'end': 26680, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'sets.', 'start': 26720, 'end': 27064, 'confidence': 0.92469, 'speaker': None, 'channel': None}, {'text': 'That', 'start': 27112, 'end': 27272, 'confidence': 0.9986, 'speaker': None, 'channel': None}, {'text': 'means', 'start': 27296, 'end': 27480, 'confidence': 0.9982, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 27520, 'end': 27672, 'confidence': 0.9872, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 27696, 'end': 27832, 'confidence': 0.99926, 'speaker': None, 'channel': None}, {'text': \"don't\", 'start': 27856, 'end': 28056, 'confidence': 0.99771, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 28088, 'end': 28280, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'Access', 'start': 28320, 'end': 28616, 'confidence': 0.99859, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 28688, 'end': 29250, 'confidence': 0.99093, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 29400, 'end': 29758, 'confidence': 0.99267, 'speaker': None, 'channel': None}, {'text': 'SOPs,', 'start': 29814, 'end': 30334, 'confidence': 0.89151, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 30382, 'end': 30542, 'confidence': 0.96304, 'speaker': None, 'channel': None}, {'text': 'standard', 'start': 30566, 'end': 30846, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'operating', 'start': 30878, 'end': 31262, 'confidence': 0.63512, 'speaker': None, 'channel': None}, {'text': 'procedures,', 'start': 31326, 'end': 32046, 'confidence': 0.99754, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 32158, 'end': 32382, 'confidence': 0.99858, 'speaker': None, 'channel': None}, {'text': \"don't\", 'start': 32406, 'end': 32606, 'confidence': 0.9977, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 32638, 'end': 32782, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'access', 'start': 32806, 'end': 33086, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 33158, 'end': 33342, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 33366, 'end': 33502, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 33526, 'end': 33710, 'confidence': 0.99756, 'speaker': None, 'channel': None}, {'text': 'ip,', 'start': 33750, 'end': 34110, 'confidence': 0.81378, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 34190, 'end': 34382, 'confidence': 0.99392, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 34406, 'end': 34590, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'records.', 'start': 34630, 'end': 35374, 'confidence': 0.99821, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 35502, 'end': 35742, 'confidence': 0.99239, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 35766, 'end': 35950, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'really', 'start': 35990, 'end': 36238, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': \"can't\", 'start': 36294, 'end': 36574, 'confidence': 0.9967, 'speaker': None, 'channel': None}, {'text': 'answer', 'start': 36622, 'end': 37054, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 37102, 'end': 37358, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 37414, 'end': 37582, 'confidence': 0.99962, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 37606, 'end': 37838, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'questions', 'start': 37894, 'end': 38334, 'confidence': 0.99873, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 38382, 'end': 38638, 'confidence': 0.67912, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 38694, 'end': 38910, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 38950, 'end': 39294, 'confidence': 0.99897, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 39382, 'end': 39806, 'confidence': 0.99261, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 39838, 'end': 39982, 'confidence': 0.99732, 'speaker': None, 'channel': None}, {'text': 'tailored', 'start': 40006, 'end': 40286, 'confidence': 0.9928, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 40318, 'end': 40462, 'confidence': 0.99842, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 40486, 'end': 40622, 'confidence': 0.99934, 'speaker': None, 'channel': None}, {'text': 'particular', 'start': 40646, 'end': 40926, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'business.', 'start': 40998, 'end': 41326, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 41398, 'end': 41726, 'confidence': 0.9878, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 41798, 'end': 41982, 'confidence': 0.99939, 'speaker': None, 'channel': None}, {'text': 'lose', 'start': 42006, 'end': 42302, 'confidence': 0.99593, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 42366, 'end': 42542, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 42566, 'end': 42702, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 42726, 'end': 42862, 'confidence': 0.99875, 'speaker': None, 'channel': None}, {'text': 'performance', 'start': 42886, 'end': 43214, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 43302, 'end': 43502, 'confidence': 0.99142, 'speaker': None, 'channel': None}, {'text': 'effectiveness', 'start': 43526, 'end': 44142, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 44206, 'end': 44430, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 44470, 'end': 44622, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'that.', 'start': 44646, 'end': 45022, 'confidence': 0.99677, 'speaker': None, 'channel': None}, {'text': 'Issue', 'start': 45126, 'end': 45486, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'number', 'start': 45558, 'end': 45790, 'confidence': 0.99098, 'speaker': None, 'channel': None}, {'text': 'two', 'start': 45830, 'end': 46030, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 46070, 'end': 46270, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'around', 'start': 46310, 'end': 46558, 'confidence': 0.99851, 'speaker': None, 'channel': None}, {'text': 'hallucinations.', 'start': 46614, 'end': 47646, 'confidence': 0.98398, 'speaker': None, 'channel': None}, {'text': 'These', 'start': 47758, 'end': 48030, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 48070, 'end': 48366, 'confidence': 0.99799, 'speaker': None, 'channel': None}, {'text': 'will', 'start': 48398, 'end': 48542, 'confidence': 0.99816, 'speaker': None, 'channel': None}, {'text': 'give', 'start': 48566, 'end': 48750, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 48790, 'end': 48990, 'confidence': 0.99943, 'speaker': None, 'channel': None}, {'text': 'responses.', 'start': 49030, 'end': 49582, 'confidence': 0.99881, 'speaker': None, 'channel': None}, {'text': 'They', 'start': 49646, 'end': 49870, 'confidence': 0.99913, 'speaker': None, 'channel': None}, {'text': 'look', 'start': 49910, 'end': 50206, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'really', 'start': 50278, 'end': 50558, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'credible,', 'start': 50614, 'end': 50974, 'confidence': 0.99936, 'speaker': None, 'channel': None}, {'text': 'but', 'start': 51022, 'end': 51182, 'confidence': 0.99972, 'speaker': None, 'channel': None}, {'text': \"they're\", 'start': 51206, 'end': 51454, 'confidence': 0.92674, 'speaker': None, 'channel': None}, {'text': 'way', 'start': 51502, 'end': 51710, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'off.', 'start': 51750, 'end': 51950, 'confidence': 0.998, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 51990, 'end': 52142, 'confidence': 0.9933, 'speaker': None, 'channel': None}, {'text': 'if', 'start': 52166, 'end': 52254, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 52262, 'end': 52430, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'run', 'start': 52470, 'end': 52670, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 52710, 'end': 52862, 'confidence': 0.99948, 'speaker': None, 'channel': None}, {'text': 'them,', 'start': 52886, 'end': 53070, 'confidence': 0.99864, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 53110, 'end': 53214, 'confidence': 0.99107, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 53222, 'end': 53438, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'might', 'start': 53494, 'end': 53662, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 53686, 'end': 53822, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 53846, 'end': 54030, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': 'problem.', 'start': 54070, 'end': 54558, 'confidence': 0.99916, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 54694, 'end': 54942, 'confidence': 0.97108, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 54966, 'end': 55102, 'confidence': 0.61985, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 55126, 'end': 55358, 'confidence': 0.9991, 'speaker': None, 'channel': None}, {'text': 'number', 'start': 55414, 'end': 55726, 'confidence': 0.99224, 'speaker': None, 'channel': None}, {'text': 'three,', 'start': 55798, 'end': 56030, 'confidence': 0.99929, 'speaker': None, 'channel': None}, {'text': 'which', 'start': 56070, 'end': 56222, 'confidence': 0.99927, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 56246, 'end': 56430, 'confidence': 0.8425, 'speaker': None, 'channel': None}, {'text': 'becoming', 'start': 56470, 'end': 56782, 'confidence': 0.99673, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 56846, 'end': 56974, 'confidence': 0.99887, 'speaker': None, 'channel': None}, {'text': 'little', 'start': 56982, 'end': 57102, 'confidence': 0.99835, 'speaker': None, 'channel': None}, {'text': 'bit', 'start': 57126, 'end': 57214, 'confidence': 0.99398, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 57222, 'end': 57342, 'confidence': 0.90287, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 57366, 'end': 57598, 'confidence': 0.90365, 'speaker': None, 'channel': None}, {'text': 'less', 'start': 57654, 'end': 57822, 'confidence': 0.99863, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 57846, 'end': 57982, 'confidence': 0.99669, 'speaker': None, 'channel': None}, {'text': 'an', 'start': 58006, 'end': 58094, 'confidence': 0.99907, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 58102, 'end': 58270, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 58310, 'end': 58510, 'confidence': 0.99753, 'speaker': None, 'channel': None}, {'text': 'search.', 'start': 58550, 'end': 59120, 'confidence': 0.97262, 'speaker': None, 'channel': None}, {'text': 'But', 'start': 59270, 'end': 59628, 'confidence': 0.9973, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 59684, 'end': 59900, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 59940, 'end': 60236, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'training', 'start': 60308, 'end': 60652, 'confidence': 0.93588, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 60716, 'end': 61036, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'cutoff', 'start': 61108, 'end': 61564, 'confidence': 0.67416, 'speaker': None, 'channel': None}, {'text': 'dates.', 'start': 61612, 'end': 62028, 'confidence': 0.90929, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 62124, 'end': 62284, 'confidence': 0.98489, 'speaker': None, 'channel': None}, {'text': 'for', 'start': 62292, 'end': 62412, 'confidence': 0.99934, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 62436, 'end': 62572, 'confidence': 0.96572, 'speaker': None, 'channel': None}, {'text': 'while', 'start': 62596, 'end': 62828, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'there', 'start': 62884, 'end': 63244, 'confidence': 0.99507, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 63332, 'end': 63532, 'confidence': 0.99865, 'speaker': None, 'channel': None}, {'text': 'had', 'start': 63556, 'end': 63692, 'confidence': 0.98363, 'speaker': None, 'channel': None}, {'text': 'chatgpt', 'start': 63716, 'end': 64540, 'confidence': 0.7027, 'speaker': None, 'channel': None}, {'text': 'missing', 'start': 64700, 'end': 65356, 'confidence': 0.52967, 'speaker': None, 'channel': None}, {'text': 'months', 'start': 65468, 'end': 65884, 'confidence': 0.99858, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 65972, 'end': 66268, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'training', 'start': 66324, 'end': 66604, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 66652, 'end': 66908, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 66964, 'end': 67228, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 67284, 'end': 67452, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': \"hadn't\", 'start': 67476, 'end': 67708, 'confidence': 0.99567, 'speaker': None, 'channel': None}, {'text': 'been', 'start': 67724, 'end': 67900, 'confidence': 0.99973, 'speaker': None, 'channel': None}, {'text': 'updated', 'start': 67940, 'end': 68380, 'confidence': 0.99944, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 68460, 'end': 68652, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 68676, 'end': 68812, 'confidence': 0.99028, 'speaker': None, 'channel': None}, {'text': 'while', 'start': 68836, 'end': 68972, 'confidence': 0.99902, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 68996, 'end': 69132, 'confidence': 0.98259, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 69156, 'end': 69292, 'confidence': 0.99689, 'speaker': None, 'channel': None}, {'text': 'part', 'start': 69316, 'end': 69452, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 69476, 'end': 69612, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 69636, 'end': 69772, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'takes', 'start': 69796, 'end': 69996, 'confidence': 0.99916, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 70028, 'end': 70124, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 70132, 'end': 70204, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 70212, 'end': 70332, 'confidence': 0.99906, 'speaker': None, 'channel': None}, {'text': 'compute', 'start': 70356, 'end': 70636, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 70668, 'end': 70812, 'confidence': 0.99705, 'speaker': None, 'channel': None}, {'text': 'train', 'start': 70836, 'end': 71228, 'confidence': 0.88696, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 71324, 'end': 71628, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'models.', 'start': 71684, 'end': 72140, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 72220, 'end': 72412, 'confidence': 0.9952, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 72436, 'end': 72524, 'confidence': 0.98723, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 72532, 'end': 72652, 'confidence': 0.9988, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 72676, 'end': 72860, 'confidence': 0.99944, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 72900, 'end': 73100, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'issues', 'start': 73140, 'end': 73580, 'confidence': 0.99689, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 73700, 'end': 73932, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 73956, 'end': 74092, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'keeping', 'start': 74116, 'end': 74444, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 74492, 'end': 74700, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 74740, 'end': 74940, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'getting', 'start': 74980, 'end': 75372, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 75476, 'end': 75788, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 75844, 'end': 76204, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 76292, 'end': 76636, 'confidence': 0.99877, 'speaker': None, 'channel': None}, {'text': 'performance', 'start': 76708, 'end': 77180, 'confidence': 0.99961, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 77300, 'end': 77580, 'confidence': 0.99973, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 77620, 'end': 77820, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'LLMs', 'start': 77860, 'end': 78364, 'confidence': 0.9317, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 78412, 'end': 78572, 'confidence': 0.99976, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 78596, 'end': 78732, 'confidence': 0.99871, 'speaker': None, 'channel': None}, {'text': 'bring', 'start': 78756, 'end': 78940, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'them', 'start': 78980, 'end': 79180, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 79220, 'end': 79420, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'house.', 'start': 79460, 'end': 79708, 'confidence': 0.97514, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 79764, 'end': 79884, 'confidence': 0.97828, 'speaker': None, 'channel': None}, {'text': \"let's\", 'start': 79892, 'end': 80076, 'confidence': 0.91209, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 80108, 'end': 80252, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'about', 'start': 80276, 'end': 80652, 'confidence': 0.99282, 'speaker': None, 'channel': None}, {'text': 'one', 'start': 80756, 'end': 81068, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'pattern', 'start': 81124, 'end': 81484, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 81532, 'end': 81836, 'confidence': 0.99866, 'speaker': None, 'channel': None}, {'text': 'has', 'start': 81908, 'end': 82332, 'confidence': 0.9945, 'speaker': None, 'channel': None}, {'text': 'emerged', 'start': 82436, 'end': 83036, 'confidence': 0.99078, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 83068, 'end': 83260, 'confidence': 0.99755, 'speaker': None, 'channel': None}, {'text': 'being', 'start': 83300, 'end': 83500, 'confidence': 0.99941, 'speaker': None, 'channel': None}, {'text': 'particularly', 'start': 83540, 'end': 83964, 'confidence': 0.99895, 'speaker': None, 'channel': None}, {'text': 'helpful', 'start': 84012, 'end': 84316, 'confidence': 0.99905, 'speaker': None, 'channel': None}, {'text': 'here,', 'start': 84348, 'end': 84894, 'confidence': 0.98289, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 85052, 'end': 85322, 'confidence': 0.98498, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 85346, 'end': 85482, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 85506, 'end': 85786, 'confidence': 0.99891, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 85858, 'end': 86506, 'confidence': 0.77335, 'speaker': None, 'channel': None}, {'text': 'retrieval,', 'start': 86618, 'end': 87546, 'confidence': 0.98123, 'speaker': None, 'channel': None}, {'text': 'augmented', 'start': 87658, 'end': 88538, 'confidence': 0.53074, 'speaker': None, 'channel': None}, {'text': 'generation.', 'start': 88634, 'end': 89290, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'You', 'start': 89370, 'end': 89562, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'may', 'start': 89586, 'end': 89674, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 89682, 'end': 89802, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'heard', 'start': 89826, 'end': 90026, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 90058, 'end': 90250, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'term', 'start': 90290, 'end': 90538, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'toss', 'start': 90594, 'end': 90874, 'confidence': 0.99748, 'speaker': None, 'channel': None}, {'text': 'around,', 'start': 90922, 'end': 91178, 'confidence': 0.99271, 'speaker': None, 'channel': None}, {'text': 'but', 'start': 91234, 'end': 91402, 'confidence': 0.99938, 'speaker': None, 'channel': None}, {'text': \"let's\", 'start': 91426, 'end': 91626, 'confidence': 0.99912, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 91658, 'end': 91802, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'about', 'start': 91826, 'end': 92154, 'confidence': 0.99563, 'speaker': None, 'channel': None}, {'text': \"what's\", 'start': 92242, 'end': 92538, 'confidence': 0.99939, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 92554, 'end': 92682, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'on', 'start': 92706, 'end': 92938, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'here.', 'start': 92994, 'end': 93258, 'confidence': 0.99897, 'speaker': None, 'channel': None}, {'text': 'First,', 'start': 93314, 'end': 93722, 'confidence': 0.99867, 'speaker': None, 'channel': None}, {'text': 'let', 'start': 93826, 'end': 94042, 'confidence': 0.99742, 'speaker': None, 'channel': None}, {'text': 'me', 'start': 94066, 'end': 94154, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'give', 'start': 94162, 'end': 94282, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 94306, 'end': 94442, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'some', 'start': 94466, 'end': 94602, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'context.', 'start': 94626, 'end': 95178, 'confidence': 0.70344, 'speaker': None, 'channel': None}, {'text': \"Here's\", 'start': 95274, 'end': 95626, 'confidence': 0.99802, 'speaker': None, 'channel': None}, {'text': 'what', 'start': 95658, 'end': 95850, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'happens', 'start': 95890, 'end': 96266, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'when', 'start': 96298, 'end': 96586, 'confidence': 0.75484, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 96658, 'end': 96938, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'send', 'start': 96994, 'end': 97322, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 97386, 'end': 97658, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 97714, 'end': 98218, 'confidence': 0.99724, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 98314, 'end': 98522, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 98546, 'end': 98730, 'confidence': 0.9992, 'speaker': None, 'channel': None}, {'text': 'standard', 'start': 98770, 'end': 99114, 'confidence': 0.9985, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 99162, 'end': 99594, 'confidence': 0.99334, 'speaker': None, 'channel': None}, {'text': 'assistant.', 'start': 99642, 'end': 100394, 'confidence': 0.99849, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 100522, 'end': 100858, 'confidence': 0.98196, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 100914, 'end': 101082, 'confidence': 0.99964, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 101106, 'end': 101386, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'your.', 'start': 101458, 'end': 101978, 'confidence': 0.9397, 'speaker': None, 'channel': None}, {'text': 'Your', 'start': 102114, 'end': 102410, 'confidence': 0.98402, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 102450, 'end': 102890, 'confidence': 0.98313, 'speaker': None, 'channel': None}, {'text': 'goes', 'start': 102970, 'end': 103226, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 103258, 'end': 103402, 'confidence': 0.99919, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 103426, 'end': 103562, 'confidence': 0.99794, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 103586, 'end': 103786, 'confidence': 0.64565, 'speaker': None, 'channel': None}, {'text': 'assistant,', 'start': 103818, 'end': 104410, 'confidence': 0.99771, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 104490, 'end': 104682, 'confidence': 0.99358, 'speaker': None, 'channel': None}, {'text': 'generates', 'start': 104706, 'end': 105114, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 105162, 'end': 105274, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 105282, 'end': 105626, 'confidence': 0.99908, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 105658, 'end': 105802, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 105826, 'end': 105962, 'confidence': 0.99529, 'speaker': None, 'channel': None}, {'text': 'gives', 'start': 105986, 'end': 106218, 'confidence': 0.99578, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 106234, 'end': 106410, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'back', 'start': 106450, 'end': 106650, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 106690, 'end': 106794, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'you.', 'start': 106802, 'end': 106970, 'confidence': 0.99915, 'speaker': None, 'channel': None}, {'text': 'Right?', 'start': 107010, 'end': 107546, 'confidence': 0.99317, 'speaker': None, 'channel': None}, {'text': 'In', 'start': 107698, 'end': 108106, 'confidence': 0.99157, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 108178, 'end': 108410, 'confidence': 0.99803, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 108450, 'end': 108810, 'confidence': 0.76677, 'speaker': None, 'channel': None}, {'text': 'implementation,', 'start': 108890, 'end': 109562, 'confidence': 0.96229, 'speaker': None, 'channel': None}, {'text': \"you're\", 'start': 109626, 'end': 109866, 'confidence': 0.99715, 'speaker': None, 'channel': None}, {'text': 'adding', 'start': 109898, 'end': 110282, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'an', 'start': 110346, 'end': 110570, 'confidence': 0.95987, 'speaker': None, 'channel': None}, {'text': 'additional', 'start': 110610, 'end': 111242, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'step', 'start': 111306, 'end': 111626, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'here.', 'start': 111698, 'end': 112212, 'confidence': 0.99461, 'speaker': None, 'channel': None}, {'text': 'Before', 'start': 112346, 'end': 112832, 'confidence': 0.99811, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 112936, 'end': 113248, 'confidence': 0.99491, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 113304, 'end': 113808, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'goes', 'start': 113904, 'end': 114480, 'confidence': 0.99764, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 114560, 'end': 114848, 'confidence': 0.99927, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 114904, 'end': 115120, 'confidence': 0.99885, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 115160, 'end': 115504, 'confidence': 0.51374, 'speaker': None, 'channel': None}, {'text': 'assistant,', 'start': 115552, 'end': 116304, 'confidence': 0.95104, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 116432, 'end': 116672, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 116696, 'end': 116928, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 116984, 'end': 117248, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'search', 'start': 117304, 'end': 117728, 'confidence': 0.98986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 117824, 'end': 118080, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'hits', 'start': 118120, 'end': 118464, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 118512, 'end': 118672, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'corpus', 'start': 118696, 'end': 119104, 'confidence': 0.98243, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 119152, 'end': 119264, 'confidence': 0.9984, 'speaker': None, 'channel': None}, {'text': 'data.', 'start': 119272, 'end': 119632, 'confidence': 0.99849, 'speaker': None, 'channel': None}, {'text': 'Now', 'start': 119736, 'end': 119952, 'confidence': 0.99222, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 119976, 'end': 120112, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 120136, 'end': 120272, 'confidence': 0.99754, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 120296, 'end': 120384, 'confidence': 0.89301, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 120392, 'end': 120464, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 120472, 'end': 120592, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 120616, 'end': 120800, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'data,', 'start': 120840, 'end': 121136, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 121208, 'end': 121440, 'confidence': 0.999, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 121480, 'end': 121680, 'confidence': 0.99943, 'speaker': None, 'channel': None}, {'text': 'documents', 'start': 121720, 'end': 122512, 'confidence': 0.98816, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 122656, 'end': 123344, 'confidence': 0.9881, 'speaker': None, 'channel': None}, {'text': 'other', 'start': 123512, 'end': 123888, 'confidence': 0.99373, 'speaker': None, 'channel': None}, {'text': 'relevant', 'start': 123944, 'end': 124528, 'confidence': 0.99962, 'speaker': None, 'channel': None}, {'text': 'information', 'start': 124624, 'end': 125024, 'confidence': 0.99997, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 125112, 'end': 125312, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 125336, 'end': 125472, 'confidence': 0.99993, 'speaker': None, 'channel': None}, {'text': 'want', 'start': 125496, 'end': 125632, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 125656, 'end': 125744, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'make', 'start': 125752, 'end': 125872, 'confidence': 0.99993, 'speaker': None, 'channel': None}, {'text': 'available', 'start': 125896, 'end': 126224, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 126312, 'end': 126464, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 126472, 'end': 126592, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 126616, 'end': 126896, 'confidence': 0.8474, 'speaker': None, 'channel': None}, {'text': 'assistant.', 'start': 126928, 'end': 127520, 'confidence': 0.99676, 'speaker': None, 'channel': None}, {'text': \"There's\", 'start': 127600, 'end': 127888, 'confidence': 0.985, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 127904, 'end': 127984, 'confidence': 0.99442, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 127992, 'end': 128160, 'confidence': 0.94329, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 128200, 'end': 128496, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 128568, 'end': 128848, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 128904, 'end': 129584, 'confidence': 0.96697, 'speaker': None, 'channel': None}, {'text': \"that's\", 'start': 129632, 'end': 129936, 'confidence': 0.99703, 'speaker': None, 'channel': None}, {'text': 'done', 'start': 129968, 'end': 130304, 'confidence': 0.99821, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 130392, 'end': 130592, 'confidence': 0.98417, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 130616, 'end': 130848, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'context', 'start': 130904, 'end': 131392, 'confidence': 0.95338, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 131456, 'end': 131632, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 131656, 'end': 131840, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 131880, 'end': 131984, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 131992, 'end': 132208, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'added', 'start': 132264, 'end': 132816, 'confidence': 0.99819, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 132968, 'end': 133280, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'addition', 'start': 133320, 'end': 133616, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 133688, 'end': 133920, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 133960, 'end': 134160, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'original', 'start': 134200, 'end': 134752, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'prompt.', 'start': 134816, 'end': 135328, 'confidence': 0.99169, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 135424, 'end': 135728, 'confidence': 0.95505, 'speaker': None, 'channel': None}, {'text': 'so', 'start': 135784, 'end': 136096, 'confidence': 0.99938, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 136168, 'end': 136352, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 136376, 'end': 136560, 'confidence': 0.99812, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 136600, 'end': 136944, 'confidence': 0.95847, 'speaker': None, 'channel': None}, {'text': 'model', 'start': 136992, 'end': 137376, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 137408, 'end': 137552, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 137576, 'end': 137712, 'confidence': 0.99827, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 137736, 'end': 137824, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'get', 'start': 137832, 'end': 137952, 'confidence': 0.9986, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 137976, 'end': 138160, 'confidence': 0.99948, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 138200, 'end': 138544, 'confidence': 0.99894, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 138592, 'end': 138704, 'confidence': 0.99819, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 138712, 'end': 138880, 'confidence': 0.99716, 'speaker': None, 'channel': None}, {'text': 'also', 'start': 138920, 'end': 139264, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 139352, 'end': 139600, 'confidence': 0.99914, 'speaker': None, 'channel': None}, {'text': 'relevant', 'start': 139640, 'end': 140064, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'information', 'start': 140112, 'end': 140608, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 140744, 'end': 141022, 'confidence': 0.38553, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 141056, 'end': 141298, 'confidence': 0.99536, 'speaker': None, 'channel': None}, {'text': 'was', 'start': 141354, 'end': 141762, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'found', 'start': 141866, 'end': 142226, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'during', 'start': 142298, 'end': 142722, 'confidence': 0.99985, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 142826, 'end': 143138, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'process.', 'start': 143194, 'end': 143746, 'confidence': 0.99862, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 143898, 'end': 144162, 'confidence': 0.98826, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 144186, 'end': 144514, 'confidence': 0.9983, 'speaker': None, 'channel': None}, {'text': 'everything', 'start': 144602, 'end': 144898, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'else', 'start': 144954, 'end': 145186, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'proceeds', 'start': 145218, 'end': 145746, 'confidence': 0.81719, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 145778, 'end': 145922, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'same', 'start': 145946, 'end': 146130, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'way.', 'start': 146170, 'end': 146322, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'The', 'start': 146346, 'end': 146482, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 146506, 'end': 146786, 'confidence': 0.93205, 'speaker': None, 'channel': None}, {'text': 'system', 'start': 146818, 'end': 147106, 'confidence': 0.56095, 'speaker': None, 'channel': None}, {'text': 'will', 'start': 147178, 'end': 147362, 'confidence': 0.99839, 'speaker': None, 'channel': None}, {'text': 'process', 'start': 147386, 'end': 147618, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 147674, 'end': 148034, 'confidence': 0.9741, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 148122, 'end': 148418, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'generate', 'start': 148474, 'end': 148962, 'confidence': 0.99905, 'speaker': None, 'channel': None}, {'text': 'typically', 'start': 149026, 'end': 149394, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 149442, 'end': 149650, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'better', 'start': 149690, 'end': 149938, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 149994, 'end': 150434, 'confidence': 0.75474, 'speaker': None, 'channel': None}, {'text': 'for', 'start': 150482, 'end': 150690, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 150730, 'end': 151026, 'confidence': 0.9998, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 151098, 'end': 151378, 'confidence': 0.99923, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 151434, 'end': 151698, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'user.', 'start': 151754, 'end': 152562, 'confidence': 0.99782, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 152706, 'end': 152962, 'confidence': 0.99749, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 152986, 'end': 153122, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 153146, 'end': 153666, 'confidence': 0.91142, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 153698, 'end': 153986, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 154058, 'end': 154338, 'confidence': 0.99941, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 154394, 'end': 154610, 'confidence': 0.99915, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 154650, 'end': 155186, 'confidence': 0.95684, 'speaker': None, 'channel': None}, {'text': 'function', 'start': 155218, 'end': 155506, 'confidence': 0.99932, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 155538, 'end': 155778, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'where', 'start': 155834, 'end': 156002, 'confidence': 0.9853, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 156026, 'end': 156226, 'confidence': 0.53037, 'speaker': None, 'channel': None}, {'text': 'grabbing', 'start': 156258, 'end': 156818, 'confidence': 0.99696, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 156914, 'end': 157074, 'confidence': 0.99853, 'speaker': None, 'channel': None}, {'text': 'information.', 'start': 157082, 'end': 157394, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'The', 'start': 157482, 'end': 157730, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'augmented', 'start': 157770, 'end': 158386, 'confidence': 0.98869, 'speaker': None, 'channel': None}, {'text': 'part', 'start': 158418, 'end': 158610, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 158650, 'end': 158850, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 158890, 'end': 159042, 'confidence': 0.99869, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 159066, 'end': 159266, 'confidence': 0.98866, 'speaker': None, 'channel': None}, {'text': 'augmenting', 'start': 159298, 'end': 159826, 'confidence': 0.99323, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 159858, 'end': 160002, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 160026, 'end': 160114, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 160122, 'end': 160194, 'confidence': 0.9998, 'speaker': None, 'channel': None}, {'text': 'original', 'start': 160202, 'end': 160562, 'confidence': 0.9976, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 160626, 'end': 160962, 'confidence': 0.72399, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 161026, 'end': 161250, 'confidence': 0.59248, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 161290, 'end': 161442, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'generation', 'start': 161466, 'end': 161874, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 161922, 'end': 162082, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 162106, 'end': 162242, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 162266, 'end': 162354, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 162362, 'end': 162546, 'confidence': 0.9947, 'speaker': None, 'channel': None}, {'text': 'generation', 'start': 162578, 'end': 163186, 'confidence': 0.70973, 'speaker': None, 'channel': None}, {'text': 'generating', 'start': 163298, 'end': 163986, 'confidence': 0.99562, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 164098, 'end': 164322, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'same', 'start': 164346, 'end': 164530, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 164570, 'end': 164786, 'confidence': 0.99625, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 164818, 'end': 165058, 'confidence': 0.99757, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 165114, 'end': 165282, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 165306, 'end': 165490, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'LLM.', 'start': 165530, 'end': 166280, 'confidence': 0.92993, 'speaker': None, 'channel': None}, {'text': 'It', 'start': 166410, 'end': 166652, 'confidence': 0.98598, 'speaker': None, 'channel': None}, {'text': 'turns', 'start': 166676, 'end': 166908, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'out', 'start': 166924, 'end': 167052, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 167076, 'end': 167212, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 167236, 'end': 167372, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 167396, 'end': 167580, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 167620, 'end': 167964, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'robust,', 'start': 168052, 'end': 168844, 'confidence': 0.87242, 'speaker': None, 'channel': None}, {'text': 'efficient', 'start': 168972, 'end': 169612, 'confidence': 0.9238, 'speaker': None, 'channel': None}, {'text': 'way', 'start': 169676, 'end': 170044, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 170132, 'end': 170428, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'tackle', 'start': 170484, 'end': 171116, 'confidence': 0.99742, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 171228, 'end': 171500, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'problems', 'start': 171540, 'end': 171868, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 171884, 'end': 172012, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 172036, 'end': 172236, 'confidence': 0.98818, 'speaker': None, 'channel': None}, {'text': 'seeing', 'start': 172268, 'end': 172476, 'confidence': 0.99286, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 172508, 'end': 172700, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'LLMs', 'start': 172740, 'end': 173244, 'confidence': 0.91888, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 173292, 'end': 173452, 'confidence': 0.64282, 'speaker': None, 'channel': None}, {'text': 'industry', 'start': 173476, 'end': 173996, 'confidence': 0.99828, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 174148, 'end': 174556, 'confidence': 0.99779, 'speaker': None, 'channel': None}, {'text': 'business.', 'start': 174628, 'end': 175196, 'confidence': 0.99822, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 175348, 'end': 175564, 'confidence': 0.99535, 'speaker': None, 'channel': None}, {'text': 'I', 'start': 175572, 'end': 175692, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'hope', 'start': 175716, 'end': 175948, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 176004, 'end': 176172, 'confidence': 0.99881, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 176196, 'end': 176332, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'helps', 'start': 176356, 'end': 176684, 'confidence': 0.99932, 'speaker': None, 'channel': None}, {'text': 'explain', 'start': 176732, 'end': 177260, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 177340, 'end': 177676, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 177748, 'end': 178268, 'confidence': 0.9594, 'speaker': None, 'channel': None}, {'text': 'framework.', 'start': 178364, 'end': 178892, 'confidence': 0.99665, 'speaker': None, 'channel': None}, {'text': 'If', 'start': 178956, 'end': 179132, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 179156, 'end': 179244, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 179252, 'end': 179372, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 179396, 'end': 179580, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'questions,', 'start': 179620, 'end': 180172, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 180236, 'end': 180460, 'confidence': 0.99753, 'speaker': None, 'channel': None}, {'text': 'comments', 'start': 180500, 'end': 180796, 'confidence': 0.99908, 'speaker': None, 'channel': None}, {'text': 'if', 'start': 180868, 'end': 181052, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'I', 'start': 181076, 'end': 181212, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'missed', 'start': 181236, 'end': 181516, 'confidence': 0.73098, 'speaker': None, 'channel': None}, {'text': 'anything,', 'start': 181548, 'end': 182108, 'confidence': 0.73069, 'speaker': None, 'channel': None}, {'text': 'please', 'start': 182204, 'end': 182556, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'drop', 'start': 182628, 'end': 182876, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'them', 'start': 182908, 'end': 183052, 'confidence': 0.99788, 'speaker': None, 'channel': None}, {'text': 'below.', 'start': 183076, 'end': 183500, 'confidence': 0.99883, 'speaker': None, 'channel': None}, {'text': 'If', 'start': 183620, 'end': 183852, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': \"you're\", 'start': 183876, 'end': 184076, 'confidence': 0.97379, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 184108, 'end': 184204, 'confidence': 0.96876, 'speaker': None, 'channel': None}, {'text': 'practitioner', 'start': 184212, 'end': 184636, 'confidence': 0.99526, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 184668, 'end': 184764, 'confidence': 0.7334, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 184772, 'end': 184892, 'confidence': 0.98492, 'speaker': None, 'channel': None}, {'text': 'want', 'start': 184916, 'end': 185052, 'confidence': 0.77671, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 185076, 'end': 185212, 'confidence': 0.99902, 'speaker': None, 'channel': None}, {'text': 'add', 'start': 185236, 'end': 185420, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'something', 'start': 185460, 'end': 185660, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 185700, 'end': 185852, 'confidence': 0.99901, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 185876, 'end': 186012, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'conversation,', 'start': 186036, 'end': 186492, 'confidence': 0.99848, 'speaker': None, 'channel': None}, {'text': 'please', 'start': 186556, 'end': 186780, 'confidence': 0.99988, 'speaker': None, 'channel': None}, {'text': 'drop', 'start': 186820, 'end': 187036, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 187068, 'end': 187212, 'confidence': 0.99649, 'speaker': None, 'channel': None}, {'text': 'below', 'start': 187236, 'end': 187420, 'confidence': 0.99976, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 187460, 'end': 187564, 'confidence': 0.99834, 'speaker': None, 'channel': None}, {'text': 'well.', 'start': 187572, 'end': 187692, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'There', 'start': 187716, 'end': 187804, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 187812, 'end': 187884, 'confidence': 0.99067, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 187892, 'end': 187964, 'confidence': 0.99872, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 187972, 'end': 188044, 'confidence': 0.9999, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 188052, 'end': 188124, 'confidence': 0.99834, 'speaker': None, 'channel': None}, {'text': 'people', 'start': 188132, 'end': 188252, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 188276, 'end': 188412, 'confidence': 0.99836, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 188436, 'end': 188524, 'confidence': 0.99876, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 188532, 'end': 188652, 'confidence': 0.90387, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 188676, 'end': 188764, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'watch', 'start': 188772, 'end': 188892, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'this,', 'start': 188916, 'end': 189148, 'confidence': 0.99878, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 189204, 'end': 189372, 'confidence': 0.99528, 'speaker': None, 'channel': None}, {'text': \"I'm\", 'start': 189396, 'end': 189548, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'sure', 'start': 189564, 'end': 189692, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 189716, 'end': 189852, 'confidence': 0.99673, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 189876, 'end': 190012, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'would', 'start': 190036, 'end': 190172, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'benefit', 'start': 190196, 'end': 190732, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 190796, 'end': 191068, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 191124, 'end': 191340, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'expertise', 'start': 191380, 'end': 191884, 'confidence': 0.97094, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 191932, 'end': 192044, 'confidence': 0.99733, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 192052, 'end': 192172, 'confidence': 0.99792, 'speaker': None, 'channel': None}, {'text': 'knowledge.', 'start': 192196, 'end': 192668, 'confidence': 0.99784, 'speaker': None, 'channel': None}, {'text': 'Thanks', 'start': 192764, 'end': 193084, 'confidence': 0.9973, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 193132, 'end': 193436, 'confidence': 0.73072, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 193508, 'end': 193692, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 193716, 'end': 193852, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 193876, 'end': 193964, 'confidence': 0.9965, 'speaker': None, 'channel': None}, {'text': 'soon.', 'start': 193972, 'end': 194020, 'confidence': 0.99913, 'speaker': None, 'channel': None}], 'utterances': None, 'confidence': 0.9741621, 'audio_duration': 195, 'webhook_status_code': None, 'webhook_auth': False, 'summary': None, 'auto_highlights_result': None, 'content_safety_labels': None, 'iab_categories_result': None, 'chapters': None, 'sentiment_analysis_results': None, 'entities': None}, page_content=\"to you. Right? In a RAG implementation, you're adding an additional step here. Before that prompt goes into the AI assistant, we have a search that hits a corpus of data. Now this is going to be your data, your own documents and other relevant information that you want to make available to the AI assistant. There's going to be a retrieval that's done and that context is going to be added in addition to your original prompt. And so the large language model is going to get your prompt and then also any relevant information that that was found during this process. And then everything else proceeds the same way. The AI system will process that and generate typically a better response for you as a user. So the retrieval here is this retrieval function here where we're grabbing the information. The augmented part here is we're augmenting that to the original prompt and the generation here is that we're generation generating the same prompt here from the LLM. It turns out that this is a\"),\n",
              " Document(metadata={'language_code': 'en_us', 'audio_url': 'https://cdn.assemblyai.com/upload/0c0fbf9b-b080-48a9-8611-eae24774762d', 'punctuate': True, 'format_text': True, 'dual_channel': None, 'multichannel': None, 'audio_channels': None, 'webhook_url': None, 'webhook_auth_header_name': None, 'webhook_auth_header_value': None, 'audio_start_from': None, 'audio_end_at': None, 'word_boost': [], 'boost_param': None, 'filter_profanity': False, 'redact_pii': False, 'redact_pii_audio': False, 'redact_pii_audio_quality': None, 'redact_pii_policies': None, 'redact_pii_sub': None, 'speaker_labels': False, 'speakers_expected': None, 'content_safety': False, 'content_safety_confidence': None, 'iab_categories': False, 'custom_spelling': None, 'disfluencies': False, 'sentiment_analysis': False, 'auto_chapters': False, 'entity_detection': False, 'summarization': False, 'summary_model': None, 'summary_type': None, 'auto_highlights': False, 'language_detection': False, 'language_confidence_threshold': None, 'language_confidence': None, 'speech_threshold': None, 'speech_model': None, 'id': '656ac874-63b0-48ef-808e-e9c01d6f394f', 'status': <TranscriptStatus.completed: 'completed'>, 'error': None, 'text': \"Large language models and AI assistants are taking the consumer world by storm. But what happens when you try to bring these large language models into business, into enterprise? Well, we have three issues, three issues that we have to overcome. Let's walk through them. Number one issue is this lack of domain knowledge. Remember, these large language models have been trained on publicly available data sets. That means that they don't have Access to your SOPs, your standard operating procedures, they don't have access to your own ip, your own records. So they really can't answer a lot of questions and have that response be tailored to your particular business. And you lose a lot of performance and effectiveness because of that. Issue number two is around hallucinations. These models will give you responses. They look really credible, but they're way off. And if you run with them, then you might have a problem. And then issue number three, which is becoming a little bit of a less of an issue with search. But we have training data cutoff dates. So for a while there you had chatgpt missing months of training data because it hadn't been updated in a while and in part because it takes a lot of compute to train these models. So you have these three issues that are keeping you from getting a lot of performance from your LLMs as you bring them in house. So let's talk about one pattern that has emerged as being particularly helpful here, and that is RAG retrieval, augmented generation. You may have heard this term toss around, but let's talk about what's going on here. First, let me give you some context. Here's what happens when you send a prompt to your standard AI assistant. So you have your. Your prompt goes into the AI assistant, it generates a response and then gives it back to you. Right? In a RAG implementation, you're adding an additional step here. Before that prompt goes into the AI assistant, we have a search that hits a corpus of data. Now this is going to be your data, your own documents and other relevant information that you want to make available to the AI assistant. There's going to be a retrieval that's done and that context is going to be added in addition to your original prompt. And so the large language model is going to get your prompt and then also any relevant information that that was found during this process. And then everything else proceeds the same way. The AI system will process that and generate typically a better response for you as a user. So the retrieval here is this retrieval function here where we're grabbing the information. The augmented part here is we're augmenting that to the original prompt and the generation here is that we're generation generating the same prompt here from the LLM. It turns out that this is a robust, efficient way to tackle these problems that we're seeing with LLMs and industry and business. So I hope that this helps explain the RAG framework. If you have any questions, any comments if I missed anything, please drop them below. If you're a practitioner and you want to add something to this conversation, please drop that below as well. There are a lot of people that are going to watch this, and I'm sure that they would benefit from your expertise and your knowledge. Thanks and talk to you soon.\", 'words': [{'text': 'Large', 'start': 240, 'end': 448, 'confidence': 0.99654, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 464, 'end': 776, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 808, 'end': 1016, 'confidence': 0.99879, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 1048, 'end': 1192, 'confidence': 0.91664, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 1216, 'end': 1496, 'confidence': 0.99191, 'speaker': None, 'channel': None}, {'text': 'assistants', 'start': 1528, 'end': 2200, 'confidence': 0.83958, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 2280, 'end': 2520, 'confidence': 0.99864, 'speaker': None, 'channel': None}, {'text': 'taking', 'start': 2560, 'end': 2856, 'confidence': 0.99972, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 2928, 'end': 3208, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'consumer', 'start': 3264, 'end': 3960, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'world', 'start': 4040, 'end': 4376, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'by', 'start': 4448, 'end': 4824, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'storm.', 'start': 4912, 'end': 5800, 'confidence': 0.99589, 'speaker': None, 'channel': None}, {'text': 'But', 'start': 5960, 'end': 6280, 'confidence': 0.99517, 'speaker': None, 'channel': None}, {'text': 'what', 'start': 6320, 'end': 6520, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'happens', 'start': 6560, 'end': 6856, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'when', 'start': 6888, 'end': 7032, 'confidence': 0.99985, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 7056, 'end': 7192, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'try', 'start': 7216, 'end': 7400, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 7440, 'end': 7592, 'confidence': 0.99887, 'speaker': None, 'channel': None}, {'text': 'bring', 'start': 7616, 'end': 7896, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 7968, 'end': 8152, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 8176, 'end': 8408, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 8464, 'end': 8824, 'confidence': 0.99961, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 8872, 'end': 9304, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 9352, 'end': 9656, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'business,', 'start': 9728, 'end': 10152, 'confidence': 0.99863, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 10256, 'end': 10664, 'confidence': 0.54956, 'speaker': None, 'channel': None}, {'text': 'enterprise?', 'start': 10752, 'end': 11496, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'Well,', 'start': 11608, 'end': 11832, 'confidence': 0.85014, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 11856, 'end': 11992, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 12016, 'end': 12296, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 12368, 'end': 12696, 'confidence': 0.9522, 'speaker': None, 'channel': None}, {'text': 'issues,', 'start': 12768, 'end': 13432, 'confidence': 0.99097, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 13616, 'end': 13960, 'confidence': 0.98567, 'speaker': None, 'channel': None}, {'text': 'issues', 'start': 14000, 'end': 14536, 'confidence': 0.99866, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 14688, 'end': 15048, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 15104, 'end': 15320, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 15360, 'end': 15560, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 15600, 'end': 15752, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'overcome.', 'start': 15776, 'end': 16344, 'confidence': 0.99912, 'speaker': None, 'channel': None}, {'text': \"Let's\", 'start': 16392, 'end': 16616, 'confidence': 0.77592, 'speaker': None, 'channel': None}, {'text': 'walk', 'start': 16648, 'end': 16840, 'confidence': 0.99926, 'speaker': None, 'channel': None}, {'text': 'through', 'start': 16880, 'end': 17080, 'confidence': 0.99726, 'speaker': None, 'channel': None}, {'text': 'them.', 'start': 17120, 'end': 17464, 'confidence': 0.9914, 'speaker': None, 'channel': None}, {'text': 'Number', 'start': 17552, 'end': 17800, 'confidence': 0.97466, 'speaker': None, 'channel': None}, {'text': 'one', 'start': 17840, 'end': 18040, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 18080, 'end': 18568, 'confidence': 0.99624, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 18704, 'end': 19048, 'confidence': 0.99853, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 19104, 'end': 19416, 'confidence': 0.99831, 'speaker': None, 'channel': None}, {'text': 'lack', 'start': 19488, 'end': 19960, 'confidence': 0.94781, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 20040, 'end': 20616, 'confidence': 0.99901, 'speaker': None, 'channel': None}, {'text': 'domain', 'start': 20768, 'end': 21672, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'knowledge.', 'start': 21816, 'end': 22600, 'confidence': 0.87386, 'speaker': None, 'channel': None}, {'text': 'Remember,', 'start': 22760, 'end': 23192, 'confidence': 0.85016, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 23256, 'end': 23528, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 23584, 'end': 23800, 'confidence': 0.99781, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 23840, 'end': 24216, 'confidence': 0.87562, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 24248, 'end': 24504, 'confidence': 0.53107, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 24552, 'end': 24712, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'been', 'start': 24736, 'end': 24872, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'trained', 'start': 24896, 'end': 25144, 'confidence': 0.96243, 'speaker': None, 'channel': None}, {'text': 'on', 'start': 25192, 'end': 25400, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'publicly', 'start': 25440, 'end': 25944, 'confidence': 0.97609, 'speaker': None, 'channel': None}, {'text': 'available', 'start': 25992, 'end': 26344, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 26432, 'end': 26680, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'sets.', 'start': 26720, 'end': 27064, 'confidence': 0.92469, 'speaker': None, 'channel': None}, {'text': 'That', 'start': 27112, 'end': 27272, 'confidence': 0.9986, 'speaker': None, 'channel': None}, {'text': 'means', 'start': 27296, 'end': 27480, 'confidence': 0.9982, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 27520, 'end': 27672, 'confidence': 0.9872, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 27696, 'end': 27832, 'confidence': 0.99926, 'speaker': None, 'channel': None}, {'text': \"don't\", 'start': 27856, 'end': 28056, 'confidence': 0.99771, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 28088, 'end': 28280, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'Access', 'start': 28320, 'end': 28616, 'confidence': 0.99859, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 28688, 'end': 29250, 'confidence': 0.99093, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 29400, 'end': 29758, 'confidence': 0.99267, 'speaker': None, 'channel': None}, {'text': 'SOPs,', 'start': 29814, 'end': 30334, 'confidence': 0.89151, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 30382, 'end': 30542, 'confidence': 0.96304, 'speaker': None, 'channel': None}, {'text': 'standard', 'start': 30566, 'end': 30846, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'operating', 'start': 30878, 'end': 31262, 'confidence': 0.63512, 'speaker': None, 'channel': None}, {'text': 'procedures,', 'start': 31326, 'end': 32046, 'confidence': 0.99754, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 32158, 'end': 32382, 'confidence': 0.99858, 'speaker': None, 'channel': None}, {'text': \"don't\", 'start': 32406, 'end': 32606, 'confidence': 0.9977, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 32638, 'end': 32782, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'access', 'start': 32806, 'end': 33086, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 33158, 'end': 33342, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 33366, 'end': 33502, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 33526, 'end': 33710, 'confidence': 0.99756, 'speaker': None, 'channel': None}, {'text': 'ip,', 'start': 33750, 'end': 34110, 'confidence': 0.81378, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 34190, 'end': 34382, 'confidence': 0.99392, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 34406, 'end': 34590, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'records.', 'start': 34630, 'end': 35374, 'confidence': 0.99821, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 35502, 'end': 35742, 'confidence': 0.99239, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 35766, 'end': 35950, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'really', 'start': 35990, 'end': 36238, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': \"can't\", 'start': 36294, 'end': 36574, 'confidence': 0.9967, 'speaker': None, 'channel': None}, {'text': 'answer', 'start': 36622, 'end': 37054, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 37102, 'end': 37358, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 37414, 'end': 37582, 'confidence': 0.99962, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 37606, 'end': 37838, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'questions', 'start': 37894, 'end': 38334, 'confidence': 0.99873, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 38382, 'end': 38638, 'confidence': 0.67912, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 38694, 'end': 38910, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 38950, 'end': 39294, 'confidence': 0.99897, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 39382, 'end': 39806, 'confidence': 0.99261, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 39838, 'end': 39982, 'confidence': 0.99732, 'speaker': None, 'channel': None}, {'text': 'tailored', 'start': 40006, 'end': 40286, 'confidence': 0.9928, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 40318, 'end': 40462, 'confidence': 0.99842, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 40486, 'end': 40622, 'confidence': 0.99934, 'speaker': None, 'channel': None}, {'text': 'particular', 'start': 40646, 'end': 40926, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'business.', 'start': 40998, 'end': 41326, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 41398, 'end': 41726, 'confidence': 0.9878, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 41798, 'end': 41982, 'confidence': 0.99939, 'speaker': None, 'channel': None}, {'text': 'lose', 'start': 42006, 'end': 42302, 'confidence': 0.99593, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 42366, 'end': 42542, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 42566, 'end': 42702, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 42726, 'end': 42862, 'confidence': 0.99875, 'speaker': None, 'channel': None}, {'text': 'performance', 'start': 42886, 'end': 43214, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 43302, 'end': 43502, 'confidence': 0.99142, 'speaker': None, 'channel': None}, {'text': 'effectiveness', 'start': 43526, 'end': 44142, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 44206, 'end': 44430, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 44470, 'end': 44622, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'that.', 'start': 44646, 'end': 45022, 'confidence': 0.99677, 'speaker': None, 'channel': None}, {'text': 'Issue', 'start': 45126, 'end': 45486, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'number', 'start': 45558, 'end': 45790, 'confidence': 0.99098, 'speaker': None, 'channel': None}, {'text': 'two', 'start': 45830, 'end': 46030, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 46070, 'end': 46270, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'around', 'start': 46310, 'end': 46558, 'confidence': 0.99851, 'speaker': None, 'channel': None}, {'text': 'hallucinations.', 'start': 46614, 'end': 47646, 'confidence': 0.98398, 'speaker': None, 'channel': None}, {'text': 'These', 'start': 47758, 'end': 48030, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'models', 'start': 48070, 'end': 48366, 'confidence': 0.99799, 'speaker': None, 'channel': None}, {'text': 'will', 'start': 48398, 'end': 48542, 'confidence': 0.99816, 'speaker': None, 'channel': None}, {'text': 'give', 'start': 48566, 'end': 48750, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 48790, 'end': 48990, 'confidence': 0.99943, 'speaker': None, 'channel': None}, {'text': 'responses.', 'start': 49030, 'end': 49582, 'confidence': 0.99881, 'speaker': None, 'channel': None}, {'text': 'They', 'start': 49646, 'end': 49870, 'confidence': 0.99913, 'speaker': None, 'channel': None}, {'text': 'look', 'start': 49910, 'end': 50206, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'really', 'start': 50278, 'end': 50558, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'credible,', 'start': 50614, 'end': 50974, 'confidence': 0.99936, 'speaker': None, 'channel': None}, {'text': 'but', 'start': 51022, 'end': 51182, 'confidence': 0.99972, 'speaker': None, 'channel': None}, {'text': \"they're\", 'start': 51206, 'end': 51454, 'confidence': 0.92674, 'speaker': None, 'channel': None}, {'text': 'way', 'start': 51502, 'end': 51710, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'off.', 'start': 51750, 'end': 51950, 'confidence': 0.998, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 51990, 'end': 52142, 'confidence': 0.9933, 'speaker': None, 'channel': None}, {'text': 'if', 'start': 52166, 'end': 52254, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 52262, 'end': 52430, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'run', 'start': 52470, 'end': 52670, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 52710, 'end': 52862, 'confidence': 0.99948, 'speaker': None, 'channel': None}, {'text': 'them,', 'start': 52886, 'end': 53070, 'confidence': 0.99864, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 53110, 'end': 53214, 'confidence': 0.99107, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 53222, 'end': 53438, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'might', 'start': 53494, 'end': 53662, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 53686, 'end': 53822, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 53846, 'end': 54030, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': 'problem.', 'start': 54070, 'end': 54558, 'confidence': 0.99916, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 54694, 'end': 54942, 'confidence': 0.97108, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 54966, 'end': 55102, 'confidence': 0.61985, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 55126, 'end': 55358, 'confidence': 0.9991, 'speaker': None, 'channel': None}, {'text': 'number', 'start': 55414, 'end': 55726, 'confidence': 0.99224, 'speaker': None, 'channel': None}, {'text': 'three,', 'start': 55798, 'end': 56030, 'confidence': 0.99929, 'speaker': None, 'channel': None}, {'text': 'which', 'start': 56070, 'end': 56222, 'confidence': 0.99927, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 56246, 'end': 56430, 'confidence': 0.8425, 'speaker': None, 'channel': None}, {'text': 'becoming', 'start': 56470, 'end': 56782, 'confidence': 0.99673, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 56846, 'end': 56974, 'confidence': 0.99887, 'speaker': None, 'channel': None}, {'text': 'little', 'start': 56982, 'end': 57102, 'confidence': 0.99835, 'speaker': None, 'channel': None}, {'text': 'bit', 'start': 57126, 'end': 57214, 'confidence': 0.99398, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 57222, 'end': 57342, 'confidence': 0.90287, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 57366, 'end': 57598, 'confidence': 0.90365, 'speaker': None, 'channel': None}, {'text': 'less', 'start': 57654, 'end': 57822, 'confidence': 0.99863, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 57846, 'end': 57982, 'confidence': 0.99669, 'speaker': None, 'channel': None}, {'text': 'an', 'start': 58006, 'end': 58094, 'confidence': 0.99907, 'speaker': None, 'channel': None}, {'text': 'issue', 'start': 58102, 'end': 58270, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 58310, 'end': 58510, 'confidence': 0.99753, 'speaker': None, 'channel': None}, {'text': 'search.', 'start': 58550, 'end': 59120, 'confidence': 0.97262, 'speaker': None, 'channel': None}, {'text': 'But', 'start': 59270, 'end': 59628, 'confidence': 0.9973, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 59684, 'end': 59900, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 59940, 'end': 60236, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'training', 'start': 60308, 'end': 60652, 'confidence': 0.93588, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 60716, 'end': 61036, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'cutoff', 'start': 61108, 'end': 61564, 'confidence': 0.67416, 'speaker': None, 'channel': None}, {'text': 'dates.', 'start': 61612, 'end': 62028, 'confidence': 0.90929, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 62124, 'end': 62284, 'confidence': 0.98489, 'speaker': None, 'channel': None}, {'text': 'for', 'start': 62292, 'end': 62412, 'confidence': 0.99934, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 62436, 'end': 62572, 'confidence': 0.96572, 'speaker': None, 'channel': None}, {'text': 'while', 'start': 62596, 'end': 62828, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'there', 'start': 62884, 'end': 63244, 'confidence': 0.99507, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 63332, 'end': 63532, 'confidence': 0.99865, 'speaker': None, 'channel': None}, {'text': 'had', 'start': 63556, 'end': 63692, 'confidence': 0.98363, 'speaker': None, 'channel': None}, {'text': 'chatgpt', 'start': 63716, 'end': 64540, 'confidence': 0.7027, 'speaker': None, 'channel': None}, {'text': 'missing', 'start': 64700, 'end': 65356, 'confidence': 0.52967, 'speaker': None, 'channel': None}, {'text': 'months', 'start': 65468, 'end': 65884, 'confidence': 0.99858, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 65972, 'end': 66268, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'training', 'start': 66324, 'end': 66604, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'data', 'start': 66652, 'end': 66908, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 66964, 'end': 67228, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 67284, 'end': 67452, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': \"hadn't\", 'start': 67476, 'end': 67708, 'confidence': 0.99567, 'speaker': None, 'channel': None}, {'text': 'been', 'start': 67724, 'end': 67900, 'confidence': 0.99973, 'speaker': None, 'channel': None}, {'text': 'updated', 'start': 67940, 'end': 68380, 'confidence': 0.99944, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 68460, 'end': 68652, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 68676, 'end': 68812, 'confidence': 0.99028, 'speaker': None, 'channel': None}, {'text': 'while', 'start': 68836, 'end': 68972, 'confidence': 0.99902, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 68996, 'end': 69132, 'confidence': 0.98259, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 69156, 'end': 69292, 'confidence': 0.99689, 'speaker': None, 'channel': None}, {'text': 'part', 'start': 69316, 'end': 69452, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'because', 'start': 69476, 'end': 69612, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 69636, 'end': 69772, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'takes', 'start': 69796, 'end': 69996, 'confidence': 0.99916, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 70028, 'end': 70124, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 70132, 'end': 70204, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 70212, 'end': 70332, 'confidence': 0.99906, 'speaker': None, 'channel': None}, {'text': 'compute', 'start': 70356, 'end': 70636, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 70668, 'end': 70812, 'confidence': 0.99705, 'speaker': None, 'channel': None}, {'text': 'train', 'start': 70836, 'end': 71228, 'confidence': 0.88696, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 71324, 'end': 71628, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'models.', 'start': 71684, 'end': 72140, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 72220, 'end': 72412, 'confidence': 0.9952, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 72436, 'end': 72524, 'confidence': 0.98723, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 72532, 'end': 72652, 'confidence': 0.9988, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 72676, 'end': 72860, 'confidence': 0.99944, 'speaker': None, 'channel': None}, {'text': 'three', 'start': 72900, 'end': 73100, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'issues', 'start': 73140, 'end': 73580, 'confidence': 0.99689, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 73700, 'end': 73932, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 73956, 'end': 74092, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'keeping', 'start': 74116, 'end': 74444, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 74492, 'end': 74700, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 74740, 'end': 74940, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'getting', 'start': 74980, 'end': 75372, 'confidence': 0.9993, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 75476, 'end': 75788, 'confidence': 0.9994, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 75844, 'end': 76204, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 76292, 'end': 76636, 'confidence': 0.99877, 'speaker': None, 'channel': None}, {'text': 'performance', 'start': 76708, 'end': 77180, 'confidence': 0.99961, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 77300, 'end': 77580, 'confidence': 0.99973, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 77620, 'end': 77820, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'LLMs', 'start': 77860, 'end': 78364, 'confidence': 0.9317, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 78412, 'end': 78572, 'confidence': 0.99976, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 78596, 'end': 78732, 'confidence': 0.99871, 'speaker': None, 'channel': None}, {'text': 'bring', 'start': 78756, 'end': 78940, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'them', 'start': 78980, 'end': 79180, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 79220, 'end': 79420, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'house.', 'start': 79460, 'end': 79708, 'confidence': 0.97514, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 79764, 'end': 79884, 'confidence': 0.97828, 'speaker': None, 'channel': None}, {'text': \"let's\", 'start': 79892, 'end': 80076, 'confidence': 0.91209, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 80108, 'end': 80252, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'about', 'start': 80276, 'end': 80652, 'confidence': 0.99282, 'speaker': None, 'channel': None}, {'text': 'one', 'start': 80756, 'end': 81068, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'pattern', 'start': 81124, 'end': 81484, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 81532, 'end': 81836, 'confidence': 0.99866, 'speaker': None, 'channel': None}, {'text': 'has', 'start': 81908, 'end': 82332, 'confidence': 0.9945, 'speaker': None, 'channel': None}, {'text': 'emerged', 'start': 82436, 'end': 83036, 'confidence': 0.99078, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 83068, 'end': 83260, 'confidence': 0.99755, 'speaker': None, 'channel': None}, {'text': 'being', 'start': 83300, 'end': 83500, 'confidence': 0.99941, 'speaker': None, 'channel': None}, {'text': 'particularly', 'start': 83540, 'end': 83964, 'confidence': 0.99895, 'speaker': None, 'channel': None}, {'text': 'helpful', 'start': 84012, 'end': 84316, 'confidence': 0.99905, 'speaker': None, 'channel': None}, {'text': 'here,', 'start': 84348, 'end': 84894, 'confidence': 0.98289, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 85052, 'end': 85322, 'confidence': 0.98498, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 85346, 'end': 85482, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 85506, 'end': 85786, 'confidence': 0.99891, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 85858, 'end': 86506, 'confidence': 0.77335, 'speaker': None, 'channel': None}, {'text': 'retrieval,', 'start': 86618, 'end': 87546, 'confidence': 0.98123, 'speaker': None, 'channel': None}, {'text': 'augmented', 'start': 87658, 'end': 88538, 'confidence': 0.53074, 'speaker': None, 'channel': None}, {'text': 'generation.', 'start': 88634, 'end': 89290, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'You', 'start': 89370, 'end': 89562, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'may', 'start': 89586, 'end': 89674, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 89682, 'end': 89802, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'heard', 'start': 89826, 'end': 90026, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 90058, 'end': 90250, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'term', 'start': 90290, 'end': 90538, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'toss', 'start': 90594, 'end': 90874, 'confidence': 0.99748, 'speaker': None, 'channel': None}, {'text': 'around,', 'start': 90922, 'end': 91178, 'confidence': 0.99271, 'speaker': None, 'channel': None}, {'text': 'but', 'start': 91234, 'end': 91402, 'confidence': 0.99938, 'speaker': None, 'channel': None}, {'text': \"let's\", 'start': 91426, 'end': 91626, 'confidence': 0.99912, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 91658, 'end': 91802, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'about', 'start': 91826, 'end': 92154, 'confidence': 0.99563, 'speaker': None, 'channel': None}, {'text': \"what's\", 'start': 92242, 'end': 92538, 'confidence': 0.99939, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 92554, 'end': 92682, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'on', 'start': 92706, 'end': 92938, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'here.', 'start': 92994, 'end': 93258, 'confidence': 0.99897, 'speaker': None, 'channel': None}, {'text': 'First,', 'start': 93314, 'end': 93722, 'confidence': 0.99867, 'speaker': None, 'channel': None}, {'text': 'let', 'start': 93826, 'end': 94042, 'confidence': 0.99742, 'speaker': None, 'channel': None}, {'text': 'me', 'start': 94066, 'end': 94154, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'give', 'start': 94162, 'end': 94282, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 94306, 'end': 94442, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'some', 'start': 94466, 'end': 94602, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'context.', 'start': 94626, 'end': 95178, 'confidence': 0.70344, 'speaker': None, 'channel': None}, {'text': \"Here's\", 'start': 95274, 'end': 95626, 'confidence': 0.99802, 'speaker': None, 'channel': None}, {'text': 'what', 'start': 95658, 'end': 95850, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'happens', 'start': 95890, 'end': 96266, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'when', 'start': 96298, 'end': 96586, 'confidence': 0.75484, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 96658, 'end': 96938, 'confidence': 0.99982, 'speaker': None, 'channel': None}, {'text': 'send', 'start': 96994, 'end': 97322, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 97386, 'end': 97658, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 97714, 'end': 98218, 'confidence': 0.99724, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 98314, 'end': 98522, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 98546, 'end': 98730, 'confidence': 0.9992, 'speaker': None, 'channel': None}, {'text': 'standard', 'start': 98770, 'end': 99114, 'confidence': 0.9985, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 99162, 'end': 99594, 'confidence': 0.99334, 'speaker': None, 'channel': None}, {'text': 'assistant.', 'start': 99642, 'end': 100394, 'confidence': 0.99849, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 100522, 'end': 100858, 'confidence': 0.98196, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 100914, 'end': 101082, 'confidence': 0.99964, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 101106, 'end': 101386, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'your.', 'start': 101458, 'end': 101978, 'confidence': 0.9397, 'speaker': None, 'channel': None}, {'text': 'Your', 'start': 102114, 'end': 102410, 'confidence': 0.98402, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 102450, 'end': 102890, 'confidence': 0.98313, 'speaker': None, 'channel': None}, {'text': 'goes', 'start': 102970, 'end': 103226, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 103258, 'end': 103402, 'confidence': 0.99919, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 103426, 'end': 103562, 'confidence': 0.99794, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 103586, 'end': 103786, 'confidence': 0.64565, 'speaker': None, 'channel': None}, {'text': 'assistant,', 'start': 103818, 'end': 104410, 'confidence': 0.99771, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 104490, 'end': 104682, 'confidence': 0.99358, 'speaker': None, 'channel': None}, {'text': 'generates', 'start': 104706, 'end': 105114, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 105162, 'end': 105274, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 105282, 'end': 105626, 'confidence': 0.99908, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 105658, 'end': 105802, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 105826, 'end': 105962, 'confidence': 0.99529, 'speaker': None, 'channel': None}, {'text': 'gives', 'start': 105986, 'end': 106218, 'confidence': 0.99578, 'speaker': None, 'channel': None}, {'text': 'it', 'start': 106234, 'end': 106410, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'back', 'start': 106450, 'end': 106650, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 106690, 'end': 106794, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'you.', 'start': 106802, 'end': 106970, 'confidence': 0.99915, 'speaker': None, 'channel': None}, {'text': 'Right?', 'start': 107010, 'end': 107546, 'confidence': 0.99317, 'speaker': None, 'channel': None}, {'text': 'In', 'start': 107698, 'end': 108106, 'confidence': 0.99157, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 108178, 'end': 108410, 'confidence': 0.99803, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 108450, 'end': 108810, 'confidence': 0.76677, 'speaker': None, 'channel': None}, {'text': 'implementation,', 'start': 108890, 'end': 109562, 'confidence': 0.96229, 'speaker': None, 'channel': None}, {'text': \"you're\", 'start': 109626, 'end': 109866, 'confidence': 0.99715, 'speaker': None, 'channel': None}, {'text': 'adding', 'start': 109898, 'end': 110282, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': 'an', 'start': 110346, 'end': 110570, 'confidence': 0.95987, 'speaker': None, 'channel': None}, {'text': 'additional', 'start': 110610, 'end': 111242, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'step', 'start': 111306, 'end': 111626, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'here.', 'start': 111698, 'end': 112212, 'confidence': 0.99461, 'speaker': None, 'channel': None}, {'text': 'Before', 'start': 112346, 'end': 112832, 'confidence': 0.99811, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 112936, 'end': 113248, 'confidence': 0.99491, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 113304, 'end': 113808, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'goes', 'start': 113904, 'end': 114480, 'confidence': 0.99764, 'speaker': None, 'channel': None}, {'text': 'into', 'start': 114560, 'end': 114848, 'confidence': 0.99927, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 114904, 'end': 115120, 'confidence': 0.99885, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 115160, 'end': 115504, 'confidence': 0.51374, 'speaker': None, 'channel': None}, {'text': 'assistant,', 'start': 115552, 'end': 116304, 'confidence': 0.95104, 'speaker': None, 'channel': None}, {'text': 'we', 'start': 116432, 'end': 116672, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 116696, 'end': 116928, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 116984, 'end': 117248, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'search', 'start': 117304, 'end': 117728, 'confidence': 0.98986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 117824, 'end': 118080, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'hits', 'start': 118120, 'end': 118464, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 118512, 'end': 118672, 'confidence': 0.99922, 'speaker': None, 'channel': None}, {'text': 'corpus', 'start': 118696, 'end': 119104, 'confidence': 0.98243, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 119152, 'end': 119264, 'confidence': 0.9984, 'speaker': None, 'channel': None}, {'text': 'data.', 'start': 119272, 'end': 119632, 'confidence': 0.99849, 'speaker': None, 'channel': None}, {'text': 'Now', 'start': 119736, 'end': 119952, 'confidence': 0.99222, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 119976, 'end': 120112, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 120136, 'end': 120272, 'confidence': 0.99754, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 120296, 'end': 120384, 'confidence': 0.89301, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 120392, 'end': 120464, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 120472, 'end': 120592, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 120616, 'end': 120800, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'data,', 'start': 120840, 'end': 121136, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 121208, 'end': 121440, 'confidence': 0.999, 'speaker': None, 'channel': None}, {'text': 'own', 'start': 121480, 'end': 121680, 'confidence': 0.99943, 'speaker': None, 'channel': None}, {'text': 'documents', 'start': 121720, 'end': 122512, 'confidence': 0.98816, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 122656, 'end': 123344, 'confidence': 0.9881, 'speaker': None, 'channel': None}, {'text': 'other', 'start': 123512, 'end': 123888, 'confidence': 0.99373, 'speaker': None, 'channel': None}, {'text': 'relevant', 'start': 123944, 'end': 124528, 'confidence': 0.99962, 'speaker': None, 'channel': None}, {'text': 'information', 'start': 124624, 'end': 125024, 'confidence': 0.99997, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 125112, 'end': 125312, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 125336, 'end': 125472, 'confidence': 0.99993, 'speaker': None, 'channel': None}, {'text': 'want', 'start': 125496, 'end': 125632, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 125656, 'end': 125744, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'make', 'start': 125752, 'end': 125872, 'confidence': 0.99993, 'speaker': None, 'channel': None}, {'text': 'available', 'start': 125896, 'end': 126224, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 126312, 'end': 126464, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 126472, 'end': 126592, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 126616, 'end': 126896, 'confidence': 0.8474, 'speaker': None, 'channel': None}, {'text': 'assistant.', 'start': 126928, 'end': 127520, 'confidence': 0.99676, 'speaker': None, 'channel': None}, {'text': \"There's\", 'start': 127600, 'end': 127888, 'confidence': 0.985, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 127904, 'end': 127984, 'confidence': 0.99442, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 127992, 'end': 128160, 'confidence': 0.94329, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 128200, 'end': 128496, 'confidence': 0.99967, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 128568, 'end': 128848, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 128904, 'end': 129584, 'confidence': 0.96697, 'speaker': None, 'channel': None}, {'text': \"that's\", 'start': 129632, 'end': 129936, 'confidence': 0.99703, 'speaker': None, 'channel': None}, {'text': 'done', 'start': 129968, 'end': 130304, 'confidence': 0.99821, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 130392, 'end': 130592, 'confidence': 0.98417, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 130616, 'end': 130848, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'context', 'start': 130904, 'end': 131392, 'confidence': 0.95338, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 131456, 'end': 131632, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 131656, 'end': 131840, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 131880, 'end': 131984, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'be', 'start': 131992, 'end': 132208, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'added', 'start': 132264, 'end': 132816, 'confidence': 0.99819, 'speaker': None, 'channel': None}, {'text': 'in', 'start': 132968, 'end': 133280, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'addition', 'start': 133320, 'end': 133616, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 133688, 'end': 133920, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 133960, 'end': 134160, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'original', 'start': 134200, 'end': 134752, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'prompt.', 'start': 134816, 'end': 135328, 'confidence': 0.99169, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 135424, 'end': 135728, 'confidence': 0.95505, 'speaker': None, 'channel': None}, {'text': 'so', 'start': 135784, 'end': 136096, 'confidence': 0.99938, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 136168, 'end': 136352, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'large', 'start': 136376, 'end': 136560, 'confidence': 0.99812, 'speaker': None, 'channel': None}, {'text': 'language', 'start': 136600, 'end': 136944, 'confidence': 0.95847, 'speaker': None, 'channel': None}, {'text': 'model', 'start': 136992, 'end': 137376, 'confidence': 0.99965, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 137408, 'end': 137552, 'confidence': 0.99949, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 137576, 'end': 137712, 'confidence': 0.99827, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 137736, 'end': 137824, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'get', 'start': 137832, 'end': 137952, 'confidence': 0.9986, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 137976, 'end': 138160, 'confidence': 0.99948, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 138200, 'end': 138544, 'confidence': 0.99894, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 138592, 'end': 138704, 'confidence': 0.99819, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 138712, 'end': 138880, 'confidence': 0.99716, 'speaker': None, 'channel': None}, {'text': 'also', 'start': 138920, 'end': 139264, 'confidence': 0.99954, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 139352, 'end': 139600, 'confidence': 0.99914, 'speaker': None, 'channel': None}, {'text': 'relevant', 'start': 139640, 'end': 140064, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'information', 'start': 140112, 'end': 140608, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 140744, 'end': 141022, 'confidence': 0.38553, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 141056, 'end': 141298, 'confidence': 0.99536, 'speaker': None, 'channel': None}, {'text': 'was', 'start': 141354, 'end': 141762, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'found', 'start': 141866, 'end': 142226, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'during', 'start': 142298, 'end': 142722, 'confidence': 0.99985, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 142826, 'end': 143138, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'process.', 'start': 143194, 'end': 143746, 'confidence': 0.99862, 'speaker': None, 'channel': None}, {'text': 'And', 'start': 143898, 'end': 144162, 'confidence': 0.98826, 'speaker': None, 'channel': None}, {'text': 'then', 'start': 144186, 'end': 144514, 'confidence': 0.9983, 'speaker': None, 'channel': None}, {'text': 'everything', 'start': 144602, 'end': 144898, 'confidence': 0.99917, 'speaker': None, 'channel': None}, {'text': 'else', 'start': 144954, 'end': 145186, 'confidence': 0.9997, 'speaker': None, 'channel': None}, {'text': 'proceeds', 'start': 145218, 'end': 145746, 'confidence': 0.81719, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 145778, 'end': 145922, 'confidence': 0.99945, 'speaker': None, 'channel': None}, {'text': 'same', 'start': 145946, 'end': 146130, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'way.', 'start': 146170, 'end': 146322, 'confidence': 0.99931, 'speaker': None, 'channel': None}, {'text': 'The', 'start': 146346, 'end': 146482, 'confidence': 0.99937, 'speaker': None, 'channel': None}, {'text': 'AI', 'start': 146506, 'end': 146786, 'confidence': 0.93205, 'speaker': None, 'channel': None}, {'text': 'system', 'start': 146818, 'end': 147106, 'confidence': 0.56095, 'speaker': None, 'channel': None}, {'text': 'will', 'start': 147178, 'end': 147362, 'confidence': 0.99839, 'speaker': None, 'channel': None}, {'text': 'process', 'start': 147386, 'end': 147618, 'confidence': 0.99958, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 147674, 'end': 148034, 'confidence': 0.9741, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 148122, 'end': 148418, 'confidence': 0.99911, 'speaker': None, 'channel': None}, {'text': 'generate', 'start': 148474, 'end': 148962, 'confidence': 0.99905, 'speaker': None, 'channel': None}, {'text': 'typically', 'start': 149026, 'end': 149394, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 149442, 'end': 149650, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'better', 'start': 149690, 'end': 149938, 'confidence': 0.99987, 'speaker': None, 'channel': None}, {'text': 'response', 'start': 149994, 'end': 150434, 'confidence': 0.75474, 'speaker': None, 'channel': None}, {'text': 'for', 'start': 150482, 'end': 150690, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 150730, 'end': 151026, 'confidence': 0.9998, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 151098, 'end': 151378, 'confidence': 0.99923, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 151434, 'end': 151698, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'user.', 'start': 151754, 'end': 152562, 'confidence': 0.99782, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 152706, 'end': 152962, 'confidence': 0.99749, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 152986, 'end': 153122, 'confidence': 0.99952, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 153146, 'end': 153666, 'confidence': 0.91142, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 153698, 'end': 153986, 'confidence': 0.9995, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 154058, 'end': 154338, 'confidence': 0.99941, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 154394, 'end': 154610, 'confidence': 0.99915, 'speaker': None, 'channel': None}, {'text': 'retrieval', 'start': 154650, 'end': 155186, 'confidence': 0.95684, 'speaker': None, 'channel': None}, {'text': 'function', 'start': 155218, 'end': 155506, 'confidence': 0.99932, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 155538, 'end': 155778, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'where', 'start': 155834, 'end': 156002, 'confidence': 0.9853, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 156026, 'end': 156226, 'confidence': 0.53037, 'speaker': None, 'channel': None}, {'text': 'grabbing', 'start': 156258, 'end': 156818, 'confidence': 0.99696, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 156914, 'end': 157074, 'confidence': 0.99853, 'speaker': None, 'channel': None}, {'text': 'information.', 'start': 157082, 'end': 157394, 'confidence': 0.99924, 'speaker': None, 'channel': None}, {'text': 'The', 'start': 157482, 'end': 157730, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'augmented', 'start': 157770, 'end': 158386, 'confidence': 0.98869, 'speaker': None, 'channel': None}, {'text': 'part', 'start': 158418, 'end': 158610, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 158650, 'end': 158850, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 158890, 'end': 159042, 'confidence': 0.99869, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 159066, 'end': 159266, 'confidence': 0.98866, 'speaker': None, 'channel': None}, {'text': 'augmenting', 'start': 159298, 'end': 159826, 'confidence': 0.99323, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 159858, 'end': 160002, 'confidence': 0.99798, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 160026, 'end': 160114, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 160122, 'end': 160194, 'confidence': 0.9998, 'speaker': None, 'channel': None}, {'text': 'original', 'start': 160202, 'end': 160562, 'confidence': 0.9976, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 160626, 'end': 160962, 'confidence': 0.72399, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 161026, 'end': 161250, 'confidence': 0.59248, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 161290, 'end': 161442, 'confidence': 0.99956, 'speaker': None, 'channel': None}, {'text': 'generation', 'start': 161466, 'end': 161874, 'confidence': 0.99918, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 161922, 'end': 162082, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 162106, 'end': 162242, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 162266, 'end': 162354, 'confidence': 0.99953, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 162362, 'end': 162546, 'confidence': 0.9947, 'speaker': None, 'channel': None}, {'text': 'generation', 'start': 162578, 'end': 163186, 'confidence': 0.70973, 'speaker': None, 'channel': None}, {'text': 'generating', 'start': 163298, 'end': 163986, 'confidence': 0.99562, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 164098, 'end': 164322, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'same', 'start': 164346, 'end': 164530, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'prompt', 'start': 164570, 'end': 164786, 'confidence': 0.99625, 'speaker': None, 'channel': None}, {'text': 'here', 'start': 164818, 'end': 165058, 'confidence': 0.99757, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 165114, 'end': 165282, 'confidence': 0.99969, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 165306, 'end': 165490, 'confidence': 0.99947, 'speaker': None, 'channel': None}, {'text': 'LLM.', 'start': 165530, 'end': 166280, 'confidence': 0.92993, 'speaker': None, 'channel': None}, {'text': 'It', 'start': 166410, 'end': 166652, 'confidence': 0.98598, 'speaker': None, 'channel': None}, {'text': 'turns', 'start': 166676, 'end': 166908, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'out', 'start': 166924, 'end': 167052, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 167076, 'end': 167212, 'confidence': 0.99892, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 167236, 'end': 167372, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'is', 'start': 167396, 'end': 167580, 'confidence': 0.99946, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 167620, 'end': 167964, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'robust,', 'start': 168052, 'end': 168844, 'confidence': 0.87242, 'speaker': None, 'channel': None}, {'text': 'efficient', 'start': 168972, 'end': 169612, 'confidence': 0.9238, 'speaker': None, 'channel': None}, {'text': 'way', 'start': 169676, 'end': 170044, 'confidence': 0.99989, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 170132, 'end': 170428, 'confidence': 0.99959, 'speaker': None, 'channel': None}, {'text': 'tackle', 'start': 170484, 'end': 171116, 'confidence': 0.99742, 'speaker': None, 'channel': None}, {'text': 'these', 'start': 171228, 'end': 171500, 'confidence': 0.99903, 'speaker': None, 'channel': None}, {'text': 'problems', 'start': 171540, 'end': 171868, 'confidence': 0.99951, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 171884, 'end': 172012, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': \"we're\", 'start': 172036, 'end': 172236, 'confidence': 0.98818, 'speaker': None, 'channel': None}, {'text': 'seeing', 'start': 172268, 'end': 172476, 'confidence': 0.99286, 'speaker': None, 'channel': None}, {'text': 'with', 'start': 172508, 'end': 172700, 'confidence': 0.99968, 'speaker': None, 'channel': None}, {'text': 'LLMs', 'start': 172740, 'end': 173244, 'confidence': 0.91888, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 173292, 'end': 173452, 'confidence': 0.64282, 'speaker': None, 'channel': None}, {'text': 'industry', 'start': 173476, 'end': 173996, 'confidence': 0.99828, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 174148, 'end': 174556, 'confidence': 0.99779, 'speaker': None, 'channel': None}, {'text': 'business.', 'start': 174628, 'end': 175196, 'confidence': 0.99822, 'speaker': None, 'channel': None}, {'text': 'So', 'start': 175348, 'end': 175564, 'confidence': 0.99535, 'speaker': None, 'channel': None}, {'text': 'I', 'start': 175572, 'end': 175692, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'hope', 'start': 175716, 'end': 175948, 'confidence': 0.99984, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 176004, 'end': 176172, 'confidence': 0.99881, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 176196, 'end': 176332, 'confidence': 0.99928, 'speaker': None, 'channel': None}, {'text': 'helps', 'start': 176356, 'end': 176684, 'confidence': 0.99932, 'speaker': None, 'channel': None}, {'text': 'explain', 'start': 176732, 'end': 177260, 'confidence': 0.99942, 'speaker': None, 'channel': None}, {'text': 'the', 'start': 177340, 'end': 177676, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'RAG', 'start': 177748, 'end': 178268, 'confidence': 0.9594, 'speaker': None, 'channel': None}, {'text': 'framework.', 'start': 178364, 'end': 178892, 'confidence': 0.99665, 'speaker': None, 'channel': None}, {'text': 'If', 'start': 178956, 'end': 179132, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 179156, 'end': 179244, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'have', 'start': 179252, 'end': 179372, 'confidence': 0.99983, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 179396, 'end': 179580, 'confidence': 0.99971, 'speaker': None, 'channel': None}, {'text': 'questions,', 'start': 179620, 'end': 180172, 'confidence': 0.99957, 'speaker': None, 'channel': None}, {'text': 'any', 'start': 180236, 'end': 180460, 'confidence': 0.99753, 'speaker': None, 'channel': None}, {'text': 'comments', 'start': 180500, 'end': 180796, 'confidence': 0.99908, 'speaker': None, 'channel': None}, {'text': 'if', 'start': 180868, 'end': 181052, 'confidence': 0.99977, 'speaker': None, 'channel': None}, {'text': 'I', 'start': 181076, 'end': 181212, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': 'missed', 'start': 181236, 'end': 181516, 'confidence': 0.73098, 'speaker': None, 'channel': None}, {'text': 'anything,', 'start': 181548, 'end': 182108, 'confidence': 0.73069, 'speaker': None, 'channel': None}, {'text': 'please', 'start': 182204, 'end': 182556, 'confidence': 0.99991, 'speaker': None, 'channel': None}, {'text': 'drop', 'start': 182628, 'end': 182876, 'confidence': 0.99963, 'speaker': None, 'channel': None}, {'text': 'them', 'start': 182908, 'end': 183052, 'confidence': 0.99788, 'speaker': None, 'channel': None}, {'text': 'below.', 'start': 183076, 'end': 183500, 'confidence': 0.99883, 'speaker': None, 'channel': None}, {'text': 'If', 'start': 183620, 'end': 183852, 'confidence': 0.99979, 'speaker': None, 'channel': None}, {'text': \"you're\", 'start': 183876, 'end': 184076, 'confidence': 0.97379, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 184108, 'end': 184204, 'confidence': 0.96876, 'speaker': None, 'channel': None}, {'text': 'practitioner', 'start': 184212, 'end': 184636, 'confidence': 0.99526, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 184668, 'end': 184764, 'confidence': 0.7334, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 184772, 'end': 184892, 'confidence': 0.98492, 'speaker': None, 'channel': None}, {'text': 'want', 'start': 184916, 'end': 185052, 'confidence': 0.77671, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 185076, 'end': 185212, 'confidence': 0.99902, 'speaker': None, 'channel': None}, {'text': 'add', 'start': 185236, 'end': 185420, 'confidence': 0.99978, 'speaker': None, 'channel': None}, {'text': 'something', 'start': 185460, 'end': 185660, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 185700, 'end': 185852, 'confidence': 0.99901, 'speaker': None, 'channel': None}, {'text': 'this', 'start': 185876, 'end': 186012, 'confidence': 0.99955, 'speaker': None, 'channel': None}, {'text': 'conversation,', 'start': 186036, 'end': 186492, 'confidence': 0.99848, 'speaker': None, 'channel': None}, {'text': 'please', 'start': 186556, 'end': 186780, 'confidence': 0.99988, 'speaker': None, 'channel': None}, {'text': 'drop', 'start': 186820, 'end': 187036, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 187068, 'end': 187212, 'confidence': 0.99649, 'speaker': None, 'channel': None}, {'text': 'below', 'start': 187236, 'end': 187420, 'confidence': 0.99976, 'speaker': None, 'channel': None}, {'text': 'as', 'start': 187460, 'end': 187564, 'confidence': 0.99834, 'speaker': None, 'channel': None}, {'text': 'well.', 'start': 187572, 'end': 187692, 'confidence': 0.99896, 'speaker': None, 'channel': None}, {'text': 'There', 'start': 187716, 'end': 187804, 'confidence': 0.99776, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 187812, 'end': 187884, 'confidence': 0.99067, 'speaker': None, 'channel': None}, {'text': 'a', 'start': 187892, 'end': 187964, 'confidence': 0.99872, 'speaker': None, 'channel': None}, {'text': 'lot', 'start': 187972, 'end': 188044, 'confidence': 0.9999, 'speaker': None, 'channel': None}, {'text': 'of', 'start': 188052, 'end': 188124, 'confidence': 0.99834, 'speaker': None, 'channel': None}, {'text': 'people', 'start': 188132, 'end': 188252, 'confidence': 0.99992, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 188276, 'end': 188412, 'confidence': 0.99836, 'speaker': None, 'channel': None}, {'text': 'are', 'start': 188436, 'end': 188524, 'confidence': 0.99876, 'speaker': None, 'channel': None}, {'text': 'going', 'start': 188532, 'end': 188652, 'confidence': 0.90387, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 188676, 'end': 188764, 'confidence': 0.99904, 'speaker': None, 'channel': None}, {'text': 'watch', 'start': 188772, 'end': 188892, 'confidence': 0.99921, 'speaker': None, 'channel': None}, {'text': 'this,', 'start': 188916, 'end': 189148, 'confidence': 0.99878, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 189204, 'end': 189372, 'confidence': 0.99528, 'speaker': None, 'channel': None}, {'text': \"I'm\", 'start': 189396, 'end': 189548, 'confidence': 0.99925, 'speaker': None, 'channel': None}, {'text': 'sure', 'start': 189564, 'end': 189692, 'confidence': 0.99986, 'speaker': None, 'channel': None}, {'text': 'that', 'start': 189716, 'end': 189852, 'confidence': 0.99673, 'speaker': None, 'channel': None}, {'text': 'they', 'start': 189876, 'end': 190012, 'confidence': 0.9996, 'speaker': None, 'channel': None}, {'text': 'would', 'start': 190036, 'end': 190172, 'confidence': 0.99847, 'speaker': None, 'channel': None}, {'text': 'benefit', 'start': 190196, 'end': 190732, 'confidence': 0.99981, 'speaker': None, 'channel': None}, {'text': 'from', 'start': 190796, 'end': 191068, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 191124, 'end': 191340, 'confidence': 0.99975, 'speaker': None, 'channel': None}, {'text': 'expertise', 'start': 191380, 'end': 191884, 'confidence': 0.97094, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 191932, 'end': 192044, 'confidence': 0.99733, 'speaker': None, 'channel': None}, {'text': 'your', 'start': 192052, 'end': 192172, 'confidence': 0.99792, 'speaker': None, 'channel': None}, {'text': 'knowledge.', 'start': 192196, 'end': 192668, 'confidence': 0.99784, 'speaker': None, 'channel': None}, {'text': 'Thanks', 'start': 192764, 'end': 193084, 'confidence': 0.9973, 'speaker': None, 'channel': None}, {'text': 'and', 'start': 193132, 'end': 193436, 'confidence': 0.73072, 'speaker': None, 'channel': None}, {'text': 'talk', 'start': 193508, 'end': 193692, 'confidence': 0.99974, 'speaker': None, 'channel': None}, {'text': 'to', 'start': 193716, 'end': 193852, 'confidence': 0.99966, 'speaker': None, 'channel': None}, {'text': 'you', 'start': 193876, 'end': 193964, 'confidence': 0.9965, 'speaker': None, 'channel': None}, {'text': 'soon.', 'start': 193972, 'end': 194020, 'confidence': 0.99913, 'speaker': None, 'channel': None}], 'utterances': None, 'confidence': 0.9741621, 'audio_duration': 195, 'webhook_status_code': None, 'webhook_auth': False, 'summary': None, 'auto_highlights_result': None, 'content_safety_labels': None, 'iab_categories_result': None, 'chapters': None, 'sentiment_analysis_results': None, 'entities': None}, page_content=\"is that we're generation generating the same prompt here from the LLM. It turns out that this is a robust, efficient way to tackle these problems that we're seeing with LLMs and industry and business. So I hope that this helps explain the RAG framework. If you have any questions, any comments if I missed anything, please drop them below. If you're a practitioner and you want to add something to this conversation, please drop that below as well. There are a lot of people that are going to watch this, and I'm sure that they would benefit from your expertise and your knowledge. Thanks and talk to you soon.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = pdf_docs + img_docs + audio_docs"
      ],
      "metadata": {
        "id": "hjOWAeIMgbHz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnw6biEgc09",
        "outputId": "5d208d99-ce2c-487a-a5f3-b04e0254898b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3. Create vector store using Cohere and Chroma (For English lang.)"
      ],
      "metadata": {
        "id": "UtpzTMQlLGaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "embedding_model = CohereEmbeddings(model=\"embed-english-v3.0\")"
      ],
      "metadata": {
        "id": "8gCvTeXfLMBj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=filter_complex_metadata(docs),\n",
        "    collection_name=\"rag\",\n",
        "    embedding=embedding_model,\n",
        ")"
      ],
      "metadata": {
        "id": "1Oy_CtCWLlSD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4. Obtain retriever"
      ],
      "metadata": {
        "id": "FyI5OMPUMSQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={'k': 4},\n",
        "            )"
      ],
      "metadata": {
        "id": "je4KKXZpMT5T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Create Pydantic Schemas:"
      ],
      "metadata": {
        "id": "_O2LcM2SMd8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class TopicSchema(BaseModel):\n",
        "    title: str = Field(..., description=\"The title of the topic covered in the cheat sheet.\")\n",
        "    description: str = Field(..., description=\"A brief description or summary of the topic.\")\n",
        "    key_points: List[str] = Field(..., description=\"List of key takeaways or bullet points for the topic.\")\n",
        "    resources: List[str] = Field(..., description=\"List of external resources or references for the topic. It must not be empty\")\n",
        "\n",
        "class ExampleSchema(BaseModel):\n",
        "    example_title: str = Field(..., description=\"The title or name of the example.\")\n",
        "    code_snippet: str = Field(..., description=\"A code snippet or text representing the example.\")\n",
        "    explanation: str = Field(..., description=\"A detailed explanation of the example and its purpose.\")\n",
        "    output: str = Field(..., description=\"The expected output or result of the example when executed.\")\n",
        "    tags: List[str] = Field(..., description=\"Tags or keywords associated with the example for indexing.\")\n",
        "\n",
        "class SectionSchema(BaseModel):\n",
        "    section_title: str = Field(..., description=\"The title of the section.\")\n",
        "    topics: List[TopicSchema] = Field(..., description=\"List of topics included in this section.\")\n",
        "    examples: List[ExampleSchema] = Field(..., description=\"List of examples relevant to the section.\")\n",
        "    summary: str = Field(..., description=\"A brief summary of the section's content and purpose.\")\n",
        "    importance: int = Field(..., description=\"An importance level or priority score for the section.\")\n",
        "\n",
        "class MetadataSchema(BaseModel):\n",
        "    author: str = Field(..., description=\"The name of the cheat sheet's author.\")\n",
        "    version: str = Field(..., description=\"The version of the cheat sheet.\")\n",
        "    last_updated: str = Field(..., description=\"The last updated date of the cheat sheet in ISO 8601 format.\")\n",
        "    created_date: str = Field(..., description=\"The creation date of the cheat sheet in ISO 8601 format.\")\n",
        "    tags: List[str] = Field(..., description=\"A list of tags or keywords describing the cheat sheet.\")\n",
        "    intended_audience: str = Field(..., description=\"The primary audience or user group for the cheat sheet.\")\n",
        "    license: str = Field(..., description=\"The licensing information for using or sharing the cheat sheet.\")\n",
        "\n",
        "class CheatSheetSchema(BaseModel):\n",
        "    title: str = Field(..., description=\"The main title of the cheat sheet.\")\n",
        "    description: str = Field(..., description=\"A brief overview or introduction to the cheat sheet.\")\n",
        "    metadata: MetadataSchema = Field(..., description=\"Metadata information about the cheat sheet.\")\n",
        "    sections: List[SectionSchema] = Field(..., description=\"A list of sections containing the cheat sheet's content.\")\n",
        "    total_sections: int = Field(..., description=\"The total number of sections in the cheat sheet.\")\n",
        "    difficulty_level: str = Field(..., description=\"The difficulty level of the cheat sheet content.\")\n",
        "    usage_scenarios: List[str] = Field(..., description=\"Scenarios or contexts where this cheat sheet is useful.\")"
      ],
      "metadata": {
        "id": "gogQFnslNUfv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Create RAG chains with Llama and Mixtral"
      ],
      "metadata": {
        "id": "39LrWgPeNuce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser"
      ],
      "metadata": {
        "id": "gHUKHEaDUELh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm_llama_3_1 = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "llm_mixtral_8_7b = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)"
      ],
      "metadata": {
        "id": "nhBqWqcqUG6N"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"Create a Cheat Sheet for Large Language Models\"\n",
        "language = \"English\"\n",
        "context = retriever.invoke(topic)"
      ],
      "metadata": {
        "id": "eKCt190BTPIy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO_nPSoEUg2V",
        "outputId": "6954f424-b917-40bb-a27d-86387db15f5a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 8, 'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj'}, page_content='that compared to multiple-choice tasks, generative tasks face the problem of accumulated\\nerrors and large model is more robust than small one. The reasons behind it still need to\\nbe explored. The post-training techniques discussed in Section 4.6 have the potential to\\nmitigate this issue and warrant further exploration.\\n6 Related works\\nTo reduce the inference cost of large language models and increase their practical applica-\\ntions, there have been many recent works on compressing models, which can be classified\\ninto two categories: model pruning and quantization. Besides, there are some works aim to\\nstudy the redundancy of model which is essential for compressing models.\\nModel pruning: model pruning (LeCun et al., 1989; Han et al., 2015) is a classic and effective\\nmethod of reducing model redundancy modules to compress models. The model pruning\\nmethods mainly include unstructured pruning and structured pruning. The unstructured'),\n",
              " Document(metadata={'page': 5, 'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj'}, page_content='4.2 Main Results\\nTo validate the efficacy of our proposed method, we conducted comparative experiments\\nagainst baseline techniques commonly employed in large language model evaluation.\\nConsidering the current structured pruning methods generally reduce parameters by no\\nmore than 30%, we performed experiments with approximately 1/4 of the parameters\\npruned. The experimental results are presented in Table 2. Additional experiments exploring\\ndifferent parameter reduction proportions will be discussed in the subsequent section.\\nThe results demonstrate that the performance of the model pruned by our method signif-\\nicantly surpasses that of the baseline methods, maintaining most of the large language\\nmodel’s capabilities. Furthermore, we note that the approach of reducing the number of\\nlayers (ShortGPT/LaCo) outperforms the method of reducing the embedding dimensions\\n(LLMPru./SliceGPT), implying that the model exhibits more redundancy in depth than in'),\n",
              " Document(metadata={'audio_duration': 195, 'audio_url': 'https://cdn.assemblyai.com/upload/0c0fbf9b-b080-48a9-8611-eae24774762d', 'auto_chapters': False, 'auto_highlights': False, 'confidence': 0.9741621, 'content_safety': False, 'disfluencies': False, 'entity_detection': False, 'filter_profanity': False, 'format_text': True, 'iab_categories': False, 'id': '656ac874-63b0-48ef-808e-e9c01d6f394f', 'language_code': 'en_us', 'language_detection': False, 'punctuate': True, 'redact_pii': False, 'redact_pii_audio': False, 'sentiment_analysis': False, 'speaker_labels': False, 'status': 'completed', 'summarization': False, 'text': \"Large language models and AI assistants are taking the consumer world by storm. But what happens when you try to bring these large language models into business, into enterprise? Well, we have three issues, three issues that we have to overcome. Let's walk through them. Number one issue is this lack of domain knowledge. Remember, these large language models have been trained on publicly available data sets. That means that they don't have Access to your SOPs, your standard operating procedures, they don't have access to your own ip, your own records. So they really can't answer a lot of questions and have that response be tailored to your particular business. And you lose a lot of performance and effectiveness because of that. Issue number two is around hallucinations. These models will give you responses. They look really credible, but they're way off. And if you run with them, then you might have a problem. And then issue number three, which is becoming a little bit of a less of an issue with search. But we have training data cutoff dates. So for a while there you had chatgpt missing months of training data because it hadn't been updated in a while and in part because it takes a lot of compute to train these models. So you have these three issues that are keeping you from getting a lot of performance from your LLMs as you bring them in house. So let's talk about one pattern that has emerged as being particularly helpful here, and that is RAG retrieval, augmented generation. You may have heard this term toss around, but let's talk about what's going on here. First, let me give you some context. Here's what happens when you send a prompt to your standard AI assistant. So you have your. Your prompt goes into the AI assistant, it generates a response and then gives it back to you. Right? In a RAG implementation, you're adding an additional step here. Before that prompt goes into the AI assistant, we have a search that hits a corpus of data. Now this is going to be your data, your own documents and other relevant information that you want to make available to the AI assistant. There's going to be a retrieval that's done and that context is going to be added in addition to your original prompt. And so the large language model is going to get your prompt and then also any relevant information that that was found during this process. And then everything else proceeds the same way. The AI system will process that and generate typically a better response for you as a user. So the retrieval here is this retrieval function here where we're grabbing the information. The augmented part here is we're augmenting that to the original prompt and the generation here is that we're generation generating the same prompt here from the LLM. It turns out that this is a robust, efficient way to tackle these problems that we're seeing with LLMs and industry and business. So I hope that this helps explain the RAG framework. If you have any questions, any comments if I missed anything, please drop them below. If you're a practitioner and you want to add something to this conversation, please drop that below as well. There are a lot of people that are going to watch this, and I'm sure that they would benefit from your expertise and your knowledge. Thanks and talk to you soon.\", 'webhook_auth': False}, page_content=\"Large language models and AI assistants are taking the consumer world by storm. But what happens when you try to bring these large language models into business, into enterprise? Well, we have three issues, three issues that we have to overcome. Let's walk through them. Number one issue is this lack of domain knowledge. Remember, these large language models have been trained on publicly available data sets. That means that they don't have Access to your SOPs, your standard operating procedures, they don't have access to your own ip, your own records. So they really can't answer a lot of questions and have that response be tailored to your particular business. And you lose a lot of performance and effectiveness because of that. Issue number two is around hallucinations. These models will give you responses. They look really credible, but they're way off. And if you run with them, then you might have a problem. And then issue number three, which is becoming a little bit of a less of an\"),\n",
              " Document(metadata={'page': 0, 'source': '/tmp/97a9252c-660d-43d0-b1d3-18e550cecee6.pdfjzc91tzj'}, page_content='ShortGPT: Layers in Large Language Models are More Redun-\\ndant Than You Expect\\nXin Men∗\\nBaichuan Inc.\\nMingyu Xu∗\\nBaichuan Inc.\\nQingyu Zhang∗\\nISCAS\\nBingning Wang †\\nBaichuan Inc.\\nHongyu Lin\\nISCAS\\nYaojie Lu\\nISCAS\\nXianpei Han\\nISCAS\\nWeipeng Chen\\nBaichuan Inc.\\nAbstract\\nAs Large Language Models (LLMs) continue to advance in performance,\\ntheir size has increased significantly, with current LLMs containing billions\\nor even trillions of parameters. In this study, we identify notable redun-\\ndancy across the layers of LLMs, where some layers contribute minimally\\nto overall network functionality. To quantify this, we introduce a metric\\ncalled Block Influence (BI) which use the similarity between layer’s input\\nand output to measure the importance of each layer. Based on the observa-\\ntion of layer redundancy, we propose a straightforward pruning method:\\nlayer removal, which eliminates redundant layers based on their BI scores.\\nOur approach, termed ShortGPT, demonstrates superior performance over')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "\n",
        "def generate_cheatsheet_topics(topic, language, context):\n",
        "    # System prompt\n",
        "    system = \"\"\"\n",
        "    You are an expert assistant specializing in creating well-structured and insightful cheat sheets.\n",
        "    When given a topic, your task is to brainstorm and return a comprehensive list of related subtopics that\n",
        "    are relevant, practical, and helpful for creating a cheat sheet. You must strictly adhere to the following:\n",
        "\n",
        "    1. **Language Specification**: Always return the subtopics in the language requested by the user (e.g., Spanish, English, etc.).\n",
        "    2. **Structure**:\n",
        "        - Each subtopic must have a descriptive title.\n",
        "        - Optionally, include a one-sentence description of each subtopic if requested.\n",
        "    3. **Coverage**:\n",
        "        - Cover fundamental concepts, advanced details, practical applications, and examples.\n",
        "        - Include tools, frameworks, or methodologies relevant to the topic.\n",
        "    4. **Clarity**:\n",
        "        - Use clear, concise language suitable for the given audience's level (beginner, intermediate, advanced).\n",
        "    5. **Examples**:\n",
        "        - Provide diverse subtopics catering to different perspectives or practical uses of the main topic.\n",
        "    6. **Scalability**:\n",
        "        - Ensure the subtopics are modular for easy expansion or division into cheat sheet sections.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt template\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"\"\"\n",
        "            User's topic and language request:\n",
        "            **Topic**: {topic}\n",
        "            **Language**: {language}\n",
        "\n",
        "            You must use the following context: {context}.\n",
        "\n",
        "            Generate a list of subtopics for the cheat sheet with the above context.\n",
        "            \"\"\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Chain definition\n",
        "    rag_chain = prompt | llm_llama_3_1 | StrOutputParser()\n",
        "\n",
        "    # Debugging information\n",
        "    print(f\"Generating cheat sheet topics for: {topic} in {language}\")\n",
        "\n",
        "    # Run the chain\n",
        "    generation = rag_chain.invoke({\n",
        "        \"context\": context,\n",
        "        \"topic\": topic,\n",
        "        \"language\": language,\n",
        "    })\n",
        "\n",
        "    # Return generation results\n",
        "    return generation"
      ],
      "metadata": {
        "id": "9UX7v2NWQSaV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cheat_sheet_topics = generate_cheatsheet_topics(topic, language, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCj58WouUoAu",
        "outputId": "8d319d4e-b27a-4007-df4b-9cd160098fba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating cheat sheet topics for: Create a Cheat Sheet for Large Language Models in English\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cheat_sheet_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "769NL4V9UvUV",
        "outputId": "f2e9b670-47df-47ae-8411-10f59162de70"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a comprehensive list of subtopics for the cheat sheet on \"Create a Cheat Sheet for Large Language Models\" in English:\n",
            "\n",
            "**I. Introduction to Large Language Models**\n",
            "\n",
            "1. **Definition and Overview**: Briefly explain what large language models are and their applications.\n",
            "2. **Types of Large Language Models**: Discuss the different types of LLMs, such as transformer-based models and recurrent neural networks (RNNs).\n",
            "3. **Advantages and Limitations**: Highlight the benefits and drawbacks of using LLMs in various industries and domains.\n",
            "\n",
            "**II. Challenges in Implementing Large Language Models in Enterprise**\n",
            "\n",
            "1. **Lack of Domain Knowledge**: Explain how LLMs are trained on publicly available data sets and lack access to proprietary information.\n",
            "2. **Hallucinations**: Discuss the issue of LLMs generating responses that are not accurate or relevant.\n",
            "3. **Training Data Cutoff Dates**: Describe the problem of LLMs not having access to the latest training data.\n",
            "\n",
            "**III. RAG Retrieval, Augmented Generation**\n",
            "\n",
            "1. **What is RAG?**: Explain the concept of RAG and its components, including retrieval, augmentation, and generation.\n",
            "2. **How RAG Works**: Describe the process of using RAG to improve LLM performance.\n",
            "3. **Benefits of RAG**: Highlight the advantages of using RAG, such as improved accuracy and relevance.\n",
            "\n",
            "**IV. Model Pruning and Quantization**\n",
            "\n",
            "1. **Model Pruning**: Explain the concept of model pruning and its types, including unstructured pruning and structured pruning.\n",
            "2. **Quantization**: Describe the process of quantization and its benefits, such as reducing model size and improving inference speed.\n",
            "3. **Comparison of Model Pruning and Quantization**: Discuss the differences and similarities between model pruning and quantization.\n",
            "\n",
            "**V. Redundancy in Large Language Models**\n",
            "\n",
            "1. **Definition of Redundancy**: Explain what redundancy means in the context of LLMs.\n",
            "2. **Types of Redundancy**: Discuss the different types of redundancy, such as layer redundancy and parameter redundancy.\n",
            "3. **Measuring Redundancy**: Describe the metric called Block Influence (BI) and its use in measuring redundancy.\n",
            "\n",
            "**VI. ShortGPT: A Pruning Method for Large Language Models**\n",
            "\n",
            "1. **Introduction to ShortGPT**: Explain the concept of ShortGPT and its purpose.\n",
            "2. **How ShortGPT Works**: Describe the process of using ShortGPT to prune redundant layers in LLMs.\n",
            "3. **Benefits of ShortGPT**: Highlight the advantages of using ShortGPT, such as improved performance and reduced model size.\n",
            "\n",
            "**VII. Tools and Frameworks for Large Language Models**\n",
            "\n",
            "1. **Popular LLM Frameworks**: Discuss the different frameworks used for building and deploying LLMs, such as TensorFlow and PyTorch.\n",
            "2. **LLM Development Tools**: Explain the various tools used for developing and fine-tuning LLMs, such as Hugging Face Transformers.\n",
            "3. **LLM Deployment Tools**: Describe the tools used for deploying and managing LLMs in production environments.\n",
            "\n",
            "**VIII. Best Practices for Implementing Large Language Models**\n",
            "\n",
            "1. **Data Preparation**: Discuss the importance of preparing high-quality data for training LLMs.\n",
            "2. **Model Selection**: Explain the factors to consider when selecting an LLM for a particular task or application.\n",
            "3. **Model Fine-Tuning**: Describe the process of fine-tuning an LLM for a specific task or domain.\n",
            "\n",
            "**IX. Conclusion**\n",
            "\n",
            "1. **Summary of Key Points**: Recap the main points discussed in the cheat sheet.\n",
            "2. **Future Directions**: Discuss potential future developments and applications of LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cheatsheet(list_topics):\n",
        "    # Parser\n",
        "    parser = JsonOutputParser(pydantic_object=CheatSheetSchema)\n",
        "\n",
        "    # Prompt\n",
        "    prompt_message = \"\"\"\n",
        "    You are an advanced assistant specializing in generating structured and insightful cheat sheets.\n",
        "    When given a list of subtopics and their descriptions, your task is to create a fully detailed\n",
        "    cheat sheet strictly following the structure defined below:\n",
        "\n",
        "    1. Use the following structure:\n",
        "       - `title`: The overall title of the cheat sheet.\n",
        "       - `description`: A brief introduction to the cheat sheet, explaining its purpose and scope.\n",
        "       - `metadata`: Include `author`, `version`, `last_updated`, `created_date`, `tags`, `intended_audience`, and `license`.\n",
        "       - `sections`: Divide content into logical sections based on the subtopics.\n",
        "\n",
        "    2. Each section must include:\n",
        "       - `section_title`: The title of the subtopic.\n",
        "       - `topics`: A list of key concepts, each with:\n",
        "         - `title`: Key concept title.\n",
        "         - `description`: Summary of the concept.\n",
        "         - `key_points`: List of essential points to understand.\n",
        "         - `resources`: Links or references for further exploration.\n",
        "       - `examples`: A list of examples, each with:\n",
        "         - `example_title`: Name of the example.\n",
        "         - `code_snippet`: Relevant code or practical example text.\n",
        "         - `explanation`: Explanation of the example's purpose.\n",
        "         - `output`: Expected output or results.\n",
        "         - `tags`: Keywords related to the example.\n",
        "       - `summary`: Summarize the section content in 1–2 sentences.\n",
        "       - `importance`: A numerical score (1–10) for the section's importance.\n",
        "\n",
        "    3. Metadata Rules:\n",
        "       - Always include metadata like the author's name, the cheat sheet's version, and a list of tags relevant to the topic.\n",
        "       - Specify the intended audience (e.g., beginners, intermediate users, experts).\n",
        "\n",
        "    4. Additional Information:\n",
        "       - Include at least one practical example in each section.\n",
        "       - Ensure the content is written in the language requested by the user.\n",
        "\n",
        "    Use this list of topics:\n",
        "    <list_topics>\n",
        "    {list_topics}\n",
        "    </list_topics>\n",
        "\n",
        "    You must respond as a JSON following this format:\n",
        "\n",
        "    <format_instruction>\n",
        "    {format_instructions}\n",
        "    </format_instruction>\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_message,\n",
        "        input_variables=[\"list_topics\"],\n",
        "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "    )\n",
        "\n",
        "    # Chain\n",
        "    cheat_sheet_chain = prompt | llm_for_img | parser\n",
        "\n",
        "    # Run\n",
        "    cheatsheet_response = cheat_sheet_chain.invoke(list_topics)\n",
        "\n",
        "    print(f\"Generated cheat sheet based on the topics: {cheatsheet_response}\")\n",
        "\n",
        "    return cheatsheet_response"
      ],
      "metadata": {
        "id": "fEXxqAWaPI46"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cheat_sheet = create_cheatsheet(cheat_sheet_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUu0pbutWryh",
        "outputId": "f35149b7-53de-4dbd-f554-9b275872e9b1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated cheat sheet based on the topics: {'title': 'Cheat Sheet for Large Language Models', 'description': 'This cheat sheet provides a comprehensive overview of Large Language Models (LLMs), including their types, advantages, limitations, implementation challenges, and best practices. It also covers advanced topics like Retrieval Augmented Generation (RAG), model pruning and quantization, and redundancy analysis.', 'metadata': {'author': 'Bard', 'version': '1.0', 'last_updated': '2024-07-27T12:00:00Z', 'created_date': '2024-07-27T12:00:00Z', 'tags': ['LLM', 'Large Language Model', 'NLP', 'Natural Language Processing', 'AI', 'Artificial Intelligence', 'RAG', 'Model Pruning', 'Quantization', 'Deep Learning'], 'intended_audience': 'Intermediate', 'license': 'CC BY-SA 4.0'}, 'sections': [{'section_title': 'Introduction to Large Language Models', 'topics': [{'title': 'Definition and Overview', 'description': 'LLMs are AI models trained on massive text datasets to understand and generate human-like text.', 'key_points': ['Process and generate human-like text', 'Used for various tasks like translation, summarization, question answering', 'Based on deep learning architectures like transformers'], 'resources': ['https://en.wikipedia.org/wiki/Large_language_model']}, {'title': 'Types of Large Language Models', 'description': 'Different architectures exist, each with strengths and weaknesses.', 'key_points': ['Transformer-based models (e.g., BERT, GPT): excel at long-range dependencies', 'Recurrent Neural Networks (RNNs): process sequential data, but struggle with long sequences'], 'resources': ['https://arxiv.org/abs/1706.03762']}, {'title': 'Advantages and Limitations', 'description': 'LLMs offer great potential but have inherent limitations.', 'key_points': ['Advantages: high accuracy, automation, scalability', 'Limitations: bias, lack of common sense, computational cost'], 'resources': ['https://www.forbes.com/sites/forbestechcouncil/2023/07/12/the-promise-and-peril-of-large-language-models/']}], 'examples': [{'example_title': 'Summarization using GPT-2', 'code_snippet': '```python\\n# Requires transformers library\\nfrom transformers import pipeline\\nsummarizer = pipeline(\\'summarization\\')\\ntext = \"...long text to summarize...\"\\nsummary = summarizer(text, max_length=130, min_length=30, do_sample=False)\\nprint(summary[0][\\'summary_text\\'])\\n```', 'explanation': 'Uses the Hugging Face Transformers library to summarize a given text.', 'output': 'A concise summary of the input text.', 'tags': ['summarization', 'GPT-2', 'Hugging Face']}], 'summary': 'This section introduces LLMs, their various types, and their strengths and weaknesses.  It highlights the key applications and challenges associated with their use.', 'importance': 9}, {'section_title': 'Challenges in Implementing Large Language Models in Enterprise', 'topics': [{'title': 'Lack of Domain Knowledge', 'description': 'LLMs trained on general data may lack specific industry knowledge.', 'key_points': ['Limited access to proprietary data', 'Need for fine-tuning on domain-specific data'], 'resources': []}, {'title': 'Hallucinations', 'description': 'LLMs can generate factually incorrect or nonsensical outputs.', 'key_points': ['Model may invent facts or relationships', 'Requires careful validation of outputs'], 'resources': []}, {'title': 'Training Data Cutoff Dates', 'description': 'LLMs are only aware of information up to their training data cutoff.', 'key_points': ['Models may not know about recent events', 'Regular retraining is often necessary'], 'resources': []}], 'examples': [{'example_title': 'Hallucination Example', 'code_snippet': '```\\nPrompt: \"Who won the 2025 World Cup?\"\\nResponse: \"Brazil won the 2025 World Cup.\"  (Incorrect)\\n```', 'explanation': 'The LLM hallucinates a result for a future event.', 'output': 'An incorrect answer about a future event.', 'tags': ['hallucination', 'inaccurate output']}], 'summary': 'This section explores the challenges of implementing LLMs in enterprise settings, focusing on the issues of domain knowledge, hallucinations, and outdated training data.', 'importance': 8}, {'section_title': 'RAG Retrieval, Augmented Generation', 'topics': [{'title': 'What is RAG?', 'description': 'RAG enhances LLMs by incorporating external knowledge sources.', 'key_points': ['Combines retrieval, augmentation, and generation', 'Improves accuracy and relevance of LLM outputs'], 'resources': []}, {'title': 'How RAG Works', 'description': \"RAG retrieves relevant information from a knowledge base and uses it to augment the LLM's input.\", 'key_points': ['Retrieves relevant documents from a knowledge base', 'Augments the prompt with retrieved information', 'Generates a response based on augmented prompt'], 'resources': []}, {'title': 'Benefits of RAG', 'description': 'RAG addresses limitations of standard LLMs.', 'key_points': ['Improved accuracy and factual correctness', 'Ability to handle up-to-date information'], 'resources': []}], 'examples': [{'example_title': 'Simple RAG Example', 'code_snippet': '```\\n# Conceptual example - requires specific RAG implementation\\nknowledge_base = { ... } # Dictionary of relevant information\\nquery = \"What is the capital of France?\"\\nretrieved_info = retrieve_information(query, knowledge_base)\\naugmented_prompt = query + \" \" + retrieved_info\\nresponse = generate_response(augmented_prompt)\\nprint(response)\\n```', 'explanation': 'Illustrates the basic steps involved in a RAG system: retrieving information, augmenting the prompt, and generating a response.', 'output': 'Paris', 'tags': ['RAG', 'knowledge retrieval', 'prompt augmentation']}], 'summary': 'This section explains the concept of Retrieval Augmented Generation (RAG), detailing its workflow and highlighting its significant advantages in improving LLM accuracy and access to up-to-date information.', 'importance': 10}, {'section_title': 'Model Pruning and Quantization', 'topics': [{'title': 'Model Pruning', 'description': 'Reduces model size by removing less important connections or neurons.', 'key_points': ['Unstructured pruning: removes individual connections randomly', 'Structured pruning: removes entire layers or filters'], 'resources': []}, {'title': 'Quantization', 'description': 'Reduces the precision of model weights and activations.', 'key_points': ['Reduces model size and memory footprint', 'Improves inference speed'], 'resources': []}, {'title': 'Comparison of Model Pruning and Quantization', 'description': 'Both techniques aim to reduce model size and improve efficiency, but with different approaches.', 'key_points': ['Pruning removes parameters, quantization reduces precision', 'Pruning can lead to accuracy loss, quantization can also but often less'], 'resources': []}], 'examples': [{'example_title': 'Conceptual Pruning', 'code_snippet': '```\\n#Conceptual example, actual implementation is complex\\nmodel = load_model(...) #Load a pre-trained model\\npruned_model = prune_model(model, pruning_rate=0.5) #Prune 50% of connections\\n```', 'explanation': 'Shows a simplified representation of model pruning.  The actual implementation is significantly more involved and depends on the specific framework and pruning technique.', 'output': 'A smaller, potentially less accurate model.', 'tags': ['model pruning', 'model compression']}], 'summary': 'This section describes model pruning and quantization, two techniques to reduce the size and improve the efficiency of LLMs.  It highlights their differences and similarities.', 'importance': 7}, {'section_title': 'Redundancy in Large Language Models', 'topics': [{'title': 'Definition of Redundancy', 'description': 'Redundancy refers to unnecessary parameters or layers in an LLM.', 'key_points': ['Can lead to increased computational cost and slower inference', 'Can be identified and removed to optimize the model'], 'resources': []}, {'title': 'Types of Redundancy', 'description': 'Different types of redundancy exist in LLMs.', 'key_points': ['Layer redundancy: entire layers can be redundant', 'Parameter redundancy: individual weights can be redundant'], 'resources': []}, {'title': 'Measuring Redundancy', 'description': 'Metrics like Block Influence (BI) can quantify redundancy.', 'key_points': [\"BI measures the impact of a block of parameters on the model's output\", 'Low BI indicates redundancy'], 'resources': []}], 'examples': [{'example_title': 'Conceptual Redundancy', 'code_snippet': '```\\n# Conceptual example.  Measuring redundancy requires specific techniques and metrics.\\nmodel = load_model(...) #Load a pre-trained model\\nredundancy_score = measure_redundancy(model) # Hypothetical function\\nprint(redundancy_score)\\n```', 'explanation': 'Illustrates how redundancy might be measured.  The actual process is much more complex and depends on the chosen metric (e.g., Block Influence).', 'output': 'A numerical score representing the level of redundancy.', 'tags': ['redundancy', 'model optimization']}], 'summary': 'This section defines redundancy in LLMs, explores its types, and introduces a metric (Block Influence) for quantifying and identifying redundant components within the model.', 'importance': 6}, {'section_title': 'ShortGPT: A Pruning Method for Large Language Models', 'topics': [{'title': 'Introduction to ShortGPT', 'description': 'ShortGPT is a pruning method designed to remove redundant layers.', 'key_points': ['Focuses on identifying and removing redundant layers', 'Aims to improve efficiency without significant accuracy loss'], 'resources': []}, {'title': 'How ShortGPT Works', 'description': 'ShortGPT uses a specific algorithm to identify and prune redundant layers.', 'key_points': ['Analyzes layer importance', 'Removes layers with low importance'], 'resources': []}, {'title': 'Benefits of ShortGPT', 'description': 'ShortGPT offers several advantages.', 'key_points': ['Reduced model size and computational cost', 'Improved inference speed', 'Minimal accuracy loss'], 'resources': []}], 'examples': [{'example_title': 'ShortGPT Application', 'code_snippet': '```\\n# Conceptual example.  Requires specific ShortGPT implementation.\\nmodel = load_model(...) #Load a pre-trained model\\npruned_model = shortgpt_prune(model) #Hypothetical ShortGPT function\\n```', 'explanation': 'Illustrates the application of ShortGPT for pruning a model.  The actual implementation would involve using a specific ShortGPT library or code.', 'output': 'A pruned model with reduced size and potentially improved efficiency.', 'tags': ['ShortGPT', 'model pruning', 'layer pruning']}], 'summary': 'This section introduces ShortGPT, a layer pruning method for LLMs, explaining its operation and highlighting its benefits in improving model efficiency while minimizing accuracy loss.', 'importance': 6}, {'section_title': 'Tools and Frameworks for Large Language Models', 'topics': [{'title': 'Popular LLM Frameworks', 'description': 'Several frameworks facilitate LLM development and deployment.', 'key_points': ['TensorFlow: widely used for deep learning', 'PyTorch: popular for its flexibility and ease of use'], 'resources': ['https://www.tensorflow.org/', 'https://pytorch.org/']}, {'title': 'LLM Development Tools', 'description': 'Tools aid in training and fine-tuning LLMs.', 'key_points': ['Hugging Face Transformers: provides pre-trained models and tools', 'Datasets: simplifies data loading and preprocessing'], 'resources': ['https://huggingface.co/', 'https://huggingface.co/datasets']}, {'title': 'LLM Deployment Tools', 'description': 'Tools for deploying and managing LLMs in production.', 'key_points': ['TensorFlow Serving', 'TorchServe'], 'resources': ['https://www.tensorflow.org/tfx/serving', 'https://pytorch.org/serve/']}], 'examples': [{'example_title': 'Using Hugging Face Transformers', 'code_snippet': '```python\\nfrom transformers import pipeline\\nclassifier = pipeline(\"sentiment-analysis\")\\nresult = classifier(\"This is a great product!\")\\nprint(result)\\n```', 'explanation': 'Shows how to use the Hugging Face Transformers library to perform sentiment analysis.', 'output': \"[{'label': 'POSITIVE', 'score': 0.98}]\", 'tags': ['Hugging Face', 'sentiment analysis', 'transformers']}], 'summary': 'This section lists popular frameworks and tools for developing, fine-tuning, and deploying LLMs, focusing on commonly used libraries and platforms.', 'importance': 8}, {'section_title': 'Best Practices for Implementing Large Language Models', 'topics': [{'title': 'Data Preparation', 'description': 'High-quality data is crucial for training effective LLMs.', 'key_points': ['Clean and preprocessed data', 'Sufficient data volume', 'Data diversity'], 'resources': []}, {'title': 'Model Selection', 'description': 'Choosing the right LLM depends on the task and resources.', 'key_points': ['Consider model size, performance, and computational cost', 'Pre-trained models vs. training from scratch'], 'resources': []}, {'title': 'Model Fine-Tuning', 'description': 'Adapting a pre-trained model to a specific task or domain.', 'key_points': ['Fine-tune on a relevant dataset', 'Monitor performance during fine-tuning'], 'resources': []}], 'examples': [{'example_title': 'Fine-tuning a Sentiment Analysis Model', 'code_snippet': '```\\n# Conceptual example.  Requires a specific framework and fine-tuning strategy.\\nmodel = load_model(...) #Load a pre-trained model\\nfine_tuned_model = fine_tune(model, training_data) #Hypothetical fine-tuning function\\n```', 'explanation': 'Illustrates the process of fine-tuning a pre-trained model.  The actual implementation depends on the chosen framework and fine-tuning technique.', 'output': 'A fine-tuned model specialized for sentiment analysis.', 'tags': ['fine-tuning', 'model adaptation']}], 'summary': 'This section outlines best practices for implementing LLMs, emphasizing data preparation, model selection, and the crucial process of model fine-tuning for specific tasks and domains.', 'importance': 9}, {'section_title': 'Conclusion', 'topics': [{'title': 'Summary of Key Points', 'description': 'Recap of the main concepts and challenges discussed.', 'key_points': ['LLMs offer powerful capabilities but also present challenges.', 'RAG, pruning, and quantization are important optimization techniques.', 'Careful consideration of data, model selection, and fine-tuning is crucial.'], 'resources': []}, {'title': 'Future Directions', 'description': 'Potential advancements and applications of LLMs.', 'key_points': ['More efficient and robust models', 'Improved methods for handling bias and hallucinations', 'Integration with other AI technologies'], 'resources': []}], 'examples': [], 'summary': 'This concluding section summarizes the key takeaways from the cheat sheet and looks towards future developments and applications in the field of large language models.', 'importance': 5}], 'total_sections': 8, 'difficulty_level': 'Intermediate', 'usage_scenarios': ['Learning about LLMs', 'Implementing LLMs in projects', 'Troubleshooting LLM issues', 'Optimizing LLM performance']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cheat_sheet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKWbl4d2W1TP",
        "outputId": "4c0e4490-5c4f-40e0-c17b-74950707facf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Cheat Sheet for Large Language Models',\n",
              " 'description': 'This cheat sheet provides a comprehensive overview of Large Language Models (LLMs), including their types, advantages, limitations, implementation challenges, and best practices. It also covers advanced topics like Retrieval Augmented Generation (RAG), model pruning and quantization, and redundancy analysis.',\n",
              " 'metadata': {'author': 'Bard',\n",
              "  'version': '1.0',\n",
              "  'last_updated': '2024-07-27T12:00:00Z',\n",
              "  'created_date': '2024-07-27T12:00:00Z',\n",
              "  'tags': ['LLM',\n",
              "   'Large Language Model',\n",
              "   'NLP',\n",
              "   'Natural Language Processing',\n",
              "   'AI',\n",
              "   'Artificial Intelligence',\n",
              "   'RAG',\n",
              "   'Model Pruning',\n",
              "   'Quantization',\n",
              "   'Deep Learning'],\n",
              "  'intended_audience': 'Intermediate',\n",
              "  'license': 'CC BY-SA 4.0'},\n",
              " 'sections': [{'section_title': 'Introduction to Large Language Models',\n",
              "   'topics': [{'title': 'Definition and Overview',\n",
              "     'description': 'LLMs are AI models trained on massive text datasets to understand and generate human-like text.',\n",
              "     'key_points': ['Process and generate human-like text',\n",
              "      'Used for various tasks like translation, summarization, question answering',\n",
              "      'Based on deep learning architectures like transformers'],\n",
              "     'resources': ['https://en.wikipedia.org/wiki/Large_language_model']},\n",
              "    {'title': 'Types of Large Language Models',\n",
              "     'description': 'Different architectures exist, each with strengths and weaknesses.',\n",
              "     'key_points': ['Transformer-based models (e.g., BERT, GPT): excel at long-range dependencies',\n",
              "      'Recurrent Neural Networks (RNNs): process sequential data, but struggle with long sequences'],\n",
              "     'resources': ['https://arxiv.org/abs/1706.03762']},\n",
              "    {'title': 'Advantages and Limitations',\n",
              "     'description': 'LLMs offer great potential but have inherent limitations.',\n",
              "     'key_points': ['Advantages: high accuracy, automation, scalability',\n",
              "      'Limitations: bias, lack of common sense, computational cost'],\n",
              "     'resources': ['https://www.forbes.com/sites/forbestechcouncil/2023/07/12/the-promise-and-peril-of-large-language-models/']}],\n",
              "   'examples': [{'example_title': 'Summarization using GPT-2',\n",
              "     'code_snippet': '```python\\n# Requires transformers library\\nfrom transformers import pipeline\\nsummarizer = pipeline(\\'summarization\\')\\ntext = \"...long text to summarize...\"\\nsummary = summarizer(text, max_length=130, min_length=30, do_sample=False)\\nprint(summary[0][\\'summary_text\\'])\\n```',\n",
              "     'explanation': 'Uses the Hugging Face Transformers library to summarize a given text.',\n",
              "     'output': 'A concise summary of the input text.',\n",
              "     'tags': ['summarization', 'GPT-2', 'Hugging Face']}],\n",
              "   'summary': 'This section introduces LLMs, their various types, and their strengths and weaknesses.  It highlights the key applications and challenges associated with their use.',\n",
              "   'importance': 9},\n",
              "  {'section_title': 'Challenges in Implementing Large Language Models in Enterprise',\n",
              "   'topics': [{'title': 'Lack of Domain Knowledge',\n",
              "     'description': 'LLMs trained on general data may lack specific industry knowledge.',\n",
              "     'key_points': ['Limited access to proprietary data',\n",
              "      'Need for fine-tuning on domain-specific data'],\n",
              "     'resources': []},\n",
              "    {'title': 'Hallucinations',\n",
              "     'description': 'LLMs can generate factually incorrect or nonsensical outputs.',\n",
              "     'key_points': ['Model may invent facts or relationships',\n",
              "      'Requires careful validation of outputs'],\n",
              "     'resources': []},\n",
              "    {'title': 'Training Data Cutoff Dates',\n",
              "     'description': 'LLMs are only aware of information up to their training data cutoff.',\n",
              "     'key_points': ['Models may not know about recent events',\n",
              "      'Regular retraining is often necessary'],\n",
              "     'resources': []}],\n",
              "   'examples': [{'example_title': 'Hallucination Example',\n",
              "     'code_snippet': '```\\nPrompt: \"Who won the 2025 World Cup?\"\\nResponse: \"Brazil won the 2025 World Cup.\"  (Incorrect)\\n```',\n",
              "     'explanation': 'The LLM hallucinates a result for a future event.',\n",
              "     'output': 'An incorrect answer about a future event.',\n",
              "     'tags': ['hallucination', 'inaccurate output']}],\n",
              "   'summary': 'This section explores the challenges of implementing LLMs in enterprise settings, focusing on the issues of domain knowledge, hallucinations, and outdated training data.',\n",
              "   'importance': 8},\n",
              "  {'section_title': 'RAG Retrieval, Augmented Generation',\n",
              "   'topics': [{'title': 'What is RAG?',\n",
              "     'description': 'RAG enhances LLMs by incorporating external knowledge sources.',\n",
              "     'key_points': ['Combines retrieval, augmentation, and generation',\n",
              "      'Improves accuracy and relevance of LLM outputs'],\n",
              "     'resources': []},\n",
              "    {'title': 'How RAG Works',\n",
              "     'description': \"RAG retrieves relevant information from a knowledge base and uses it to augment the LLM's input.\",\n",
              "     'key_points': ['Retrieves relevant documents from a knowledge base',\n",
              "      'Augments the prompt with retrieved information',\n",
              "      'Generates a response based on augmented prompt'],\n",
              "     'resources': []},\n",
              "    {'title': 'Benefits of RAG',\n",
              "     'description': 'RAG addresses limitations of standard LLMs.',\n",
              "     'key_points': ['Improved accuracy and factual correctness',\n",
              "      'Ability to handle up-to-date information'],\n",
              "     'resources': []}],\n",
              "   'examples': [{'example_title': 'Simple RAG Example',\n",
              "     'code_snippet': '```\\n# Conceptual example - requires specific RAG implementation\\nknowledge_base = { ... } # Dictionary of relevant information\\nquery = \"What is the capital of France?\"\\nretrieved_info = retrieve_information(query, knowledge_base)\\naugmented_prompt = query + \" \" + retrieved_info\\nresponse = generate_response(augmented_prompt)\\nprint(response)\\n```',\n",
              "     'explanation': 'Illustrates the basic steps involved in a RAG system: retrieving information, augmenting the prompt, and generating a response.',\n",
              "     'output': 'Paris',\n",
              "     'tags': ['RAG', 'knowledge retrieval', 'prompt augmentation']}],\n",
              "   'summary': 'This section explains the concept of Retrieval Augmented Generation (RAG), detailing its workflow and highlighting its significant advantages in improving LLM accuracy and access to up-to-date information.',\n",
              "   'importance': 10},\n",
              "  {'section_title': 'Model Pruning and Quantization',\n",
              "   'topics': [{'title': 'Model Pruning',\n",
              "     'description': 'Reduces model size by removing less important connections or neurons.',\n",
              "     'key_points': ['Unstructured pruning: removes individual connections randomly',\n",
              "      'Structured pruning: removes entire layers or filters'],\n",
              "     'resources': []},\n",
              "    {'title': 'Quantization',\n",
              "     'description': 'Reduces the precision of model weights and activations.',\n",
              "     'key_points': ['Reduces model size and memory footprint',\n",
              "      'Improves inference speed'],\n",
              "     'resources': []},\n",
              "    {'title': 'Comparison of Model Pruning and Quantization',\n",
              "     'description': 'Both techniques aim to reduce model size and improve efficiency, but with different approaches.',\n",
              "     'key_points': ['Pruning removes parameters, quantization reduces precision',\n",
              "      'Pruning can lead to accuracy loss, quantization can also but often less'],\n",
              "     'resources': []}],\n",
              "   'examples': [{'example_title': 'Conceptual Pruning',\n",
              "     'code_snippet': '```\\n#Conceptual example, actual implementation is complex\\nmodel = load_model(...) #Load a pre-trained model\\npruned_model = prune_model(model, pruning_rate=0.5) #Prune 50% of connections\\n```',\n",
              "     'explanation': 'Shows a simplified representation of model pruning.  The actual implementation is significantly more involved and depends on the specific framework and pruning technique.',\n",
              "     'output': 'A smaller, potentially less accurate model.',\n",
              "     'tags': ['model pruning', 'model compression']}],\n",
              "   'summary': 'This section describes model pruning and quantization, two techniques to reduce the size and improve the efficiency of LLMs.  It highlights their differences and similarities.',\n",
              "   'importance': 7},\n",
              "  {'section_title': 'Redundancy in Large Language Models',\n",
              "   'topics': [{'title': 'Definition of Redundancy',\n",
              "     'description': 'Redundancy refers to unnecessary parameters or layers in an LLM.',\n",
              "     'key_points': ['Can lead to increased computational cost and slower inference',\n",
              "      'Can be identified and removed to optimize the model'],\n",
              "     'resources': []},\n",
              "    {'title': 'Types of Redundancy',\n",
              "     'description': 'Different types of redundancy exist in LLMs.',\n",
              "     'key_points': ['Layer redundancy: entire layers can be redundant',\n",
              "      'Parameter redundancy: individual weights can be redundant'],\n",
              "     'resources': []},\n",
              "    {'title': 'Measuring Redundancy',\n",
              "     'description': 'Metrics like Block Influence (BI) can quantify redundancy.',\n",
              "     'key_points': [\"BI measures the impact of a block of parameters on the model's output\",\n",
              "      'Low BI indicates redundancy'],\n",
              "     'resources': []}],\n",
              "   'examples': [{'example_title': 'Conceptual Redundancy',\n",
              "     'code_snippet': '```\\n# Conceptual example.  Measuring redundancy requires specific techniques and metrics.\\nmodel = load_model(...) #Load a pre-trained model\\nredundancy_score = measure_redundancy(model) # Hypothetical function\\nprint(redundancy_score)\\n```',\n",
              "     'explanation': 'Illustrates how redundancy might be measured.  The actual process is much more complex and depends on the chosen metric (e.g., Block Influence).',\n",
              "     'output': 'A numerical score representing the level of redundancy.',\n",
              "     'tags': ['redundancy', 'model optimization']}],\n",
              "   'summary': 'This section defines redundancy in LLMs, explores its types, and introduces a metric (Block Influence) for quantifying and identifying redundant components within the model.',\n",
              "   'importance': 6},\n",
              "  {'section_title': 'ShortGPT: A Pruning Method for Large Language Models',\n",
              "   'topics': [{'title': 'Introduction to ShortGPT',\n",
              "     'description': 'ShortGPT is a pruning method designed to remove redundant layers.',\n",
              "     'key_points': ['Focuses on identifying and removing redundant layers',\n",
              "      'Aims to improve efficiency without significant accuracy loss'],\n",
              "     'resources': []},\n",
              "    {'title': 'How ShortGPT Works',\n",
              "     'description': 'ShortGPT uses a specific algorithm to identify and prune redundant layers.',\n",
              "     'key_points': ['Analyzes layer importance',\n",
              "      'Removes layers with low importance'],\n",
              "     'resources': []},\n",
              "    {'title': 'Benefits of ShortGPT',\n",
              "     'description': 'ShortGPT offers several advantages.',\n",
              "     'key_points': ['Reduced model size and computational cost',\n",
              "      'Improved inference speed',\n",
              "      'Minimal accuracy loss'],\n",
              "     'resources': []}],\n",
              "   'examples': [{'example_title': 'ShortGPT Application',\n",
              "     'code_snippet': '```\\n# Conceptual example.  Requires specific ShortGPT implementation.\\nmodel = load_model(...) #Load a pre-trained model\\npruned_model = shortgpt_prune(model) #Hypothetical ShortGPT function\\n```',\n",
              "     'explanation': 'Illustrates the application of ShortGPT for pruning a model.  The actual implementation would involve using a specific ShortGPT library or code.',\n",
              "     'output': 'A pruned model with reduced size and potentially improved efficiency.',\n",
              "     'tags': ['ShortGPT', 'model pruning', 'layer pruning']}],\n",
              "   'summary': 'This section introduces ShortGPT, a layer pruning method for LLMs, explaining its operation and highlighting its benefits in improving model efficiency while minimizing accuracy loss.',\n",
              "   'importance': 6},\n",
              "  {'section_title': 'Tools and Frameworks for Large Language Models',\n",
              "   'topics': [{'title': 'Popular LLM Frameworks',\n",
              "     'description': 'Several frameworks facilitate LLM development and deployment.',\n",
              "     'key_points': ['TensorFlow: widely used for deep learning',\n",
              "      'PyTorch: popular for its flexibility and ease of use'],\n",
              "     'resources': ['https://www.tensorflow.org/', 'https://pytorch.org/']},\n",
              "    {'title': 'LLM Development Tools',\n",
              "     'description': 'Tools aid in training and fine-tuning LLMs.',\n",
              "     'key_points': ['Hugging Face Transformers: provides pre-trained models and tools',\n",
              "      'Datasets: simplifies data loading and preprocessing'],\n",
              "     'resources': ['https://huggingface.co/',\n",
              "      'https://huggingface.co/datasets']},\n",
              "    {'title': 'LLM Deployment Tools',\n",
              "     'description': 'Tools for deploying and managing LLMs in production.',\n",
              "     'key_points': ['TensorFlow Serving', 'TorchServe'],\n",
              "     'resources': ['https://www.tensorflow.org/tfx/serving',\n",
              "      'https://pytorch.org/serve/']}],\n",
              "   'examples': [{'example_title': 'Using Hugging Face Transformers',\n",
              "     'code_snippet': '```python\\nfrom transformers import pipeline\\nclassifier = pipeline(\"sentiment-analysis\")\\nresult = classifier(\"This is a great product!\")\\nprint(result)\\n```',\n",
              "     'explanation': 'Shows how to use the Hugging Face Transformers library to perform sentiment analysis.',\n",
              "     'output': \"[{'label': 'POSITIVE', 'score': 0.98}]\",\n",
              "     'tags': ['Hugging Face', 'sentiment analysis', 'transformers']}],\n",
              "   'summary': 'This section lists popular frameworks and tools for developing, fine-tuning, and deploying LLMs, focusing on commonly used libraries and platforms.',\n",
              "   'importance': 8},\n",
              "  {'section_title': 'Best Practices for Implementing Large Language Models',\n",
              "   'topics': [{'title': 'Data Preparation',\n",
              "     'description': 'High-quality data is crucial for training effective LLMs.',\n",
              "     'key_points': ['Clean and preprocessed data',\n",
              "      'Sufficient data volume',\n",
              "      'Data diversity'],\n",
              "     'resources': []},\n",
              "    {'title': 'Model Selection',\n",
              "     'description': 'Choosing the right LLM depends on the task and resources.',\n",
              "     'key_points': ['Consider model size, performance, and computational cost',\n",
              "      'Pre-trained models vs. training from scratch'],\n",
              "     'resources': []},\n",
              "    {'title': 'Model Fine-Tuning',\n",
              "     'description': 'Adapting a pre-trained model to a specific task or domain.',\n",
              "     'key_points': ['Fine-tune on a relevant dataset',\n",
              "      'Monitor performance during fine-tuning'],\n",
              "     'resources': []}],\n",
              "   'examples': [{'example_title': 'Fine-tuning a Sentiment Analysis Model',\n",
              "     'code_snippet': '```\\n# Conceptual example.  Requires a specific framework and fine-tuning strategy.\\nmodel = load_model(...) #Load a pre-trained model\\nfine_tuned_model = fine_tune(model, training_data) #Hypothetical fine-tuning function\\n```',\n",
              "     'explanation': 'Illustrates the process of fine-tuning a pre-trained model.  The actual implementation depends on the chosen framework and fine-tuning technique.',\n",
              "     'output': 'A fine-tuned model specialized for sentiment analysis.',\n",
              "     'tags': ['fine-tuning', 'model adaptation']}],\n",
              "   'summary': 'This section outlines best practices for implementing LLMs, emphasizing data preparation, model selection, and the crucial process of model fine-tuning for specific tasks and domains.',\n",
              "   'importance': 9},\n",
              "  {'section_title': 'Conclusion',\n",
              "   'topics': [{'title': 'Summary of Key Points',\n",
              "     'description': 'Recap of the main concepts and challenges discussed.',\n",
              "     'key_points': ['LLMs offer powerful capabilities but also present challenges.',\n",
              "      'RAG, pruning, and quantization are important optimization techniques.',\n",
              "      'Careful consideration of data, model selection, and fine-tuning is crucial.'],\n",
              "     'resources': []},\n",
              "    {'title': 'Future Directions',\n",
              "     'description': 'Potential advancements and applications of LLMs.',\n",
              "     'key_points': ['More efficient and robust models',\n",
              "      'Improved methods for handling bias and hallucinations',\n",
              "      'Integration with other AI technologies'],\n",
              "     'resources': []}],\n",
              "   'examples': [],\n",
              "   'summary': 'This concluding section summarizes the key takeaways from the cheat sheet and looks towards future developments and applications in the field of large language models.',\n",
              "   'importance': 5}],\n",
              " 'total_sections': 8,\n",
              " 'difficulty_level': 'Intermediate',\n",
              " 'usage_scenarios': ['Learning about LLMs',\n",
              "  'Implementing LLMs in projects',\n",
              "  'Troubleshooting LLM issues',\n",
              "  'Optimizing LLM performance']}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}